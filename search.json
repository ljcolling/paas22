[
  {
    "objectID": "lectures/week01/handout/index.html",
    "href": "lectures/week01/handout/index.html",
    "title": "Lecture 1: Introduction to Psychology as a Science",
    "section": "",
    "text": "The aim of today’s lecture is to provide you with an overview of the structure of the research methods modules that you’ll be taking in your degree. I’ll also try to give you some tips that will help you to succeed in this course. Although this is not an inherently difficult course, a lot of the material you’ll be encountering may be new to you. Some of it might even seem a little intimidating (in particular, the programming that you’ll be doing in the practical classes). But if you stick with it, and give it your best shot, then I’m confident that you’ll be able to do well."
  },
  {
    "objectID": "lectures/week01/handout/index.html#introductions",
    "href": "lectures/week01/handout/index.html#introductions",
    "title": "Lecture 1: Introduction to Psychology as a Science",
    "section": "Introductions",
    "text": "Introductions\nThe main teaching team for Psychology as a Science (PAAS) is made up of three people. Each of us has different roles and responsibilities. Knowing this will help you to know who to contact if you need help with something (but see more on knowing who to contact below).\nDr Lincoln Colling (lecture content convenor)\nDr Colling is responsible for the lecture content, and for preparing the final exam. The final exam will cover the topics covered in the main weekly lectures.\nDr Jenny Terry (practical content convenor)\nDr Terry is responsible for the content covered in the practical sessions, the R homework activities, and the weekly R assessments. She is also responsible for the lab report, the major piece of written work you’ll produce this year.\nIf you don’t know what R is then don’t worry—99% of the people taking this course won’t know what R is. But you’ll learn all about R and RStudio in the practical classes.\nDr Vlad Costin\nDr Costin, along with Dr Terry, is one of the lead practical lecturers.\nApart from these three people you’ll also get to meet Dr Bryan Singer, who will be the lecturer for the special ethics lecture in Week 6. There are also several doctoral tutors who will be helping out with the practical classes and who will also be responsible for marking the lab report."
  },
  {
    "objectID": "lectures/week01/handout/index.html#tips-for-doing-well",
    "href": "lectures/week01/handout/index.html#tips-for-doing-well",
    "title": "Lecture 1: Introduction to Psychology as a Science",
    "section": "Tips for doing well",
    "text": "Tips for doing well\nThe information that most of you will want to know is how to do well in this course, so we’ll get it out of the way right up the front. The single best thing you can do to do well in this course is to show up to class every week. Turning up in person every week not only means that you’ll get exposed to the lecture content, but you’ll also get to meet me, talk to me, meet your classmates, and talk to them too. So much of what makes university special are those moments that happen around the classes. And all you need to do to experience those moments is to show up to class every week.\nApart from showing up every week the other thing you can do is to do all the assessments. This is particularly important for the weekly assessments linked to the practical content. Each of these assessments builds on the one before, so if you fall behind it can be hard to catch up. Even if you find them difficult, you should still do them. They will get easier!\nStart off by being present for everything and the rest will fall into place!"
  },
  {
    "objectID": "lectures/week01/handout/index.html#finding-the-information-you-need",
    "href": "lectures/week01/handout/index.html#finding-the-information-you-need",
    "title": "Lecture 1: Introduction to Psychology as a Science",
    "section": "Finding the information you need",
    "text": "Finding the information you need\nOne of the most important things you need to know if you want to succeed in the course (and this is true of all the courses you’ll be taking) is how to find the information you need. The first place to look for any information is on Canvas. If you’re reading this lecture handout then you’ve probably already found Canvas. The Canvas page for this course will be your one-stop shop for almost all the information you need about this course. Familiarise yourself with it. Especially the front page! Click all the links, and read through the pages. If you have a question like “What do I do if I can’t make a practical or lecture?”, then that information is on Canvas. If you want to know what the assessments are for this course, then that will also be on Canvas. And If you want to know whether references are counted as part of the word count for your report, then that will be on Canvas too!\nThere’s a lot of information on Canvas. In fact, there’s probably too much information, and so it can be a little overwhelming. To help with this, I have two tips.\nFirst, as I said above, click the links and read through all the pages. But while you’re doing this, keep a bit of paper or a notebook handy. Use this to note down bits of information, or where to find certain information, so you can check back when you need it. You can also bookmark pages that you think will be useful later. And when you bookmark a page you can give it an informative title that is meaningful to you. All this will help you find the information you need when you need it. And remember, on any web-page you can hit Ctrl F (windows) or Command F (mac) to search for a word. This is useful if it’s a long page with a lot of text, and you’re looking for something specific.\nSecond, check out the Frequently Asked Questions (FAQ). Here I will list the answers to some questions that I tend to encounter often. You might just find the answer to your question here. I will try to update this as I encounter more frequently asked questions, so make sure to check back whenever you’re stuck.\n\nKnowing who to contact\nOne thing that is tricky when you’re first starting out at university is knowing who to contact with your queries. Here are some tips to guide you in the correct direction.\n\nAdmin queries\nAdmin queries are any queries than are not directly related to the course content. For example, if you’ve been absent from class, and you want to inform somebody, then that’s an admin query. If you’re meant to have an extension to hand-in date for an assessment, but it hasn’t been correctly applied, then that’s an admin query. If you’re unable to submit and assessment because you’re unwell, then that too is an admin query. All admin queries should be directed at the admin staff.\nYou can contact the admin staff by emailing them at psychology@sussex.ac.uk. If you’d specifically like to report an absence then you can psychologyabsence@sussex.ac.uk. For more information about Attendance and Absences, check out the page on Canvas.\n\n\n\n\n\n\nImportant\n\n\n\nLecturers can’t grant extensions for assessments. Please don’t email us to ask, because there’s nothing we can do about it.\nFor information about Requests for Extended Deadlines see the information on the Student Support Unit page.\n\n\n\n\nQueries about course content\nThe lecturers are your point of contact if you have questions about the course content. Who you contact will depend on what section of the course your question covers:\nFor questions about lecture materials\nIf you have a question about material that was covered in one of the regular weekly lectures then the best person to get in touch with is me, Dr Lincoln Colling. I can also help with queries related to the final exam (because the final exam covers the material from the regular weekly lectures).\nThe best way to get in touch with me is to grab me right after the lecture. I’ll always hang around for a few minutes to answer questions. But if you need to run off then you can always book into a drop in session. To book into a drop in session, use the calendar on the front page of the module’s canvas site.\nFor questions about the practical content\nIf you have a question about something that was covered in one of your weekly practical sessions then the best person to contact would be the practical lead for your practical. This will either be Dr Jenny Terry or Dr Vlad Costin.\nYour practical lead will also be able to help you out with queries about the weekly R assessments.\nThe best way to contact Dr Terry and Dr Costin is to talk to them in your practical session.\nFor queries about the report\nIf you have queries about the report, then the best person to contact is Dr Jenny Terry.\nFor general R help\nIf you have general R and RStudio queries, and you weren’t able to get them answered in class, then you can go to one of the R Drop in Sessions. You can go to any of the R Drop in Sessions, but try to go to one run by somebody that teaches on PAAS. To book into an R Drop in session, use the calender on the front page of the module’s canvas site.\n\n\n\nModule Discussion Board (Discord)\nThe course discussion board is run through Discord. To sign up to the course discord server use the following link https://discord.gg/TBf8Zju2xv\nYou may sign up under an alias so that you can ask your questions anonymously, if you prefer.\nThe Discord server will be checked regularly, but is not moderated 24/7. If anyone spots anything that could be deemed inappropriate, please tag the teaching team. Inappropriate posts will be removed, as will any repeat offenders.\n\n\n\n\n\n\nEmailing the teaching team\n\n\n\nYou should only email the teaching team if you have something that you need to discuss in confidence. Instead of emailing, the best way to get help is to check the FAQ. You can also talk to the lecturer during or after class or you could post on the discussion board if you need help with something not covered in the FAQ.\nIf you have longer queries, or need extra help with something, then book into a drop in session using the calender on the front page of the module’s canvas site. Remember, book into a session with me if you have a query about the Lecture content, or with Dr Jerry or Dr Costin if you have a query about the lab report."
  },
  {
    "objectID": "lectures/week01/handout/index.html#research-methods-in-psychology",
    "href": "lectures/week01/handout/index.html#research-methods-in-psychology",
    "title": "Lecture 1: Introduction to Psychology as a Science",
    "section": "Research methods in Psychology",
    "text": "Research methods in Psychology\nPsychology as a Science is the first of a series of research methods modules that you’ll take during your psychology degree.\nFollowing this module, you’ll take the following modules:\n\nAnalysing Data (next term)\nDiscovering Statistics (year 2)\nQuantitative and Qualitative Methods (year 2)\n\nThese courses build up to prepare you for your research dissertation in the final year. But they’re also a great way to learn a lot of transferable skills that are useful outside university, for example: how to analyse data, how to make sense of statistics, and computer programming/coding skills.\nPsychology as a Science (PAAS) covers an introduction to the research process. Some topics you’ll cover include:\n\nIntroduction to Philosophy of Science\nDifferent approaches to research including quantitative methods and qualitative methods\nBasics of statistical theory\n\nWe’ll also introduce you to coding in the R programming language. You’ll cover the basics of the R language, how to process and summarise data, and how to make plots in R. Along the way we’ll work with some of the same data that you’ll be using for your Cognition in Clinical Contexts lab report.\nIn Analysing Data (AD) you’ll learn about the core statistical tests used in Psychology. You’ll also cover more advanced use of R, and you’ll learn how to perform statistical tests in R. AD will also give you your first chance to independently analyse some data.\nIn Discovering Statistics (DS) you’ll learn even more advanced statistical tests, and more in-depth knowledge of R.\nAnd in the final course, Quantitative and Qualitative Methods (QQM) you’ll learn about advanced multivariate statistical techniques. But you’ll also learn about non-statistical approaches such as interviews and discourse analysis.\nThe work you do in these modules will also connect up with other modules:\n\nAnalysing Data with, for example, Psychobiology\nDiscovering Statistics with, for example, Developmental Psychology\nAnd Quantitative and Qualitative Methods with, for example, Social Psychology\n\nResearch methods does not happen in isolation, but it’s connected with everything else you do in your degree.\n\nWhy research methods?\nThe dominant approach to training psychologists is the scientist practitioner model. Doing research is integral to this approach! Just like medical doctors, who not only deliver treatments but also develop treatments, psychologists also apply and produce knowledge. As a psychologist you want to do what works and being able to read, critique, and conduct research will help you know what works and allow you to develop evidence-based care.\nEven for those that don’t become psychologists, research methods is still useful and the skills you’ll learn in your research methods courses will prepare you for careers in a range of industries. These might include working as a data scientist, working in consultancy, or working in the civil service and government."
  },
  {
    "objectID": "lectures/week01/handout/index.html#structure-of-this-module",
    "href": "lectures/week01/handout/index.html#structure-of-this-module",
    "title": "Lecture 1: Introduction to Psychology as a Science",
    "section": "Structure of this module",
    "text": "Structure of this module\nLike most research methods modules PAAS is made up of three main activities:\n\nWeekly lectures\n\nOne hour each week.\nCovers research methods, statistics, and theory\nNote that you’ll also have a special lecture one evening in Week 6 that will cover research ethics. Check your timetable!\n\nTutorials/Practical preparation homework\n\nAbout an hour a week.\nDone in R as preparation for the practical class.\n\nPractical classes\n\nTwo hours a week.\nGives you hands-on experience with the R programming language.\n\n\nYou’ll find out more about the exact structure of the tutorials and practicals in the practical sessions.\n\nAssessment Structure\nFor the assessment there’s a 50/50 split between coursework and the exam.\nThe coursework is made up of four parts. Pay special attention to this section, because the list on Sussex Direct can be a little confusing.\n\nComputer Based Exam worth 10% due in approximately Week 8\nThis will cover the material from the ethics lecture in Week 6\nComputer Based Exam worth 40% (see below about due date)\nThis refers to the R assessments that you’ll do. You’ll find out more about these in the practical classes. It’s very important to note that this isn’t a single assessment due at the end of term. Rather these consist of several assessments you’ll do through out the term. Make sure you keep up with them, because doing them will help you achieve a good grade.\nReport worth 40% due in approximately Wk 9\nPortfolio with 10% (listed on Sussex Direct as due in Wk 11)\nThis refers to the 20 credits worth of research participation that you’re required to do as part of the course. To find out more about this follow the big yellow link labelled Research Participation/Sona.\n\nThe due date is the final day you can complete research participation. But don’t wait until the last week to do it, because there’ll probably be no studies left to take part in. These are easy marks so don’t miss out on getting them. Doing research participation is also important for your academic development, because it will give you a first-hand experience of the research process."
  },
  {
    "objectID": "lectures/week01/handout/index.html#lecture-topics",
    "href": "lectures/week01/handout/index.html#lecture-topics",
    "title": "Lecture 1: Introduction to Psychology as a Science",
    "section": "Lecture topics",
    "text": "Lecture topics\nThe rough outline of the lecture schedule is as follows:\n\n\n\n\n\nWeek\nTopic\n\n\n\n\n1\nIntroduction to Psychology as a Science\n\n\n2\nWhat is this thing called “Science”?\n\n\n3\nApproaches to Research\n\n\n4\nIntroduction to study design\n\n\n5\nOpen Science\n\n\n6\nDescribing measurements I\n\n\n7\nDescribing measurements II\n\n\n8\nDistributions\n\n\n9\nTransformation and comparisons\n\n\n10\nVisual summaries of data\n\n\n11\nIntroduction to probability\n\n\n\n\n\nThe first set of lectures will cover big picture ideas. These lectures will probably be most useful in helping you to prepare for the report.\nIn these lectures we’ll talk about issues like:\n\nWhat are scientific theories?\nWhat issues do we need to consider when we’re measuring phenomena?\nWhat does it mean to operationalise our variables?\nWhat are different approaches you can take when conducting a study?\nWhat are some sources of bias in psychology studies and publishing of psychology studies, and how might we be able to ameliorate some of these biases?\n\nThe second set of lectures are all about preparing you for learning about statistics and working with data.\nIn these lectures you’ll learn the underlying theory of statistical testing. You’ll learn how to reason about statistics and data, and the relationship between scientific hypotheses and statistical hypotheses.\nDoing statistics isn’t like following a recipe. It’s not about just picking the “correct” statistical test out of a list. It involves thinking about what you want to know, why you want to know it, and how statistics can help you to know it. So we spend a bit of time this term just learning about this reasoning before you actually learn about statistical tests next term."
  },
  {
    "objectID": "lectures/week02/handout/index.html",
    "href": "lectures/week02/handout/index.html",
    "title": "Lecture 2: What is this thing called \"Science\"?",
    "section": "",
    "text": "The title of this course is Psychology as a Science. But what do we actually mean when we say something is a “science”?\nScience is certainly something that is held in high esteem in public discourse. “Following the science” or “believing in science” is viewed positively. Conversely, “denying science” is viewed negatively. This positive view of science may come from the belief that there is something special about the “scientific method”. We can see this in the fact that “taking a scientific approach” is often equated with reliability.\nFigure 1: An example of a “We Believe” sign that was created in response to the election victory of Donald Trump\nSo we might want to say that science is a special way of learning about the world. But if it is special, then what makes it special?\nAnswering the question, “what is science” or “what is the scientific method” might seem like something that is easy to do. After-all, many of you might have a sense that you know what science is. A claim like “science is real” (e.g., see Figure 1) certainly implies some sense of knowing what science is, or at least being able to distinguish between science and non-science or pseudo-science. But when we try and clearly articulate this, we might find that it isn’t actually that easy. In fact, the problem of telling the difference between science and non-science is a problem that has plagued philosophers for a very long time. This is something known as the demarcation problem.1\nA lot of work on the demarcation problem has focused on examining methods—that is, it has looked at whether there is something like “the scientific method” that might mark science out as special. In this lecture, we’ll look at some of the prominent approaches for attempting to mark out something like “the scientific method”.\nYou might be asking yourself, “what is the point of this?” First, I’ll be clear about what the aim is not. The aim of this lecture is not to give you an answer to the question “What is science?” or “What is the scientific method?”. If I’m not going to answer these questions, then what am I going to do?\nThe aim of today is to get you to start thinking about what you think science is. I want you to apply that same kind of inquisitiveness that you might apply to thinking about a scientific problem to the problem of thinking about what science is. I want you to start thinking deeply about how we get to learn about the world–about what it means to know something about the world. I want you to do this, because I think in the long run it’ll make you a better scientist. I don’t expect you to have all the answers in your first year. As you progress in your scientific career—whether that lasts the duration of your degree or longer—you’ll learn more, and your views might change. Willingness to change your mind when new information becomes known is, after all, another feature often associated with science, so I expect your answer to the question “what is science?” to change as you learn more. So I don’t expect you to have the answer now, but I want you to start thinking about it."
  },
  {
    "objectID": "lectures/week02/handout/index.html#the-common-sense-view-of-science",
    "href": "lectures/week02/handout/index.html#the-common-sense-view-of-science",
    "title": "Lecture 2: What is this thing called \"Science\"?",
    "section": "The common-sense view of science",
    "text": "The common-sense view of science\nThe common-sense view of science might go something like this:\n\nScience is special because it’s knowledge based on facts\n\nBy framing science in this way, it’s often contrasted with other forms of knowledge that might be, based on authority (e.g., celebrities, religious and political leaders), revelation (e.g., personal religious or spiritual experiences), or superstition (e.g., “knowledge of the ancients”).\nBut by saying that science is based on facts immediately raises two questions: (1) If science is based on facts, then where do “facts” come from? (2) And how is knowledge then derived from these facts?\nThe common-sense view of science was formalised by two schools of thought: The empiricists and the positivists. Together they held a view that went something like this:\n\nKnowledge should be derived from the facts of experience\n\nWe can break this idea down:\n\nCareful and unbiased observers can directly access facts through the senses/observation.\nFacts come before, and are independent of, theories.\nFacts form a firm and reliable base for scientific knowledge.\n\nBut is this true?\n\nAccess through the senses\nFirst, let us think about the idea that we can directly access facts through the senses. A simple story about how the senses work is that there are some external physical causes (light, sound waves etc.) that produce some physical changes in our sense organs (e.g., our eyes). These changes are then registered by the brain.\nThis account implies direct and unmediated access to the world through our senses. But is this actually the case?\n\n\n\n\n\nFigure 2: An example of an ambiguous image\n\n\n\n\nThe image in Figure 2 is an example of an ambiguous image. This image could be seen as an old woman or a young woman. Some of you might see one and not the other. Some of you might be able to see both and switch between the two percepts. Whichever you see, the physical causes (the light hitting our eyes) is more-or-less the same for everyone. But you might “see” different things. Although this is just a toy example, it reveals a larger point: Two scientists might “observe” something different even when looking at the same thing.\nIn some fields, being able to make “observations” actually requires training. This training might, for example, be in how to make observations through a microscope, training in how to distinguish different kinds of behaviour, or training in how to read an x-ray. So a simple claim that observations are “unbiased” or “straightforwardly given by the senses” seems to be false.\n\n\nBut what do we even mean by “facts”?\nThe second part of the common-sense view of science has to do with “facts”. When we think of a “fact” there are two things we could mean. First, a “fact” could refer to some external state of the world. Or second, “fact”, might refer to statements about those external states. That is, things we say about the external world.\nThis distinction can be a little tricky, so here is an example. The fact that this university is in East Sussex could refer to this actual university and its actual being in East Sussex. That is, some external state of the world, independent of what anyone has to say about it or whether anyone has ever said anything about it. Alternatively, it might refer to the statement: “This university is in East Sussex.”\nWhen we talk about “facts” as the basis for science, we’re talking about these statements. That is, science is what we say about the world rather than just the world itself. We’ll call this type of fact—the things we say about the world—an “observation statement.”\nDo these facts come before theories? Like the common-sense view of science might suppose? Again, let us think of an example:\nThink of a child learning the word apple:\n\nThey might initially imitate the word “apple” when shown an apple by their parent.\nNext, they might use the word “apple” when pointing to apples.\nBut then one day they might see a tennis ball and say “apple”. The parent would then correct them, and show them that a tennis ball isn’t an apple because you can’t, for example, bite into it like an apple.\nBy the time the child can make accurate “observation statements” about the presence of apples they might already know a lot about the properties of apples (have an extensive “theory of apples”).\n\nAlthough this is just a toy example of a child learning about the world, we might see it as analogous to how a scientist learns about the world. The takeaway here is that formulating observation statements (the facts that form the basis of science) is no simple task. Formulating observation statements might require substantial background knowledge or a conceptual framework to place them in. So observation statements aren’t completely independent of theory.\nBut let’s say that we’ve been able to acquire some facts—whatever that might exactly mean. Will any old facts do?\nAgain, let’s take a simple example:\n\nYou observe that grass grows longer among the cowpats in a field.\nYou think this is because the dung traps moisture that helps the grass grow.\nYour friend thinks this is because the dung acts as a fertiliser\n\nObservations alone can’t distinguish these two explanations. To tell which is correct you need to intervene on the situation.\nFor example, you might grind up the dung so that it still fertilises the ground. Or you might use something else to trap the moisture. Intervening on the world, for example, through experiment allows you to tell what the relevant facts of your observation are.\nBy intervening on the system, we can tell which facts are relevant, but your scientific theories may play a part in helping to determine what is and isn’t relevant. We can see this with another example. This time from the history of cognitive psychology.\n\nIn certain kinds of reading tasks psychologists thought it was relevant that people made errors, but they didn’t think the exact nature of the errors was relevant.\nBut after certain kinds of theories were developed (ones based on neural network models) they came to realise that the particular kinds of errors (e.g., if people swapped letters between words) was relevant to understanding how people learn to read.\nThe nature of the errors which was once thought of as irrelevant now became relevant.\n\nIn short, observations can’t be completely divorced from theories."
  },
  {
    "objectID": "lectures/week02/handout/index.html#objectivity",
    "href": "lectures/week02/handout/index.html#objectivity",
    "title": "Lecture 2: What is this thing called \"Science\"?",
    "section": "“Objectivity”",
    "text": "“Objectivity”\n\nFacts don’t care about your feelings\n\n Guy on the internet\nWhat the preceding ideas are trying to get to is the idea of objectivity. But the idea that science is objective in a simple sense of “objectivity” is misleading. Your conceptual framework, theoretical assumptions, and even your knowledge and training, can play a part in your observations and can influence the observation statements you can make.\n“Objectivity” doesn’t mean observations free from theoretical assumptions (“the view from nowhere”). Objectivity is more complex.\n“Objectivity” does mean:\n\nPublicly and independently verifiable methods\nRecognising theoretical assumptions\nTheory/data that are open to revision and improvement\nFree from avoidable forms of bias (confounds, cherry picking data, experimenter bias)\n\nWe might also say that science is objective in the sense that despite all this, when you make the observations either the behaviour will happen or it won’t, the detector will flash or it won’t etc. Your theory can’t make things happen."
  },
  {
    "objectID": "lectures/week02/handout/index.html#deriving-theories-from-facts",
    "href": "lectures/week02/handout/index.html#deriving-theories-from-facts",
    "title": "Lecture 2: What is this thing called \"Science\"?",
    "section": "Deriving theories from facts",
    "text": "Deriving theories from facts\nThe last part of the common-sense view of science is that facts form the basis of scientific knowledge—that is, that scientific knowledge is derived from facts.\nUsually this idea of derived means something like logically derived. We might sum up the view like this:\n\nScience = Facts + Logic\n\n Guy on the internet \nTo understand what it might mean to logically derive scientific knowledge we need to know a bit about logic.\n\nDeductive logic\nA deductive argument is called valid if the conclusions follow from the premises. Let us take a look at two examples:\n\n\nExample 1\n\nAll research methods lectures are boring\nThis is a research methods lecture\n(Therefore) this lecture is boring\n\nIn this example, if we accept that (1) and (2) are true, then we have to accept (3) as true. We cannot accept (1) and (2) as true and then deny that (3) is true because we would be contradicting ourselves.\n\n\n\nExample 2\n\nMost research methods lectures are boring\nThis is a research methods lecture\n(Therefore) this lecture is boring\n\nIn our new example, we can accept (1) and (2) as true without accepting (3) as true. That is, (3) does not necessarily follow from (1) and (2). This might just be a case of a research methods lecture that isn’t boring.\n\n\nDeduction is only concerned with whether (3) follows from (1) and (2). It is not concerned with determining whether (1) and (2) are true or false. The argument assumes that (1) and (2) are true, but it doesn’t establish truth.\nThis means that conclusions can be false but valid.\n\n\nExample 3\n\nAll pigs can fly\nPercy is a pig\n(Therefore) Percy can fly.\n\n\nThe conclusion is valid. However, it is also false because (1) is false.\nIt is valid because if we accept (1) and (2) we have to accept (3)\n\n\nLogic only tells us what follows from what. If there is truth in our premises, then there is truth in our conclusions. If our premises are false, then our conclusions will also be false.\nDeductive logic is truth-preserving, but it can’t tell us what is true and what is false. And the conclusion is just a re-statement of the information contained in the premises. So deductive logic can’t create new knowledge. What can you do instead?\nTo create new knowledge we need a way to go from particular observations to generalizations. This process is called induction.\n\n\nInduction\nTo create new knowledge, we need a way to construct arguments of the following form:\nPremises\n\nEmily the swan is white\nKevin the swan is white\n… the swan is white\n\nConclusion\nAll swans are white\nBut the problem with arguments like this is that all the premises may be true and yet the conclusion can be false. Maybe we just haven’t observed the one swan that isn’t white?\nSo even though we might not be able to say whether an inductive argument is true, are we able to distinguish between good and bad inductive arguments?\nWe might, for example, say that more observations are better than fewer observations. But if so, then how many observations are enough? We might also want to say that observations need to be made in many different contexts. But what makes a context different”? And what kinds of differences are relevant? For example, we might want to say that different contexts should be novel in some sense. Or that we should not just make trivial changes. Finally, we might want to say that good inductive arguments have no contradicting observations. But where does this leave us with probabilistic phenomena, where the phenomenon of interest might not happen every time?\nClear and simple rules aren’t easy to come by. But the bigger problem is induction can never establish truth.\nSo how do we ever prove anything for certain in science?. The short answer is, we don’t. We can never be certain of truth. This might feel like it leaves us on very uncomfortable ground. And this is a realisation that has certainly troubled a lot of philosophers of science. As a result, some philosophers have tried to come up with a way to put science on a firmer logical footing. And this is where we turn our attention to next."
  },
  {
    "objectID": "lectures/week02/handout/index.html#falsification",
    "href": "lectures/week02/handout/index.html#falsification",
    "title": "Lecture 2: What is this thing called \"Science\"?",
    "section": "Falsification",
    "text": "Falsification\nInstead of just collecting confirmations we can use induction and deduction together (see Figure 3).\n\n\n\n\n\nFigure 3: Using induction and deduction together\n\n\n\n\nTo do this, we might collect observations and then use induction to come up with general laws and theories from these particular observations. We might then use deduction to figure out what logically follows from these general laws and theories.\nThis approach nicely captures the idea of testability. Our theories should make predictions about what we expect to find, and we can test these predictions with more observations.\nThe philosopher Karl Popper also saw trouble with relying on induction. He wanted to put science on a firmer logical footing. To do this, he proposed that while scientists can’t use deduction to figure out what is true, they can use deduction to figure out what is false! He suggested that a key quality of scientific theories is that they should be falsifiable.\nTheories can come into existence through any means (wild speculation, guesses, dreams, or whatever), but once a theory has been proposed it has to be rigorously and ruthlessly tested.\nTo see how falsification works in practise we’ll take a look at another example.\n\n\nConfirmation\nPremise: A swan that was white was spotted in London at time t\nConclusion: All swans are white.\nThe conclusion might be true or false, but it doesn’t logically follow from the premise.\n\n\n\nFalsification\nPremise: A swan, which was not white, was spotted in Australia at time t.\nConclusion: Not all swans are white.\nThe conclusion logically follows from the premise, so if the premise is true the conclusion is true.\n\n\nIn short, we can’t prove the claim “all swans are white”. But we can reject it.\nPopper also suggested that falsifiability can come in degrees. Good theories are falsifiable, better theories are more falsifiable.\nBelow we have some example theories:\n\nMars moves in an elliptical orbit\nMars and Venus move in elliptical orbits\nPlanets move in elliptical orbits\n\nOf these three theories, (1) is the least falsifiable and (3) is the most falsifiable. Why? For theory (1) only an observation of Mars could falsify it. But for theory (3), an observation of Mars, Venus, Saturn, Neptune, or any other yet undiscovered planet would falsify it. That is, there are more possible observations that can count against it.\nThis is in contrast to bad theories. Bad theories are ones that can seemingly accommodate any observation. If two outcomes are possible and the theory can explain both then this is bad. If it can account for both possible observations then what would be evidence against the theory?\nIn short, we can say that good theories are broad in their applicability but precise in their predictions!\n\nEncountering a falsifier\nOnce you have a theory that can be falsified, what actually happens when you make an observation that falsifies the theory? That is, what do you do when you observe something that contradicts the theory you’re testing. There are at least a couple of options.\nFirst, you could abandon your theory. But what happens if you have a probabilistic theory? Your theory might say that you should observe phenomena A and you fail to observe it, but your theory might also say that it might not occur every single time. And what of auxiliary assumptions? Maybe your observations rely on a brain imaging machine, and you have certain assumptions about how that machine works and it’s ability to actually detect the things you want to observe.\nAlternatively, if you don’t want to abandon your theory you might instead choose to modify or amend it. But are there better ways and worse ways of doing this? Let us dive into some of these issues in a little more detail.\nFirst, the issue of probabilistic theories. Theories in psychology often tend to be probabilistic. They make claims about how things are on average, not claims about how things are in every case.2 So knowing how to deal with probabilistic theories will be very important. Much of what we do with statistics is figuring out how to test and specify probabilistic claims. For example, what does it mean for things to be different on average? How many cases do you have to observe before you have evidence for a probabilistic claim? And how many cases do you have to observe before you have evidence against a probabilistic claim (that you might previously have believed).\nBut putting that aside, a single contradictory observation can’t falsify a probabilistic claim because we will sometimes expect contradictions with probabilistic claims.\nLet us, for now, just assume that you have a simpler non-probabilistic theory. Should contradictory observations now lead you to abandon it? You might not want to abandon your theory too quickly. Any experiment is not just testing one theory in isolation but also relies on a range of auxiliary assumptions and other support theories. For example, an experiment on memory using brain imaging is also making assumptions about the truth of theories related to physics and brain function, besides testing the theory about memory.\nIt may be the case that what is actually at fault is one of these auxiliary assumptions and not your theory. Telling which part of the interconnected web of theories is at fault can be tricky. Philosophers call this the Duhem-Quine problem. Popper didn’t have a good answer on how to figure out where to lay the blame for an apparent falsification. It’s certainly not an easy question to answer. But Popper also didn’t think that theories should be abandoned too quickly.\nInstead of quickly abandoning theories in the face of a falsifier, Popper instead suggested some dogmatism. He pointed out that early on in some scientific field scientists might still be figuring out the details of their theories and assumptions. Therefore, they might need to make some tweaks and modifications rather than simply abandoning theories completely.\n\n\nRevising and amending theories\nIf we do decide to amend a theory rather than abandoning it, then how do we do this? Are there good ways and bad ways to modify theories? Popper considered ad-hoc modifications to be bad. But he also thought that it was possible to come up with acceptable modifications. To see the difference between these two, let us examine the following theory:\nTheory: All bread is nourishing\nOnce we have this theory, we might make the following observation that seemingly falsifies it:\nObservation: Bread eaten in a French village in 1951 was not nourishing3\nNow that we’ve encountered the falsifying observation we can choose to make a modification to our theory. I might make a modification as follows:\nModification: All bread except bread eaten in a French village in 1951 is nourishing\nHowever, if we examine this modification, we can see that it has fewer tests. That is, the original theory can be tested by eating any bread. The modified theory can be tested by eating any bread except that particular bread. Because our modified theory has fewer tests than our original theory our modified theory is worse. Is there a way to modify theories that doesn’t make them worse? Wouldn’t it be better if our modifications could actually make the theory better? Here is another example of a modification to our theory:\nModification: All bread except bread made with flour containing ergot fungus is nourishing\nThis modification has made our theory better, because the modification leads to new tests. For example, we could now test the bread for the presence of fungus. Or we could cultivate the fungus and make bread with it and test whether it nourishes. Or we could analyse the fungus for poisons. All these new tests create new chances to encounter possible falsifiers and, therefore, our modification has improved upon our original theory."
  },
  {
    "objectID": "lectures/week02/handout/index.html#research-programmes",
    "href": "lectures/week02/handout/index.html#research-programmes",
    "title": "Lecture 2: What is this thing called \"Science\"?",
    "section": "Research programmes",
    "text": "Research programmes\nPopper’s approach is, however, not without its problems. His focus on falsifying theories leads to at least a couple of issues. First, it can be difficult to figure out when to abandon theories and when to amend theories. And when parts of the theory are abandoned or modified are all parts of the theoretical web of the same status?\nIt can also be difficult to compare two theories to see which is “better”. What I mean by this is, if you have Theory A and Theory B and neither has been falsified, which is the better theory? One might think that the theory with more confirming observations is better? But then won’t trivial theories always win? The philosopher Imre Lakatos developed his idea of research programmes4 as a reaction to these two problems. Lakatos came to his view by actually observing how science operates in the real world.\nOne key aspect of Lakatos’s idea of research programmes is that not all parts of a science are on par. Some laws or principles are so fundamental they might be considered a defining part of the science. And other parts of the science might be more peripheral and of lower importance.\nLakatos called these fundamental parts the hard core and the more peripheral parts the protective belt. He suggested that the hard core is resistant to falsification, so when an apparent falsifier is observed the blame is placed on theories in the protective belt. Research programmes are defined by what is in their hard core.\nWhat is in the hard core and what is in the protective belt might not always be explicit, but these might be some examples: In Cognitive Science the hard core might include the theory that the mind/brain is a particular kind of computational system and the protective belt might include specific theories about how memory works. Or in the biomedical view of mental illness the hard core might include the theory that mental illness can be explained biochemically, and the protective belt might include the dopamine theory of schizophrenia (see Figure 4 for some more examples).\nWhen apparent falsifications occur the protective belt is up for revision, but the hard core stays intact. Falsifying the hard core amounts to abandoning the research programme. But modifying the protective belt is more commonplace.\nOn Lakatos’s view, scientists work within a research programme. He split guidelines for working within a research programme into a negative and positive heuristic, specifying what scientists shouldn’t do but also what they should do. The negative heuristic includes things like not abandoning the hard core. The positive heuristic is harder to specify exactly, but it includes suggestions on how to supplement the protective belt to develop the research programme further. That is, the positive heuristic should actually specify a programme of research—they should identify the problems that need to be solved.\n\n\n\n\n\nFigure 4: Example of a research programme from Dienes (2008)\n\n\n\n\nLakatos was also interested in comparing research programmes, something that is difficult to do on a strictly falsificationist account. He divided research programmes into those that are progressive and those that are degenerating.\nProgressive research programmes are coherent. That is, they have minimal contradictions. Progressive research programmes make novel predictions that follow naturally from theories that are part of the programme. And these predictions are then confirmed by experiments.\nIn contrast, degenerating research programmes are those that have faced so many falsifications that they have been modified to the point of being incoherent. At this point, it’s no longer sustainable to carry on modifying the protective belt, and instead, the hard core must be abandoned!\nWhen the hard core is abandoned then scientists move from one research programme into a new one. We can see some examples of where this might have occurred in the history of psychology. For example, the move from psychological behaviourism to cognitive science might be one example. The move from classical cognitive science to embodied cognitive science, might be another. Other examples might be the move from connectionism to deep neural networks or from sociobiology to evolutionary psychology.\nBut again, what is and isn’t a research programme isn’t always clear, because often the hard core and the protective belt are left implicit and not made explicit. However, I think it’s valuable to keep these distinctions in mind as you move through your university career. All this might just help to make you a better scientist."
  },
  {
    "objectID": "lectures/week02/handout/index.html#whats-been-left-out",
    "href": "lectures/week02/handout/index.html#whats-been-left-out",
    "title": "Lecture 2: What is this thing called \"Science\"?",
    "section": "What’s been left out",
    "text": "What’s been left out\nThere’s so much more that I would’ve loved to have covered in this lecture, but unfortunately there simply isn’t the time. To take just a few: I would’ve liked to have been able to cover feminist perspectives on science (see Stanford Encyclopedia of Philosophy if you’re interested). And I would have also have liked to cover work on Race, Racism, and Colonialism—for example, see the work of Prescod-Weinstein (2022) if you’re interested.\nFinally, if you’re interested in Philosophy of Science in general, then a good book to start with is “What is this thing called Science?” (Chalmers, 2013), from which I took the title of this lecture."
  },
  {
    "objectID": "lectures/week02/handout/index.html#check-your-understanding",
    "href": "lectures/week02/handout/index.html#check-your-understanding",
    "title": "Lecture 2: What is this thing called \"Science\"?",
    "section": "Check your understanding",
    "text": "Check your understanding\n\n\n\n\n\n\n\n\nChalmers, Alan. F. (2013). What is this thing called sicence? University of Queensland Press.\n\n\nDienes, Z. (2008). Psyschology as a science. Palgrave MacMillan.\n\n\nPrescod-Weinstein, C. (2022). Making black women scientists under white empiricism: The racialization of epistemology in physics. Signs: Journal of Women in Culture and Society, 45(2). https://doi.org/10.1086/704991"
  },
  {
    "objectID": "lectures/week02/handout/index.html#footnotes",
    "href": "lectures/week02/handout/index.html#footnotes",
    "title": "Lecture 2: What is this thing called \"Science\"?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want to read a short digestible piece on the demarcation problem then you can check out the wikipedia page. If you’re looking for something more philosophically heavy, then you can check out the page on Science and Pseudo-Science at the Stanford Encyclopaedia of Philosophy↩︎\nA probabilistic claim might say something like on average “men are taller than women”, but of course there are shorter men and taller women. ↩︎\nThis is based on the true story of the Pont-Saint-Esprit mass poisoning. You can read more on wikipedia.↩︎\nA similar idea to Lakatos’s idea of research programmes was developed by the philosopher Thomas Kuhn. Kuhn used the term paradigms for his idea.↩︎"
  },
  {
    "objectID": "lectures/week03/handout/index.html",
    "href": "lectures/week03/handout/index.html",
    "title": "Lecture 3: Approaches to Research",
    "section": "",
    "text": "Doing research is an integral part of your training as a psychologist. But before you can start thinking about doing research you need to be aware of the different approaches that you can take. The aim of today’s lecture is to give you an overview of these approaches.\nIn this lecture I’ll primarily focus on two broad categories of approaches— qualitative methods, and quantitative methods–but at the end I’ll also briefly mention how computer simulation can be used in psychology research. The current module, as with most of the research methods modules you’ll take in your degree, is focused on quantitative methods. Therefore, this lecture will be the one chance we’ll have to discuss methods that don’t fit neatly under the label quantitative until you have the chance to take more advanced courses later in your degree."
  },
  {
    "objectID": "lectures/week03/handout/index.html#introduction-to-qualitative-and-quantitative-methods",
    "href": "lectures/week03/handout/index.html#introduction-to-qualitative-and-quantitative-methods",
    "title": "Lecture 3: Approaches to Research",
    "section": "Introduction to Qualitative and Quantitative methods",
    "text": "Introduction to Qualitative and Quantitative methods\nApproaches to research are ordinarily split into two broad categories. We can give some simple descriptions of these categories.\n\nQuantitative methods collect numbers/numerical data and use statistical tools.\nQualitative methods collect words, pictures, and artefacts.\n\nAs we’ll see, either of these approaches can be used to study a wide range of phenomena, and they don’t map neatly on to different subfields of psychology. Some researchers also adopt both approaches (mixed-methods) and some might apply quantitative methods to qualitative style data. Quantitative methods are probably easier to group together, because qualitative methods include such a broad range of approaches. For this reason we’ll start with quantitative methods first.\n\nOutline of quantitative methods\nQuantitative approaches take a phenomenon and try to condense it down into a few dimensions or variables that can be measured as precisely and reliably as possible. Because of this, it is very important to choose variables that are representative of the phenomenon you’re studying. This process of choosing variables that are representative of the phenomenon you’re interested in is called operationalisation.\nOperationalisation means choosing a measurable proxy for the phenomenon you’re interested in. This is not always an easy process and involves lots of careful thought by researchers. How one researcher operationalises a phenomenon might be different to how another researcher does. And debates between researchers on exactly how to operationalise some phenomena can be common in the literature. Operationalisation will be an important part of your lab report.\nQuantitative approaches often make use of statistical methods. Using statistical methods means looking at lots of cases by, for example, studying lots of people, and not just one or two. The goal with quantitative methods is often to develop generalisations, or theories that are generally applicable. This involves testing predictions that logically follow from theories (the deductive step).1\n\n\nOutline of qualitative methods\nQualitative methods are focused on meaning rather than measurement. Instead of condensing a phenomenon down to a simple set of features dimensions (as is common in quantitative approaches), qualitative research tries to examine many features of a phenomenon. Qualitative approaches can do this, because instead of measuring many instances of a single variable, qualitative approaches try to look at all aspects of one or a few instances.\nQualitative approaches view the context (physical environment, social setting, cultural context) as a central part of the phenomenon being studied. In contrast, it is common for quantitative approaches to place less emphasis on the context by abstracting away from it.\nQualitative approaches—for example, grounded theory and phenomenology—also emphasise the idea of following the data wherever it leads (that is, the inductive step).\nQualitative methods is an umbrella term for a wide range of different methodologies. Each of these will have their own underlying theoretical assumptions and intellectual histories. I can’t do justice to them all in one short lecture. In fact, just one of these approaches would require an entire course to itself if you were to learn enough about it to be able to apply it to your own research. So instead what I’ll do is just pick out a few and highlight their key features.\nThe methods I have chosen are: (1) Verbal protocol analysis, (2) ethnographic methods, (3) discourse analysis, and (4) phenomenology. However, there are many more, including Case Studies, Grounded Theory, Participatory Research, and Focus Groups, to name just a few.\nI’ll try to draw out some contrasts between qualitative and quantitative methods more generally and highlight strengths and weaknesses of each approach."
  },
  {
    "objectID": "lectures/week03/handout/index.html#qualitative-methods",
    "href": "lectures/week03/handout/index.html#qualitative-methods",
    "title": "Lecture 3: Approaches to Research",
    "section": "Qualitative methods",
    "text": "Qualitative methods\n\nVerbal protocol analysis\nAlso known as “thinking aloud protocols” (or “talking aloud protocols”). Verbal protocol analysis, involves collecting and analysing verbal data on cognitive processing.\nA typical setup might involve participants being given a task (usually a task that involves multiple steps chained together). They are asked to verbalise (speak aloud) what they are thinking as they go about solving the task. Finally, the data (i.e., recordings of what the participant said) are coded and analysed to infer the information processing steps involved in solving the problem. Consequently, verbal protocol analysis can carry certain assumptions about the nature of human cognition/thinking—for example, that it involves information processing in discrete sequential steps. For an example of verbal protocol analysis, see Table 1.\n\n\n\n\nTable 1: Example Verbal Protocol Analysis\n\n\nSegment\nContent\nPlan level\nPlan type\n\n\n\n\n1\nOK… the first thing I would do is to make a list of the shops that are quite close to each other\nExecutive\nGenerate plan\n\n\n2\nand highlight the dance class remembering that it is at a specific time\nMetaplan\nSatisfy time constraints\n\n\n3\nI would try to get to it first and get it over with…\nExecutive\nOrder messages\n\n\n4\nprobably, in reality I would drop it….\nExecutive\nEvaluate plan Eliminate\n\n\n\n\n\n\n\n\nAlthough it might be common to associate qualitative methods with subfields of psychology like social psychology and quantitative methods with subfields like cognitive psychology, the qualitative–quantitative division does not map onto subfields this easily. Verbal protocol analysis is one example of a qualitative approach that has a long history in cognitive psychology. The approach was used in early cognitive science by Simon and Newell who were pioneering researchers in Cognitive Science and Artificial Intelligence.\n\n\nEthnography\nMore a style of research than a method of data collection, ethnography involves studying people in “the field” (i.e., their naturally occurring setting), and requires the researcher to enter into the setting they are studying. It attempts to understand how the socio-cultural practices and behaviours of people are shaped by their social, physical, and cultural contexts. And it tries to make sense of events from the perspective of the participants. Ethnography could include data from interviews, or participant observation. In auto-ethnography, researchers engage in critical self-reflection and treat themselves as the participant.\nIn cognitive psychology, ethnographic approaches have been used to understand how people solve problems in real-world settings. For example, how do technological artefacts (that is, the context) support cognitive processing. One famous study in cognitive ethnography involved studying how sailors use technological artefacts (instruments etc) and the layout of a ship to help them navigate.\nFinally, in critical psychology, ethnographic approaches have been used to understand the interplay between, race, class, gender, and education in shaping participants’ life worlds.\n\n\nDiscourse analysis\nDiscourse analysis is the social study of language as used in talk, text, and other forms of communication. It involves a distinctive way of thinking about talk and text where language doesn’t just represent the world but also constructs the world.\nSome questions one might examine with this approach include: How does language shape social relations? For example, how might certain kinds of talk establish professional distance in doctor-patient communication? How might language construct or open up space for particular identities. For example, how might language enforce or break down the concept of binary gender?\nThe strengths of this approach are that it allows you to examine how language constructs reality. It can make use of primary data (interviews, talk in focus groups) or secondary data (books, newspaper articles). But it can be difficult to use discourse analysis to develop the same kind of generalisations as you might develop with other approaches.\n\n\nPhenomenology\nThis approach is associated with the philosophers Husserl, Merleau-Ponty, and Sartre. The phenomenological approach involves bracketing off any preconceived notions we might have about a phenomenon to achieve an understanding of that phenomenon that has not been influenced by our prior beliefs. Phenomenology emphasises peoples first-hand experience and attempts to understand and describe subjective experience from the participant’s point of view.\nIn fields like cognitive psychology, phenomenology has been used to understand the nature of subjective sensory experiences, skilled actions, and even the nature of cognition itself (e.g., it has been used to argue against the computational theory of mind).\nTo draw out the distinction between, for example, a phenomenological and an ethnographic approach we can look at the example of studying an inclusive classroom setting. A phenomenological approach might try to understand what it is like for a student with a disability to be in that classroom setting. While an ethnographic approach might look at how the classroom setting changes interactions between students with and without disabilities.\n\n\nIssues in qualitative research\nUnlike quantitative methods that might use printed questionnaires or computers to record and measure responses, in qualitative research, the researcher is the instrument. For this reason, it is important for researchers to reflect on their values, assumptions, biases, and beliefs to understand how these might impact the research. Because the researcher is the research instrument, the research instrument can change during the course of the research. For example, in ethnographic research, the changes in the researchers experience might alter how they record and observe behaviours.\nThere are parallels to validity (internal and external), reliability, and “objectivity” in qualitative research2. These are Credibility, Transferability, Dependability, and Confirmability.\nCredibility means: Can the data support the claims? It can be established through prolonged engagement, through discussions with other researchers/participants, and through critical self-reflection. Transferability means: Can the findings be transferred to similar contexts? Establishing transferability requires extensive, detailed, and careful descriptions of the research context (what are called “thick descriptions”). Dependability means ensuring that researchers maintain a record of changes in the research process or research instrument (i.e., themselves) over time. And finally, confirmability is concerned with ensuring that the data used to support the conclusions are verifiable."
  },
  {
    "objectID": "lectures/week03/handout/index.html#quantitative-methods",
    "href": "lectures/week03/handout/index.html#quantitative-methods",
    "title": "Lecture 3: Approaches to Research",
    "section": "Quantitative methods",
    "text": "Quantitative methods\nAs the name suggests, a key aspect of quantitative methods is quantification. Quantification means putting numbers to the thing we’re interested in studying so that it can be measured. The motivation behind measuring phenomena is that measurements are publicly available and verifiable (e.g., other scientists can check or verify your measurements). Unlike qualitative research, where researchers try to simultaneously study many aspects of a single phenomenon, quantitative research tries to condense a phenomenon down into a single (or a few) dimension(s).\nThe first step in quantitative research is often figuring out how to quantify the phenomenon of interest. This involves choosing a proxy (something measurable) that can stand-in for the phenomenon. This process is called operationalisation.\n\nOperationalisation\nIf you’re interested in anxiety, you have to decide how to measure anxiety, because you can’t measure an abstract concept directly. The process of choosing a proxy is known as operationalisation. There are lots of ways to choose a measurable proxy that can stand in for anxiety. For example, you could develop a scale or a questionnaire. Or you could measure physiological responses like increased heart rate or galvanic skin response.\nMeasurements have to be reliable (reproducible) and valid (actually measure what you think you’re measuring). This means that if we were to develop a scale for depression then the scale must produce similar numbers when applied to the same person or to different people who are similarly depressed. And if we were to use this scale to assess a treatment for depression then the treatment should not just reduce scores on our depression scale, but it must also result in people experiencing less depression.\n\n\nQuantitative methods and causation\nUnlike qualitative research, which studies phenomena in the wild, quantitative approaches try to exert a lot of control over phenomena. This control allows researchers to make claims about causation and give causal explanations. There are a few ways to understand causation, and thinking about what causation means will help us to think through ways to examine, study, or identify it:\nOne view of causation can be summed up as a difference that makes a difference: If you take two situations, one in which the phenomenon occurs and another in which it does not occur, then whatever is different between those situations is the cause of the phenomenon. For example, take one situation in which a window is broken and another in which a window isn’t broken. If the only difference between them is that in one a boy has thrown a rock and in the other a boy has not thrown a rock then a boy throwing a rock is the cause of the broken window.\nYou can also understand causation in terms of manipulation: If you can manipulate one thing and observe a change in another, then the two things may be causally connected. For example, as I put my foot down or lift it from the accelerator pedal in a car I can observe a change in the speed of the car, so I know the accelerator pedal and the speed of the car are causally connected. By intervening and manipulating parts of a system you can identify how they work (you can identify mechanisms).\nFinally, causation can also be understood in terms of probability: If the presence of one thing increases the probability of the other thing occurring, then there may be a causal relationship. For example, the presence of smoking increases the probability of developing cancer, so smoking may be the cause of cancer.\nIn the examples above, they are all examples of possible causes To be justified in claiming a causal relationship other conditions must usually be met. And causal claims are not always black and white. Sometimes we can only be more or less sure about causal relationships.\nThe control of confounds is an important part of trying to understand causal relationships. To see the role of confounds in understanding causal relationships, let us look at the example of smoking and cancer. The presence of smoking increases the probability of developing cancer, so smoking may be the cause of cancer. But having emphysema also increases the probability of developing cancer. So is emphysema the cause of cancer?\nThere is a plausible mechanism of action between smoking and cancer but not between emphysema and cancer, so we can be more sure that smoking causes cancer than we can be about emphysema causing cancer. A more likely explanation is that emphysema and cancer have a common cause—smoking.\nLet’s say you are studying the relationship between emphysema and cancer, because you think emphysema might cause cancer. In this situation, smoking is a confound. If you wanted to see whether emphysema caused cancer then you’d have to control for smoking. To do this, you might only look at smokers and see if there’s still a relationship between emphysema and cancer or whether cancer also occurs in the absence of emphysema. Or you might only look at non-smokers and see whether emphysema and cancer are still related or whether cancer develops in the absence of emphysema.\nEmphysema and cancer are correlated (the increase in one is associated with an increase in the other), but emphysema doesn’t cause cancer because they have a common cause. Sometimes two correlated variables have a causal relationship, such as the correlation between smoking and cancer. Sometimes they have a common cause—such as the correlation between developing emphysema and cancer. And sometimes, they have neither. For example, the correlation between the number of men getting engineering degrees and per capita consumption of sour cream (see Figure 1).\n\n\n\n\n\nFigure 1: An example of a spurious correlation"
  },
  {
    "objectID": "lectures/week03/handout/index.html#qualitative-vs-quantitative-methods",
    "href": "lectures/week03/handout/index.html#qualitative-vs-quantitative-methods",
    "title": "Lecture 3: Approaches to Research",
    "section": "Qualitative vs Quantitative methods",
    "text": "Qualitative vs Quantitative methods\nIn qualitative research, you study phenomena in context while in quantitative research you aim for control. But you can use either approach to study the same phenomena and psychological processes. Let’s say you’re interested in memory: How could you study memory from a qualitative perspective? And from a quantitative perspective?\nWhen using a quantitative approach you might use experiments in a lab where you give people lists of words to remember. You could manipulate aspects of the words—for example, their emotional salience—and measure performance (accuracy scores) to try to understand something about memory and emotional salience.\nYou would need to ensure that the only thing that differs between the words on each list is the emotional salience. And you would need to control for possible confounds like: Word length, by ensuring that one list doesn’t contain long words and the other short words. And Word order, by making sure that some people get the lists in one order and some in the other order, to ensure that memory differences aren’t a result of people getting tired from the experiment.\nFor a qualitative approach you don’t want to study memory in the lab—you want to study it in the wild. This allows you to ask different kinds of questions. For example, you could use an ethnographic approach with, for example, bartenders. You might do fieldwork in a bar observing bartenders. Through this, you might see that bartenders structure their environment in a particular way—e.g., put certain types of glasses or bottles in particular places. This might lead you to form the hypothesis that bartenders structure their environment to support their memory—i.e., placing certain bottles and glasses together helps them remember what goes in what kinds of cocktails.\nYou might then conduct follow-up interviews or discussions with bartenders or observe the training of bartenders, which might provide further evidence for this hypothesis. You might also engage in bartending yourself and critically reflect on your own experience to understand how this environmental structuring supports memory."
  },
  {
    "objectID": "lectures/week03/handout/index.html#computer-simulation",
    "href": "lectures/week03/handout/index.html#computer-simulation",
    "title": "Lecture 3: Approaches to Research",
    "section": "Computer simulation",
    "text": "Computer simulation\nQualitative and quantitative methods try to understand phenomena by studying the phenomena themselves. The data they use comes from the world. In approaches like computer simulation and formal/mathematical modelling researchers instead generate the data. To do this, researchers try to build systems that replicate or reproduce some aspects of systems or phenomena they are studying. Doing so might allow them to gain new insights into these systems. And comparing the behaviour of their artificial systems with the natural system allows researchers to test theories about the processes that produce phenomena.\nComputer simulation has been used to study a lot of different phenomena in psychology. However, the examples that I have found particularly interesting are cases where simulation has been used to show how seemingly complex behaviour can arise from very simple processes. For example, flocking behaviour in birds seems very complex, and it looks as if there must be something very complex going on inside their brains. But you can simulate this behaviour with only three simple rules:\n\navoid collisions with other birds\nalign direction with nearby birds\napproach distant birds\n\nYou can use Explorable 1 to explore how these three rules influence the simulated flocking behaviour in birds.\n\n\n\n\n\n\n\nExplorable 1 (Explore flocking behaviour)  \n\nUse the sliders to make adjustments to each of the three rules that determine flocking behaviour in the simulated birds.\n\n\n\nimport {viewof boidsdisplay} from \"@ljcolling/boids\"\nviewof boidsdisplay\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport {viewof avoidanceStrength} from \"@ljcolling/boids\"\n\n\n\n\n\n\n\nimport {viewof alignmentStrength} from \"@ljcolling/boids\"\n\n\n\n\n\n\n\nimport {viewof cohesionStrength} from \"@ljcolling/boids\"\n\n\n\n\n\n\n\nviewof avoidanceStrength\n\n\n\n\n\n\n\nviewof alignmentStrength\n\n\n\n\n\n\n\nviewof cohesionStrength\n\n\n\n\n\n\n\n\n\n\n\nAnother example of simple rules giving rise to complex patterns of behaviour is Conway’s Game of Life, which is a very simplified version of an Agent-Based Model. Conway’s Game Of Life has 4 simple rules:\n\nA live cell with 2 or 3 live neighbours lives on\nA live cell with &lt; 2 live neighbours dies (underpopulation)\nA live cell with &gt; 3 live neighbours dies (overpopulation)\nA dead cell with exactly 3 live neighbours becomes a live cell (reproduction)\n\nWith just these 4 simple rules Conway’s Game of Life can exhibit some very complex behaviour. You can explore the Game of Life in Explorable 2. You can learn more about Conway’s game of life on the scholarpedia page, co-written by Anil Seth, a neuroscientist at the University of Sussex.\n\n\n\n\n\n\n\nExplorable 2 (Explore the Game of Life)  \n\nYou can use the slider to select some of the more well known patterns. These patterns produce some interesting behaviour such as oscillating at particular rates. You can adjust the Generation time slider to speed up the simulation.\nEach time you select pattern 6, a new random set of live and dead cells will be produced. These random patterns might produce some interesting behaviour, or they could just burst into live and then die off!\n\n\nimport {gun} from \"@ljcolling/game-of-life\"\nimport {viewof n} from \"@ljcolling/game-of-life\"\nimport {viewof delay} from \"@ljcolling/game-of-life\"\nimport {gol_des} from \"@ljcolling/game-of-life\"\ngun\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof n\n\n\n\n\n\n\n\nviewof delay\n\n\n\n\n\n\n\n\n\ngol_des\n\n\n\n\n\n\nYou can read about some of the more interesting patterns on the LifeWiki.\n\n\n\nFinally, agent-based modelling takes a cue from approaches like those used to model bird flocking and Conway’s Game of Life. In an agent-based model, the researcher simulates a group of ‘agents’. The ‘agents’ will typically have some memory, a set of goals, and some rules. The memory allows them to store their current state or consequences of their previous actions. The goals usually represent some state they’re trying to achieve. And rules govern their interactions. By allowing these agents to interact, and by manipulating aspects of the agents (their memory, goals, and rules) it is possible to see how social phenomena can arise.\nAgent-based modelling can be used for modelling a wide range of social phenomena. For example, it could be used to model the spread of misinformation through social groups. If you thought that misinformation was more likely to spread if passed on by particularly influential individuals (e.g., celebrities or politicians), then you could examine this through an agent based model. Or if you thought that misinformation was more likely to spread inside socially isolated groups, then you could modify your simulation to create socially isolated groups to test this hypothesis. After your simulation, you could still go and check the real world to see if it behaves like your simulation. You’re unlikely to do any computer simulations as part of your undergraduate degree, but if it’s something you find interesting, then you might just get a chance to do some as part of your final year dissertation. My aim in introducing it here is really just to give you an idea of the broad range of approaches that are applicable to psychological research."
  },
  {
    "objectID": "lectures/week03/handout/index.html#test-your-knowledge",
    "href": "lectures/week03/handout/index.html#test-your-knowledge",
    "title": "Lecture 3: Approaches to Research",
    "section": "Test your knowledge",
    "text": "Test your knowledge"
  },
  {
    "objectID": "lectures/week03/handout/index.html#footnotes",
    "href": "lectures/week03/handout/index.html#footnotes",
    "title": "Lecture 3: Approaches to Research",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRefer to Lecture 2 if you’re unsure what deductive means.↩︎\nWe’ll touch on these topics today, but you’ll also learn more about these concepts in coming lectures↩︎"
  },
  {
    "objectID": "lectures/week04/handout/index.html",
    "href": "lectures/week04/handout/index.html",
    "title": "Lecture 4: Introduction to study design",
    "section": "",
    "text": "As I said in the previous lecture, the focus of this course, and most of the undergraduate research methods courses that you will take, will be on quantitative methods. Today will be the start of our journey of learning more about the specifics of study design for quantitative research.\nQuantitative research is all about measurement, and the aim of quantitative research is to take a phenomenon and condense it down into a few variables that can be measured as precisely and reliably as possible. We also saw that quantitative research often made use of statistical methods, and that the aim of quantitative research was often to develop generalisations or generally applicable theories. In today’s lecture, we’re going to learn a bit about how to design quantitative studies so that all these things are possible."
  },
  {
    "objectID": "lectures/week04/handout/index.html#the-how-of-quantitative-research",
    "href": "lectures/week04/handout/index.html#the-how-of-quantitative-research",
    "title": "Lecture 4: Introduction to study design",
    "section": "The “how” of quantitative research",
    "text": "The “how” of quantitative research\nThe conclusions that we can draw from our research depends on how that knowledge was generated and the research design that was used. For any piece of research that we conduct (or piece of research that we read), we need to be able to answer several questions: 1) How do we actually test hypotheses appropriately? 2) How do we generalise our findings? 3) How do we quantify seemingly unquantifiable things? The answer to these questions lies in research design.\n\nResearch design\nWe can use different types of research design to answer different research questions and to test different hypotheses.1 Study designs can vary along many dimensions. For example, study designs can differ on whether they include a manipulation or not. They can differ on whether they use a between-subjects or within-subjects measurement. And they can differ with respect to the time frame used for data collection.\nStudy designs are usually broken into two broad categories: Experimental and non-experimental designs. The key feature of experimental designs is that there is some form of manipulation. By manipulation, we just mean some sort of change that is introduced that may have some impact on the thing our study is measuring. The manipulation might be something that is intentionally introduced by the experiment, or it might be some change that is naturally occurring. We’ll see that this will be the difference between true experiments and natural experiments.\nNot all possible designs include manipulations. Designs that don’t include manipulations are known as observational or correlational designs. In correlational designs we rely on observed data that are not subjected to any experimental manipulation. By observational, we don’t mean that we’re just looking. There is still measurement, whether this is through questionnaires, lab tasks, or similar.\nCorrelational designs have several practical advantages over experimental designs. These include the ability to collect data from far more people than would be practical for an experimental design. And they also allow us to study phenomena where experimental manipulations would be unethical. But correlational designs also have drawbacks when compared with experimental designs. In particular, while correlational designs can be used to investigate relationships between variables, but in order to make causal claims, experimental designs (whether true experiments or natural experiments) are needed."
  },
  {
    "objectID": "lectures/week04/handout/index.html#an-example-ice-cream-and-murder",
    "href": "lectures/week04/handout/index.html#an-example-ice-cream-and-murder",
    "title": "Lecture 4: Introduction to study design",
    "section": "An example: Ice cream and murder",
    "text": "An example: Ice cream and murder\nIn Figure 1 we can see a plot of the relationship between the murder rate and the number of ice creams sold in New York City. We can see from the plot that when the murder rate is high then the number of ice cream sales is also high and when the murder rate is low then the number of ice creams sold is also low. But are they just correlated or does ice cream cause murder?\n\n\n\n\n\nFigure 1: The relationship between the murder rate and ice cream sales in New York City\n\n\n\n\nWe might decide to conduct some research into this relationship between ice cream and murder to see whether there actually is some sort of causal relationship. Obviously, there are many ethical issues with conducting an experiment on this, but let’s put those aside for now and see what such an experiment might actually look like.\nThe first step in our experiment would be to specify our research question. In this example, our research question might be something like: Does eating ice cream make you more prone to murderous tendencies? From this we can now formulate our hypothesis. In our hypothesis we will actually specify the outcome we expect. Our hypothesis might be something like: Eating ice cream increases the desire to commit murder.\nTesting our hypothesis with an experiment might involve something like the following steps: First, we might invite a group of people into the lab. Half these people will be given some ice cream to eat, and the other half will not be given ice cream. This is our manipulation. Next, we might get all of our participants to look at several images of people (our stimuli) while we get to rate how much they want to eliminate them on a scale of 0 (no desire) to 9 (all the desire possible).\nIn this study, we have an independent variable (IV) or predictor variable. This is the thing that we’re manipulating. In our case, this is whether people ate ice cream or not. We manipulated our IV by assigning people to one of two groups: The ice cream condition or the no ice cream condition. We also have one dependent variable (DV) or outcome variable. In our case, this would be participants total score on our desire to eliminate measure.\nAfter we have all our data we could then compare the results between the ice cream condition and the no ice cream condition to see which group gave higher ratings for their desire to eliminate people (the outcome or DV).\n\n\n\n\n\n\nIndependent and Dependent Variables\n\n\n\nThe terms independent and dependent variable are two pieces of jargon that are important to learn. You’ll come across them a lot in your research methods modules (and in other modules like Cognition in Clinical Contexts).\nThe dependent variable is the variable that you analyse. Its value depends on the values of other variables.\nAn independent variable is a variable that influences the values of your dependent variable.\nIn our ice cream example, the variable we’re measuring is the total score on the desire to eliminate measure. This is our dependent variable. We expect this to be influenced by whether somebody was given ice cream or not. Because we expect ice cream to influence our dependent variable, the presence, or absence, of ice cream is our independent variable.\nIndependent and dependent variables might be measured at any level of measurement, and they might be continuous or discrete.\nIn the context of regression analyses (which you’ll learn about later) the DV is sometimes also called the outcome, and the IVs are sometimes also called the predictors."
  },
  {
    "objectID": "lectures/week04/handout/index.html#features-of-good-study-design",
    "href": "lectures/week04/handout/index.html#features-of-good-study-design",
    "title": "Lecture 4: Introduction to study design",
    "section": "Features of good study design",
    "text": "Features of good study design\nOur study of the relationship between murder and ice cream is an example of a very unethical study, but it is also a poorly designed study. In a well-designed experiment, we can be confident in saying our manipulation caused a change in our outcome variable. But this isn’t the case for our ice-cream study. Let’s take a look at some features of good study design to see why.\n\nControls\nOur imaginary study didn’t use any controls. We recruited all kinds of people without giving consideration to how different characteristics might affect our results. Let’s say, for example, that all the people that we recruited were actually lactose intolerant and that for these people eating ice-cream caused a great deal of discomfort. Perhaps it is actually this discomfort that caused their murderous rage and their murderous rage is not specific to ice cream per se. It could equally well be caused by drinking milkshakes.\nWe might also not have had a standardised set of instructions that we gave to participants. Some participants might have arrived very hungry and other participants might have arrived very full. Maybe the differences that we observe are actually a result of some participants being hangry.\nMaybe we didn’t control our IV appropriately. We might have often changed the brand or the flavour of the ice cream. Some days we might have run out of ice cream and used frozen yoghurt instead. And some days we might have given a small amount of ice-cream and on other days we might have forced our participants to eat a large amount of ice cream. If we had so many changes from participant to participant then it would be difficult to know exactly what caused any changes in our outcome.\nWe might also not have controlled the lab environment adequately. On some days the heating might have been up really high. And other days the air-conditioning might have been up really high instead. Maybe it’s just the heat that’s driving people into a murderous rage?\n\n\nRandomisation\nAnother feature that might have been missing from our study is that we didn’t randomly assign people to groups: Maybe we did our participant recruitment first at a dentist’s office and then at a supermarket. And maybe all participants recruited first were assigned to the ice cream condition with the second batch of participants assigned to the no ice cream condition. Because of this, it might so happen to be the case that all the participants in the ice cream condition just so happened to have sensitive teeth. This would affect the results by giving us the impression that ice cream increases murderous tendencies when it doesn’t. To deal with these issues, participants should have been sorted into groups randomly. A well-designed experiment would randomise the participant allocation and the stimulus presentation order (in our experiment, this refers to the order in which the images of faces were rated).\n\n\nBlinding\nIn our study, we might have told participants that we were interested in the effect of ice cream on murderous tendencies. And we might have also given participants the ice cream ourselves. Participants may have (consciously or not) modified their behaviour to either fit the hypothesis or to contradict the hypothesis. Because of this, it is crucial that participants are unaware of what the hypothesis was and also what condition they have been allocated to. If participants are naïve to group allocation then the study is to be single-blind. If neither the participants nor the researcher know which condition the participants are put in, then the study design is known as double-blind. In such a case allocation might be recorded but only revealed once the study is over and the data are being analysed.\n\n\nTheoretical framework\nThe choice of predictor (IV) and outcome (DV) variables does not happen in a theoretical vacuum. Rather, these choices are based on theory. In our experiment, the decision to have ice cream as the IV and murderous tendencies as the DV was not based on any theory. It could be that committing murder causes people to eat ice cream. In which case, we should probably have swapped around our IV and DV. Or it might be that they’re completely unrelated. To have good reasons for running the experiment we did, we would probably have to be able to tell some kind of a plausible story about how eating ice cream a murder were related. Such a story would probably have to make reference to some kind of psychologically or biologically plausible mechanism that might explain the connection."
  },
  {
    "objectID": "lectures/week04/handout/index.html#types-of-experimental-studies",
    "href": "lectures/week04/handout/index.html#types-of-experimental-studies",
    "title": "Lecture 4: Introduction to study design",
    "section": "Types of experimental studies",
    "text": "Types of experimental studies\nWe’ve already talked about experimental designs through our example. But it’s worth spending a little more time talking about different kinds of experimental studies.\n\n(True) Experiments\nFirst, we have true experimental studies. True experiments usually have tight controls. As a result, they can be somewhat artificial, because they abstract away from reality. This means that they can sometimes lack something called ecological validity. Ecological validity refers to the ability to generalise the results of the study to real-life settings. That is, just because something is true in the lab, doesn’t necessarily mean that it will be true in “the real world”. Experimental designs do, however, provide the most rigorous methodology for investigating causal relationships.\nAs we saw in our example, good experimental design requires randomisation, manipulation, and adequate controls. For many research questions there are methodological, logistical, and ethical obstacles that make it infeasible to conduct experiments. In these cases, we might want to use a quasi-experimental design or a natural experiment.\n\n\nQuasi-experiments\nQuasi-experimental designs are similar to (true) experimental designs except for participant randomisation. As a result, they’re used in situations where it’s not possible to randomise the allocation of participants into groups. An example of a study using a quasi-experimental** design might be one looking at the effectiveness of attending summer school. If one school offers summer school, but another does not, then we can’t randomise students into the intervention. In situations like this, we should still try to match the participants so that the groups don’t differ on characteristics that might be relevant to our outcome, except, of course, for the characteristic we’re actually interested in—attending summer school.\n\n\nNatural experiments\nNatural experiments are studies where randomisation and manipulation occur through natural or socio-political means. A good example of a natural experiment might be twin studies. Identical twins share 100% of their genes while fraternal twins only share on average 50% of their genes. However, both kinds of twins tend to share the same home environment (if they’re raised together). By comparing the degree of similarity (on some characteristic) between identical and fraternal twins we might be able to estimate the degree of variation in a characteristic that is due to genetic variation. Other examples of natural experiments might be made possible by differences or changes in government policy: For example, bans on cigarette advertising, or differences the in length of compulsory education. Natural events, or even natural disasters, might create the manipulations that natural experiments can be based on."
  },
  {
    "objectID": "lectures/week04/handout/index.html#aspects-of-study-design",
    "href": "lectures/week04/handout/index.html#aspects-of-study-design",
    "title": "Lecture 4: Introduction to study design",
    "section": "Aspects of study design",
    "text": "Aspects of study design\nIn our ice cream example, we used a between-subjects design, but this isn’t the only option available to us. And we just collected our data at one point in time. And this too isn’t the only option available to us.\n\nWithin-subjects and between-subjects designs\nIn a between-subjects or independent design we compare different groups of participants. For example, in our ice cream study, one group of participants ate ice cream while the other group of participants did not.\nBetween-subjects designs can be contrasted with within-subjects or repeated measures designs. As the name suggests, this kind of design involves repeatedly measuring participants, but under different conditions. We can show the distinction by way of an example. Let’s say that I conduct a study where I’m interested in whether people are better at remembering long words or short words. A between-subjects design would involve getting one group of participants to remember long words and another group of participants to remember short words. With a within-subjects design, however, all participants would be asked to remember the words under both conditions (short words and long words). We’d measure their memory for words under both of these conditions.\nWith within-subjects designs there are some additional things than we will need to consider. For example, if participants perform one condition first followed by the other condition, then we might need to consider whether there are order effects present. It might be that participants are fatigued by the time they get to the later condition and as a result they perform worse on it. Or it might be the case that by the time they get to the later condition they have already had some practise with the task and, therefore, they might perform better.\nFinally, we can mix within-subject manipulations with between-subject manipulations. This is what is known as a mixed-design. To use our memory example above, an example of a mixed-design might be as follows: First, we might split our participants into two groups and give one group some special memory training. All participants would then be asked to perform the task. We could then examine whether the influence of the memory training made a difference to whether people were better at remembering short words over long words.\n\n\nTime frame\nStudies can also vary in terms of whether participants are measured at one point in time or whether they are followed over time.\nIn our ice cream example we took our measurements at one point in time. Designs where measurements are made at one point in time are called cross-sectional designs. Cross-sectional studies are logistically easier compared to other types of studies, but they’re not very useful in telling us how phenomena change over time. For that, we need another kind of design.\nAn alternative to a cross-sectional design is a longitudinal design. This kind of design involves repeated measurement of the same characteristic of the same participants at multiple time points. A classic example of this would be studies that follow participants over time to understand the nature of developmental changes in, for example, language. The logistics involved in longitudinal designs can make them difficult to carry out. This is particularly true where studies carry on for several decades. They can also be time-consuming and expensive. And, finally, the risk of missing data (for example, from participants dropping out of the study) is high. Missing data can be difficult to deal with. Sometimes data can be missing at random. However, it’s also possible that data can be missing because participants with, for example, particular characteristics are more likely to drop out. Consider the case of a study looking at the effectiveness of online dating apps: If all the participants that find true love drop out of the study then your dataset would only consist of data from participants that were unlucky in love. As a result, the data would show a 0% success rate for online dating apps!"
  },
  {
    "objectID": "lectures/week04/handout/index.html#issues-in-measurement",
    "href": "lectures/week04/handout/index.html#issues-in-measurement",
    "title": "Lecture 4: Introduction to study design",
    "section": "Issues in measurement",
    "text": "Issues in measurement\n\nConstruct validity\nA measure is valid if it measures what it’s supposed to measure. In psychology, we often want to measure things that may be difficult to observe directly, and difficult to quantify. For example, things like happiness, or cognitive ability, or personality. We attempt to measure these unobservable things using a range of different tools including questionnaires or experimental tasks. We design these tools by using the theoretical underpinnings behind the constructs we are trying to measure. Construct validity is the extent to which a tool can be justifiably trusted to actually measure the construct it is supposed to measure.\n\n\nExternal validity\nA study has external validity if its findings can be applied to the entire population of people with the relevant characteristics. 2 For example, if a study uses a sample of white men in western cultures, the findings might only be true for white men in western cultures. They might not apply or generalise to all people. In psychology research we should probably be spending more time thinking about the samples we use and whether they’re representative of the populations we’d like to generalise our results to. For more on this see below.\n\n\n\n\n\n\nWEIRD samples\n\n\n\nSeveral researchers have noted that the samples used in the vast majority of psychology research tend to be rather WEIRD. That is, the samples used in most published psychological science are drawn from Western, Educated, Industrialized, Rich, and Democratic (WEIRD) societies (Henrich et al., 2010). The preponderance of WEIRD samples implies that researchers either assume that there is little variation across different population groups when it comes to the phenomena they’re studying, or that these samples are as representative of humans in general as any other group.\nBut is this true? Answering this question is difficult because it would plausibly be conditional on the nature of the research question or phenomenon being studied. For example, it seems plausible that there would be little variation across human populations when it comes to phenomena like low-level auditory and visual processing. However, it also seems plausible to expect to see more variation when it comes to phenomena like moral reasoning, where cultural practices may play a more prominent role. However, it is an empirical question—that is, it is a question that can only be answered by looking at the actual data. Unfortunately, the data required to answer these questions are sparse. However, the data that does exist suggests that there is probably more variation than people would expect, even when it comes to low-level phenomena like visual perception (Henrich et al., 2010). In the past few years, there has been some movement to try and make samples in psychological research more diverse, but there is still a long way to go and, therefore, this is an issue worth bearing in mind.\n\n\nWe’ve already touched on ecological validity in discussing experiments (see above). Ecological validity, which is often an issue in experiments, refers to whether the findings of a study apply to the “real world”. Just because something is true in the lab doesn’t mean it’s going to be true in the real world.\n\n\n\nReliability\nFinally, reliability refers to the consistency of a measure. A measure is reliable if it produces consistent results each time it’s used on the same participant. For example, we could measure someone’s maths anxiety with a questionnaire. Our questionnaire would be considered reliable if when we tested the same participant on different occasions they got similar scores each time. This stability over time is known as test-retest reliability."
  },
  {
    "objectID": "lectures/week04/handout/index.html#levels-of-measurement",
    "href": "lectures/week04/handout/index.html#levels-of-measurement",
    "title": "Lecture 4: Introduction to study design",
    "section": "Levels of Measurement",
    "text": "Levels of Measurement\nThe last couple of topics we’ll cover in this lecture are about the jargon we use to describe the nature of the measurements we’re taking. The first set of terms describe the kind of information we’re working with. We call this the level of measurement. There are four levels of measurement:\n\nNominal/categorical\nOrdinal\nInterval\nRatio\n\nSometimes a construct can fall into many of these levels, and it’s on the researcher to decide what measurement level is the most appropriate to use.\nNominal/categorical, refers to names, categories, labels, or group membership. Some examples include: 1) eye colour (e.g., green, brown, blue); occupation status (e.g., FT employed, PT employed, unemployed, student…); study condition (control, experimental); or marital status. Even age can be nominal if we wanted it to be (under 50s vs over 50s).\nWhen using nominal variables, we cannot compare the different groups in any quantifiable way. That is, it doesn’t make sense to say that green is more blue when it comes to eye colour.\nAt the ordinal level, individual observations of the measured attribute can be ordered in a meaningful way. For example, we could order marathon runners ranked in order of who came 1st, 2nd, or 3rd. The ordinal level, however, doesn’t provide any information about the differences between individual points. For example, we don’t know how much faster the winner was compared to the runner-up. The distance between 1st and 2nd doesn’t have to be the same as the distance between 2nd and 3rd, or between any adjacent pair of runner. An example of this type of measure in psychology is the Likert scale (see Figure 2).\n\n\n\n\n\nFigure 2: An example of a Likert scale\n\n\n\n\nThese scales are classed as ordinal because we can’t say that the difference in agreement between “Agree” and “Neither agree nor disagree” is the same as the difference between “Disagree” and “Strongly disagree”.\nAt the interval level of measurement, the differences (intervals) between pairs of adjacent values are the same. For example, the difference in temperature between 20°C and 21°C is the same as that between 35°C and 36°C.The intervals marked by the degrees are evenly spaced or equidistant. But there is not an absolute zero point, so we cannot say that 40°C is twice as hot as 20°C. A good example, of this type of measurement in psychology is IQ. Someone with an IQ of 200 is not twice as smart as someone with an IQ of 100, and there’s also no such thing as an IQ of zero.\nFinally, there is the ratio level. The ratio level is similar to the interval level, but with ratio data there is a meaningful 0 point. Some examples of the ratio level of measurement that you might encounter in psychology are: Reaction time, number of correct responses, score on an exam, and income."
  },
  {
    "objectID": "lectures/week04/handout/index.html#variabledata-types",
    "href": "lectures/week04/handout/index.html#variabledata-types",
    "title": "Lecture 4: Introduction to study design",
    "section": "Variable/Data Types",
    "text": "Variable/Data Types\nWhen using quantitative methods, we represent variables with a numerical value, but we can have different types depending on the type of data it represents. Continuous variables can contain any numerical value within a certain range. Some examples might include time, height, and weight.\nDiscrete variables can only contain some values: For example, the number of children. Hypothetically you can have any number of children, but it has to be a whole number. There’s no such thing as 2.4 children.\nFinally, binary variables can only take one of two possible values. They are a special case of discrete variables. Examples of this are Yes or No responses, Heads or Tails, and Pass or Fail scores."
  },
  {
    "objectID": "lectures/week04/handout/index.html#test-your-knowledge",
    "href": "lectures/week04/handout/index.html#test-your-knowledge",
    "title": "Lecture 4: Introduction to study design",
    "section": "Test your knowledge",
    "text": "Test your knowledge"
  },
  {
    "objectID": "lectures/week04/handout/index.html#footnotes",
    "href": "lectures/week04/handout/index.html#footnotes",
    "title": "Lecture 4: Introduction to study design",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you’re not sure what the terms research question and hypothesis mean, then just keep them in your mind for now. Their meanings will become clearer when we get to the examples.↩︎\nWe’ll talk more about populations in later lectures.↩︎"
  },
  {
    "objectID": "lectures/week05/handout/index.html",
    "href": "lectures/week05/handout/index.html",
    "title": "Lecture 5: Open Science",
    "section": "",
    "text": "Today’s lecture comes in two parts. In the first part of the lecture, we’ll discuss a few contemporary issues in psychological research, specifically around what’s known as the replication crisis and the open science movement that has emerged to address some of the shortcomings of how science is done. Some of the practices in the open science movement, particularly something known as pre-registration are the inspiration for the lab report that you’ll complete this term.\nWhile the first part of the lecture will tell you about the motivation behind the lab report, the second part will cover some of the practicalities. Specifically, it will cover some of the details of exactly what is expected from you.\nThis style of lab report will probably be new for all of you. So if it looks like something you’ve never done before then don’t worry. You’re all in the same boat."
  },
  {
    "objectID": "lectures/week05/handout/index.html#the-replication-crisis",
    "href": "lectures/week05/handout/index.html#the-replication-crisis",
    "title": "Lecture 5: Open Science",
    "section": "The replication crisis",
    "text": "The replication crisis\nSeveral large-scale studies in the early part of the last decade (see Open Science Collaboration, 2012, 2015) attempted to replicate some classic findings in the psychology literature. That is, researchers tried to run the studies again to see if they could produce the same results and conclusions as the original studies. This turned out to be spectacularly unsuccessful. Some estimates suggest that possibly 50% or more of these classic findings (the kind of findings that support the theories you read about in your textbooks) could not be replicated. This has prompted some to claim that psychology is in a state of crisis.\nIdentifying that the crisis exists, however, is only the first step. Several researchers have tried to understand the causes of this crisis. It’s probably the case that there isn’t a single cause of the replication crisis. Rather, there are likely several related causes that might be to blame.\nThese causes might include how statistics and statistical procedures are used and abused in psychology. Incentives in the publishing and university system that might favour flashy findings above methodological rigour. And the lack of clearly defined theories in psychological science. That is, theories that make unambiguous predictions about what should happen in experiments (see Lecture 2).\nWhen we designed the psychology methods courses at Sussex, many of these issues were at the forefront of our minds. I won’t be able to go into all of these possible causes in detail in this lecture, so instead, I’ll try and pull out some of the issues that were most relevant to our thinking about the lab report."
  },
  {
    "objectID": "lectures/week05/handout/index.html#bias-in-publishing",
    "href": "lectures/week05/handout/index.html#bias-in-publishing",
    "title": "Lecture 5: Open Science",
    "section": "Bias in Publishing",
    "text": "Bias in Publishing\nIf we look specifically at the published literature in psychology we’ll notice something odd. The vast majority of published papers in psychology journals report findings that support the tested hypotheses (see Figure 1).\n\n\n\n\n\nFigure 1: Psychology researchers find support for their theories more often than researchers in other fields. (image from Yong, 2012)\n\n\n\n\nBut how is this possible? It could be psychology researchers as psychic, and they only test hypotheses that turn out to be true. Or it could be that the hypotheses they test are trivial. But remember in Lecture 2, I said that theories in psychology often make probabilistic claims, so even if they are true then we might not expect every experiment that tests a specific theory to show evidence for that theory. Rather, it seems plausible that there may be some sort of bias in the kinds of papers that get published.\nOne source of bias in the publishing of psychology studies that people have argued exists is that journal editors and peer reviewers might not want to publish studies when they don’t like the results. They might not like the results for a variety of reasons, but one, in particular, might be that if studies don’t show support for the tested hypothesis then reviewers might believe that the results are less reliable. And this belief might be particularly strong in cases where new studies fail to find support for famous or influential theories.\nIf a new study doesn’t find support for a famous or influential theory then editors/reviewers might be more likely to suspect there’s some kind of problem with the new study, and they might choose not to publish it.\nBias might also happen at the level of the researcher in deciding what to submit for publication in the first place. For example, if a researcher fails to find support for the theory they’ve tested then they might not even try to have it published at all."
  },
  {
    "objectID": "lectures/week05/handout/index.html#bias-in-statistical-procedures",
    "href": "lectures/week05/handout/index.html#bias-in-statistical-procedures",
    "title": "Lecture 5: Open Science",
    "section": "Bias in statistical procedures",
    "text": "Bias in statistical procedures\nIn addition to bias in publishing, there are also certain practices that researchers can engage in that can invalidate the results of their studies. These practices are collectively known as questionable research practices or QPRs. One group of QPRs, known as p-hacking can make it so that researchers are more likely to find statistical results that support the tested hypothesis even if the tested hypothesis is not true. Some of these practices are very subtle and may occur without researchers deliberately trying to engage in any form of malpractice. One such practice can occur when researchers repeatedly run their statistical tests after adding more and more participants to their sample.\nWe’ll learn more about the theory that explains the problems with this practice in later lectures, but for now, it will probably be easier to explain by way of an example. Let’s say you have a theory that says that people from West Sussex are taller than people from East Sussex. But let’s also say that theory isn’t true. However, you decide to test it by measuring a random group of 10 people from West Sussex and a random group of 10 people from East Sussex. The heights of these people won’t be exactly the same, so you might find that the East Sussex people are slightly taller. If you added another random group of 20 people, the heights of these people will again not be identical. As a result, you might find that the West Sussex people are now slightly taller. But you might equally find the opposite.\nWhether you find that West Sussex people are taller or East Sussex people are taller is just due to random variation in your sample. That is, because of this random variation, even though West Sussex people aren’t taller, you’ll sometimes find that they are. And you’ll sometimes find that people from East Sussex are taller. If you decide to continue adding more people to your study and checking the results after each batch of people, then you can just decide to stop your study whenever your results happen to show what you want. And it can be easy for you, as a researcher, to justify this to yourself by simply saying that before the point where you decided to stop, you simply hadn’t measured enough people for your sample to be representative.\n\nBut if there is bias in what studies get published, and if it can be easy for researchers to engage in practices that invalidate their statistical procedures, then what is the solution? One proposed solution is pre-registration.\n\n\n\n\n\n\nPre-registration in the media\n\n\n\nThe idea of pre-registration has been covered in popular media. For example, it’s been written about in The Guardian on several occasions. Some examples include:\n\nTrust in science would be improved by study pre-registration\nPsychology’s ‘registration revolution’\nPsychology uses ‘registered replication reports’ to improve reliability\nThe psychology behind a nice cup of tea"
  },
  {
    "objectID": "lectures/week05/handout/index.html#pre-registration-and-combating-bias",
    "href": "lectures/week05/handout/index.html#pre-registration-and-combating-bias",
    "title": "Lecture 5: Open Science",
    "section": "Pre-registration and combating bias",
    "text": "Pre-registration and combating bias\nPreregistration means that before conducting a study, researchers plan their study in detail. This involves specifying the theory they plan to test and specifying all their hypotheses. They also specify details such as the number of participants they intend to sample, and the statistical procedures that they intend to use. Additionally, this plan is made publicly accessible in some way so that there is a record of what the researcher intended to do.\nBy doing this, preregistration might improve the reliability of published research by combating certain kinds of researcher bias, such as the example of p-hacking outlined above. Additionally, because the hypotheses are specified before the data are collected, this also means that researchers can’t change their hypotheses to make them fit whatever their data happened to show (think about falsification and infinitely flexible theories from Lecture 2). All this helps to ensure that the results of studies are more reliable.\nPreregistration can also help combat bias at the publishing stage. Because researchers outline their plans in detail, peer reviewers and journal editors can judge whether the methods are scientifically rigorous and whether the study is likely to produce reliable results. And, importantly, they can make these decisions before seeing the results of a study.\nIn a special form of preregistration, known as a registered report, editors and reviewers can accept studies for publication before the data are collected. This can happen because the detailed research plans allow editors and reviewers to judge whether the study will be scientifically sound before the results are known. And they can make these decisions without being influenced by the actual results."
  },
  {
    "objectID": "lectures/week05/handout/index.html#registered-reports-in-action",
    "href": "lectures/week05/handout/index.html#registered-reports-in-action",
    "title": "Lecture 5: Open Science",
    "section": "Registered reports in action",
    "text": "Registered reports in action\nTo see an example of a registered report in action, we’ll take a look at a study by Colling et al. (2020). Our story starts with a paper published by Fischer et al. (2003). In this, they reported a study that claimed to show that merely looking at numbers would cause a shift in attention to either the left or the right side of space depending on whether the number was big (6-10) or small (1-4).\nThe exact details of the study and the theories that the data was used to support aren’t vitally important for our purposes. The key point, however, was that this finding was very influential, with more than 700 subsequent studies citing this finding or building on this work. Because this finding was influential, it’s not surprising that some researchers tried to replicate it. That is, they tried to run a study using the same methods as the original experiment to see if they could get the same results. Most of these studies were successful—that is, they found results that were broadly in agreement with what Fischer et al. (2003) had shown. A few published studies failed to replicate it, but these were in the minority.\nBut is the finding true? Does merely looking at numbers cause you to shift your attention to the left or the right? If you looked just at the published studies you would certainly have reason to believe that it is true. But if you spoke to people at scientific conferences then many researchers would tell you that they couldn’t successfully replicate it. This just wasn’t reflected in the scientific literature. But how did it come to be like this?\nKnowing exactly how it came to be like this is difficult (probably impossible) to know. But possible reasons might be that the original finding was published in an extremely prestigious journal (Nature Neuroscience), and that it quickly became a very influential finding. This means that it probably got accepted as something like an established fact.\nOnce a finding is accepted as something like an established fact then journal editors and reviewers might be reluctant to publish studies that don’t support the original finding. This is not totally surprising. If something is an established fact, and a new study comes along trying to overturn it then what is more likely? That the established fact is wrong? Or that there’s something wrong with the new study?\nFor example, let’s say that I ran a study that showed that gravity didn’t exist. What is more likely, that gravity doesn’t exist or that there is a problem with my study. It’s reasonable to conclude that there is a problem with my study. But with other theories, it might be the case that the established theory is wrong. The best way to decide this is to try to judge the study on its methods rather than being influenced by what the study found. A registered report makes this possible.\nWith a registered report, you actually submit your research plan to a journal before you run the study. The journal reviews the plan and agrees to publish the study when it’s done, provided that you do the study exactly how you said you would.\nKnowing whether the findings by Fischer et al. (2003) were true or not was something I was curious about. As mentioned, there was this disconnect between what people said at conferences and what you could read in journals. So I decided to put together a registered report where I would attempt to replicate the study by Fischer et al. (2003) (I also added in a few extra tasks, so that if I could replicate it then I would also be able to get a better idea of the exact mechanisms that are responsible for the effect).\nThe plan that I put together contained a lot of detail. It specified exactly who would be recruited to take part as participants. It specified exactly how the data would be collected, and it described all the tasks in detail. It also described all the statistical analyses that would be performed on the data, and what conclusions I would draw based on the results of these analyses.\nAfter planning the study in detail, I then approached a journal with this plan to see if they were willing to publish the study if I did it according to the plan. The plan went out to reviewers to check (including people involved with the original study), and once everyone was happy the journal agreed that they would publish it.\nI then gathered together 30+ psychological scientists from 17 different universities around the world, and we ran the experiment on over 1300 participants. This sample size was nearly 100 times bigger than the original study. So what did we find?\nWe found absolutely no evidence for the original finding. We found no evidence that the additional manipulations that we included, manipulations that people thought might modulate the size of the effect, modulated the size of the effect.\nThis finding is now no longer accepted as true. But a lot of resources might have been wasted studying this non-existent effect. Think of all the people that tried to replicate it, but failed, and then couldn’t get their studies published. This finding is by no means a unique case. There are likely to be many zombie findings in psychology, and that is why things like registered reports are so important. The concept of the registered report is the inspiration for the lab report that you’ll be doing this year."
  },
  {
    "objectID": "lectures/week05/handout/index.html#the-reproducibility-crisis",
    "href": "lectures/week05/handout/index.html#the-reproducibility-crisis",
    "title": "Lecture 5: Open Science",
    "section": "The reproducibility crisis",
    "text": "The reproducibility crisis\nThis lecture is primarily about the replication crisis and registered reports. But it’s also worth (very briefly) touching on reproducibility because reproducibility is the inspiration for why you’re learning about R and R Studio in the practical classes.\nReplication is the idea that we should be able to run a study again and find the same results. Reproducibility is the idea that we should be able to take a dataset that was collected for a study, run the analysis described in the journal article, and re-produce the same numbers that we see in the published paper.\nThis might seem like it’s something that should be easy to do. If you have the data from the original study, you should just be able to run the analysis that is described in the journal article. And if you do this, then you should get the same numbers.\nMy colleagues and I decided to test this out (see Crüwell et al., in press). Sharing data from studies is relatively rare. More commonly, when researchers publish papers, only the results of the analysis are reported in the article, and they don’t share the data they collected. However, sharing data has recently become more common. For example, in one 2019 issue of the journal Psychological Science, all the papers published in that issue shared their data so it was publicly available. As a result, we decided to take all that data, and re-analyze it according to the descriptions in the journal articles, to see whether we could produce the same numbers.\nWhat did we find? In short, we found that of the 14 papers, we could only exactly reproduce the numbers in one of them. For an additional 3, we could get very close to the numbers reported (there were only small differences that didn’t change any of the conclusions). For the remainder, however, we found it impossible to reproduce the numbers. So what went wrong?\nThere were a few things that went wrong. First, some of the researchers did not share the appropriate data. For example, some of the data they shared was missing key parts. Or some of the data they shared wasn’t adequately labeled. However, a major issue, was that the analyses were not adequately described. That is, the researchers didn’t give enough detail in how they analyzed the data for us to re-analyze it.\nBut why was this detail lacking? It’s hard to know exactly why, but a likely explanation is that it can be difficult to give verbal descriptions in sufficient detail so that another researcher can follow them. But I did say that we could exactly reproduce the numbers in one of the papers. So what did these authors do differently?\nThey wrote the entire paper using R and Quarto just like you’re learning about in the practical classes. For this paper, we didn’t need to sift through the verbal descriptions describing what analysis they did, how they kept some participants and rejected others, or what version of a particular statistical test they performed. We just needed to re-run their code! And we could also check their R code in detail to see exactly what they did, which would allow us, if we wanted to, to also assess whether what they did was correct.\nMaking the code available helps to make sure the findings are reproducible. It allows reviewers to check for errors in the analysis. And it helps us be more certain of the soundness of the published literature. But sharing code isn’t common, because most psychology students, and psychological scientists, aren’t taught how to write code. But this is slowly changing. And we’re teaching you because we want you to be better than the generation that came before you."
  },
  {
    "objectID": "lectures/week05/handout/index.html#finding-out-more",
    "href": "lectures/week05/handout/index.html#finding-out-more",
    "title": "Lecture 5: Open Science",
    "section": "Finding out more",
    "text": "Finding out more\nThe concepts and ideas that we learned about in this lecture are all part of the open science (or, more broadly, the open scholarship) movement. Open science and open scholarship are a big focus in the School of Psychology at Sussex. You can find out more about the School of Psychology’s involvement in Open Science at the Open Science Hub."
  },
  {
    "objectID": "lectures/week05/handout/index.html#test-your-knowledge",
    "href": "lectures/week05/handout/index.html#test-your-knowledge",
    "title": "Lecture 5: Open Science",
    "section": "Test your knowledge",
    "text": "Test your knowledge"
  },
  {
    "objectID": "lectures/week06/handout/index.html",
    "href": "lectures/week06/handout/index.html",
    "title": "Lecture 6: Describing measurements I",
    "section": "",
    "text": "In Lecture 4 we started talking about how quantitative methods deal with measurement, and we started talking about putting numbers to things. In today’s lecture, we’re going to start talking about what we actually do with these numbers, and how to make a little more sense of measurements in general. This lecture will be the first in a set of lectures where we’ll talk about samples and populations, how to describe them, and how to understand the relationship between them. A lot of these ideas are interconnected, so we’ll be touching on some ideas multiple times. But hopefully each time we’ll be able to gain a richer understanding.\nThe first thing we’ll want to do when we’ve collected a set of measurements is to describe them in some way. One way to do this is to work out what the typical value is. And it’s this kind of description that we’ll turn our attention to first. We’ll start by looking at three different ways we can describe the typical value in a set of numbers."
  },
  {
    "objectID": "lectures/week06/handout/index.html#measures-of-central-tendency",
    "href": "lectures/week06/handout/index.html#measures-of-central-tendency",
    "title": "Lecture 6: Describing measurements I",
    "section": "Measures of central tendency",
    "text": "Measures of central tendency\nWhat we mean by the typical value is not always clear. For example, in Figure 1 we can see the average annual salary (in US dollars) for a set of 78 countries. Each bar of the plot represents the number of countries in the given salary bracket ($0–$10k, $10-$20k, …).\n\n\n\n\n\nFigure 1: National average annual salary [source: worlddata.info]\n\n\n\nFrom the plot, we can see that there are a lot of countries where the average annual salary is less than USD 30,000. There are also a handful of countries where the average annual salary is more than USD 100,000. What would you consider the most typical annual salary? The bracket with the most countries in it? If so, that would mean the most typical salary is between $0 and USD 10,000 per year. Or maybe we should pick the value where half the countries have a lower average salary and half the countries have a higher one? Choosing this option leads to an estimate of the most typical salary of $12,855 USD per year.\nAs you can see, depending on how we define the most typical value, we get different answers. We’ll cover the three main ways of defining the typical value or average. Together, these ways of describing the typical or average value are known as measures of central tendency.\n\nMode\nThe mode is a term that refers to the most frequent value in a set of measurements. This is the kind of average we discussed above when we said the most typical salary is between USD 0 and USD 10,000 a year. The easiest way to spot the mode is to draw a plot like the one we did in Figure 1 and then look for the tallest bar.\nA set of numbers can have one or more modes. If it has one mode, then it is said to be unimodal. Bimodal means it has two modes. If it has three or more modes, then it’s usually called multimodal. Some examples of this are shown in Figure 2.\n\n\n\n\n\n\n\n(a) A unimodal dataset\n\n\n\n\n\n\n\n(b) A bimodal dataset\n\n\n\n\n\n\n\n(c) A multimodal dataset\n\n\n\n\nFigure 2: Datasets with different numbers of modes.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe mode is the only definition of typical value that works for data that is measured at the nominal/categorical level (see Lecture 4).\nWhen it comes to truly continuous variables, such as height, the mode is often not very informative. Why? Take the example of reaction time. Is it likely that two people will have exactly the same reaction time, right down to the exact number of nanoseconds? Or would each reaction time be unique? If so, each value in the dataset would be unique and, therefore, each value in the data set would be the mode. For this reason, the mode is rarely used for continuous variables measured at the interval or ratio levels.\nAverage salary is a continuous variable, but we’ve turned it into a discrete variable by placing measurements into discrete ranges. By doing so, we can make the mode useful.\n\n\n\n\nMedian\nThe median is the second kind of average we talked about above; the middle value where half the measurements are above that value and half the measurements are below. To find the median, we first need to sort our data. Let’s say we roll a 6-sided dice 5 times and get the following: 3, 4, 6, 1, and 1\nTo calculate the median, we’ll do two steps:\n\nSort the data from smallest to largest: 1, 1, 3, 4, 6\nFind the mid-point: We have five observations so the third one in the sorted sequence is the mid-point.\n\nOut of the five rolls the median is 3 (and the mode is 1). If we had an even number of observations then the median would be the half-way point between the two mid-point values. For example, if we instead rolled the dice 6 times and got the results 1, 1, 3, 4, 4, and 6 then the median would be the midpoint between 3 and 4, or 3.5.\nFigure 3 below shows the annual salary in USD for each of the countries in the dataset, sorted from lowest to highest. Notice, that this time, we’re not grouping countries into salary brackets and looking at how many there are in each, as was the case in Figure 1. Here, each bar represents a country.\nBecause we have an even number of countries in our dataset (78), there are two mid-points. These are highlighted in orange in Figure 3. To get the median annual national salary, we need to find the value half-way between Romania and Venezuela, which in this data set turns out to be $12,855 USD.\n\n\n\n\n\nFigure 3: Average national salary per country sorted from lowest to highest (Hover over the bars to see the name of the country and the value).\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nTo be able to calculate a meaningful median, the variable must be measured on at least the ordinal level. For example, if we had categorical data like eye colour, then it wouldn’t make sense to ask for the median between a set of four blue eyes and 3 green eyes.\n\n\n\n\nMean\nThe arithmetic mean is what most people are referring to when they talk about the average. You can work out the mean of a set of numbers by adding up all the values and then dividing this by the number of values in the data set. Mathematically, you can represent this with the formula shown in Equation 1, below:\n\\[\\bar{x}=\\frac{\\displaystyle\\sum^{N}_{i=1}{x_i}}{N} \\tag{1}\\]\nThis formula just tells use that the mean (\\(\\bar{x}\\)) is equal to the sum of all the numbers (\\(\\sum^{N}_{i=1}{x_i}\\)) divided by the number of values in the dataset (\\(N\\)).\n\n\n\n\n\n\nNote\n\n\n\nWe use the symbol \\(\\bar{x}\\) (pronounced x-bar) to represent the mean of a sample of data. We use the symbol \\(\\mu\\) (pronounced mew) to represent the mean of the population (see sample means and population means below).\nIn fact, we usually use Greek letters for population values (parameters) and Latin letters for sample value (statistics).\n\n\n\n\nMean vs Median\nBoth the mean and median have their advantages and disadvantages. The mean is easier to work with from a mathematical point of view. And for this reason, most of the statistical methods we’ll be learning about are based on the mean.\nCompared to other measures of central tendency, means taken from different samples of the same population tend to be more similar to each other. (We’ll talk more about samples and populations below). If, on the other hand, we calculated medians for different samples from the same population, there would be more variability in the values we’d get.\nTurning back to our dice example: If we took the median and the mean of 5 dice rolls, and we did this over and over, the mean values would be more bunched around 3.5 with very few values less than 2. The median values would still be centred around 3.5, but we’d get more values less than 2, so they would be more spread out.\nThere are also some downsides to the mean relative to the median. The primary one is that the mean is sensitive to extreme values in a way that the median is not. This means that, even if we have a really big sample size, adding a single value that is extremely big or extremely small can shift the mean dramatically. This is not the case for the median.\nFor example, let’s say we have a list of numbers: 5, 3, 1, 7, 10, 4, 5. This list of numbers has a median of 5 and a mean of 5. If we added a value of 1000 to this list, then our new median would be unchanged at 5, but our new mean would be 129.375. You can explore the influence of data points on the mean and the median in Explorable 1.\n\n\n\n\n\n\n\nExplorable 1 (Explore the mean and median)  \n\nClick on the plot below to add data to the data set. After you’ve placed a data point, you can click and drag it to move it. You can use the check boxes to display the mean (red) and median (blue).\nTo see how the mean and the median are influenced by extreme values try adding several points to the left side of the plot before adding a single point to the far right. Drag this point around moving it closer or further away from the left side points. Notice how the median is unchanged but the mean does change.\nTick the show data table box to show the raw data. From this you should be able to work out the mean and median yourself. You can try doing this to make sure you understand how they’re calculated.\n\nimport {central_tendency_display} from \"@ljcolling/measures-of-central-tendency\"\nimport {viewof ave_select} from \"@ljcolling/measures-of-central-tendency\"\nimport {viewof clear} from \"@ljcolling/measures-of-central-tendency\"\nimport {viewof show_data_table} from \"@ljcolling/measures-of-central-tendency\"\nimport { data_table} from \"@ljcolling/measures-of-central-tendency\"\nimport {viewof summaryText} from \"@ljcolling/measures-of-central-tendency\"\ncentral_tendency_display\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof ave_select\n\n\n\n\n\n\n\nviewof clear\n\n\n\n\n\n\n\nviewof show_data_table\n\n\n\n\n\n\n\n\n\ndata_table\n\n\n\n\n\n\n\nviewof summaryText"
  },
  {
    "objectID": "lectures/week06/handout/index.html#sample-means-and-population-means",
    "href": "lectures/week06/handout/index.html#sample-means-and-population-means",
    "title": "Lecture 6: Describing measurements I",
    "section": "Sample means and population means",
    "text": "Sample means and population means\nSo far we’ve talked about describing the typical value in a set of measurements that we have—our sample. But one of the key things that we want to do with statistics is to make inferences about populations from the information that we get from samples. That is, we often want to make a judgement, or draw a conclusion, about an aspect of the population when all we have access to is a sample. We’ll get to more formal definitions of populations and samples shortly, but first, let’s make things more concrete by introducing an example.\nLet’s say you’re interested in the average height of people in the UK. The “easy” way to find an answer to this question is to measure all the people in the UK and then work out the average height. Doing this will give you the exact answer to your question. But if you can’t measure everyone in the UK, then what do you do?\nYou could instead select a smaller group, or subset, of people from the UK. You could then measure the height of people in this group, and then try to use this information to figure out plausible values for the average height of people in the UK.\nIn this example, the group (or groups) you’re making claims about is the population—you want to make claims about the average height of people in the UK. And the sample is a subset of this population—the smaller group of people that you were eventually able to measure.\nIt’s important to note that there isn’t a single population. What counts as the population will depend on the claim you’re making. For example, let’s say I’m interested in testing the claim, “Do people in East Sussex show an interference effect on the Stroop task?”. Here the population would be people in East Sussex. If, however, I want to make claims about people in general, then the population might be all living humans.\n\nTheoretical populations\nWe often talk about populations as if they’re a set of actually existing things that we can take our sample from—for example, all living humans. But populations don’t have to be sets of actually existing things. Instead, they can be the set of possible things from which our samples can be drawn. This is a fairly advanced idea, so it might seem a little confusing for now. But keeping this idea in the back of your mind will help statistics make more sense as you progress through your degree. Because this idea is a little confusing, an example might help.\n\nLet’s say we want to collect a sample of 2 dice rolls. To collect our sample we take a die and roll it twice. From this we can easily work out the typical value—that is, the mean—just as we did above.\nOur sample is the set of 2 dice rolls that we’ve collected. But what is our population? One way to think of our population is as the set of possible outcomes that could occur if we rolled a die twice. What is the typical value of this population? It turns out that we can actually work this out.\nTo work it out we would do something like the following:\n\nimport {viewof view} from \"@ljcolling/probability\"\nviewof view\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis diagram is a little complex, so we’ll step through it. Each colour represents rolling a different number. So, for example, the blue circle means that you rolled 1, the green is rolling a 2, the yellow is rolling a 3, black for 4, white for 6, and purple for 6.\nThe diagram shows all the possible things that could happen if we rolled the dice twice. So for example, one thing that could happen is we could roll a 1 (blue) followed by another 1 (blue). Or we could roll a 1 (blue) followed by a 2 (green).\nSo how do we work out the typical value? First we work out how many of the sequences sum up to 2 (the minimum total you can get with two dice rolls). We then count up the number of sequences that sum up to 3, to 4, 5, and so on all the way up to 12 (the maximum total you can get with two dice rolls).\n\n\n\n\n\n\nExpand to see all the possible outcomes\n\n\n\n\n\n\n\nWarning: `cross_df()` was deprecated in purrr 1.0.0.\nℹ Please use `tidyr::expand_grid()` instead.\nℹ See &lt;https://github.com/tidyverse/purrr/issues/768&gt;.\n\n\n\n\n\n1st Roll\n2nd Roll\nTotal\n\n\n\n\n1\n1\n2\n\n\n2\n1\n3\n\n\n3\n1\n4\n\n\n4\n1\n5\n\n\n5\n1\n6\n\n\n6\n1\n7\n\n\n1\n2\n3\n\n\n2\n2\n4\n\n\n3\n2\n5\n\n\n4\n2\n6\n\n\n5\n2\n7\n\n\n6\n2\n8\n\n\n1\n3\n4\n\n\n2\n3\n5\n\n\n3\n3\n6\n\n\n4\n3\n7\n\n\n5\n3\n8\n\n\n6\n3\n9\n\n\n1\n4\n5\n\n\n2\n4\n6\n\n\n3\n4\n7\n\n\n4\n4\n8\n\n\n5\n4\n9\n\n\n6\n4\n10\n\n\n1\n5\n6\n\n\n2\n5\n7\n\n\n3\n5\n8\n\n\n4\n5\n9\n\n\n5\n5\n10\n\n\n6\n5\n11\n\n\n1\n6\n7\n\n\n2\n6\n8\n\n\n3\n6\n9\n\n\n4\n6\n10\n\n\n5\n6\n11\n\n\n6\n6\n12\n\n\n\n\n\n\n\n\nAfter we’ve done this we’ll see that more sequences add up to 7 than any other total. So we could say that the typical outcome is an outcome that leads to the two dice adding up to 7. If we had a set of 2 dice rolls where the sum was 7, then the mean would be 3.5 (because 7 ÷ 2 = 3.5). So the population mean for two dice rolls is 3.5.\nWe’ll talk more about this idea when we start talking about distributions, but for now this basic description should suffice.\n\nWe can work it out because we know something about the process that gives rise to our data (We’ll talk more about how we know this in a later lecture on probability). That is, we can characterise the process that gives rise to the samples, and our samples are just a set of instances of data generated by this process.\nTaking this idea and applying it to the Stroop task, where we’re interested in humans in general, we might want to say that our population isn’t just all living humans. Instead, we might want to say that our population is all possible humans that might have lived, might be living now, and will live in the future. Or that our samples are just some instances of data generated by some process that goes on in people’s brains when they do the Stroop task. Unlike the dice roll example, we don’t know the characteristics of this process. This means that we couldn’t, for example, draw out what the population would look like. But the point of doing science and statistics is so that we can use the information from samples to start to characterise this process and start to work out what the population might look like. But how do we go from samples to populations?\n\n\nThe relationship between samples and populations\nLet’s assume that we have explicitly defined our population as all people in the UK, and we’ve collected a sample by taking measurements from a subset of this population. What is the relationship between this sample and the population from which it was drawn?\nThe sample should resemble the population in some way. Most often we’re interested in knowing something like: “What is the typical value (i.e., the mean) of the population?“ So for our height example, this would mean that we’re interested in knowing the mean height of the population. Ideally then, the average height of our sample should resemble the average height of our population. But if we don’t know the average of our population, then how will we know whether our sample resembles it?\nFor purposes of illustration, let’s say that I actually know what the population looks like. We’ll say that adults in the UK range in height between 78cm and 231cm, but that the average height of an adult in the UK is 170cm. So our population mean is 170cm. Now let’s collect a sample of data. We’ll talk more about the influence of sample size in a later lecture, but for now let’s just say that we collect a sample of 50 people. And let’s say that we don’t only collect one sample, but that we collect a sample of 50 people over and over again. In Explorable 2 you can see the average height of each of our samples of 50 people, with the solid line showing the population mean.\n\n\n\n\n\n\n\nExplorable 2 (Explore sample means)  \n\nIn the plot below, each dot represents the mean height from a sample of 50 people from our simulated UK population. The solid horizontal line shows the actual population mean of our simulated UK population. The plot shows the means for 100 different samples of 50 people. Press Replay to generate a fresh set of 100 samples of 50 people.\n\nimport {sample_means} from \"@ljcolling/measures-of-central-tendency\"\nimport {viewof replay_mean} from \"@ljcolling/measures-of-central-tendency\"\nsample_means()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof replay_mean\n\n\n\n\n\n\n\n\n\nWhat do you notice in Explorable 2? The sample means don’t always line up exactly with the population mean. Sometimes the sample mean is higher and sometimes the sample mean is lower. It moves around a bit from sample to sample. Because it moves around, and because we don’t know the population mean, this tells us that on any particular sample we won’t know whether the sample mean is the same as the population mean.\n\n\nThe average of the sample means\nBut let’s think of things from a slightly different perspective. Let’s treat the mean of each sample of 50 people as a measurement. We’ll now take a “sample” of these measurements. That is, we’ll measure the height of 50 people and we’ll work out the average height. This might be something like 168 cm. We’ll then measure another 50 people, and we’ll work out the average height, which might be something like 175 cm. We’ll then work out the average of these two averages (171.5 cm). We’ll do this over and over recalculating our average of averages after each new sample of 50 people. In Explorable 3 we can see what happens to our average of our sample of samples. What do you notice?\n\n\n\n\n\n\n\nExplorable 3 (Explore sample of samples)  \n\nIn the plot below we can see the average of our sample of samples (or our running average) as we add more samples. That is, we can see what the average would be after 2, 10, or 100 samples (etc), where at each step we’re adding the average of our new sample to the set of averages of all the samples that have come before. Click replay to start the process over with a new set of samples.\n\nimport {sample_means_ave} from \"@ljcolling/measures-of-central-tendency\"\nimport {viewof replay_running_mean} from \"@ljcolling/measures-of-central-tendency\"\nsample_means_ave()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof replay_running_mean\n\n\n\n\n\n\n\n\n\nThat’s right, as we carry on collecting more and more samples of samples the average of these will eventually line up and match the population mean. This shouldn’t be completely surprising. If we collect enough samples, and combine them all together, then eventually we’ll have sampled the entire population.\nBut the point I want you to take away from this demonstration is the following: This tells us that even though we don’t know whether the mean of any particular sample is the same as the population mean, the sample mean will on average be the same as the population mean. We’ll touch on this idea more in future lectures when we talk about the sampling distribution, but for now this simple idea is all you need to know."
  },
  {
    "objectID": "lectures/week06/handout/index.html#check-your-understanding",
    "href": "lectures/week06/handout/index.html#check-your-understanding",
    "title": "Lecture 6: Describing measurements I",
    "section": "Check your understanding",
    "text": "Check your understanding\nUse this quiz to make sure that you’ve understood the key concepts."
  },
  {
    "objectID": "lectures/week07/handout/index.html",
    "href": "lectures/week07/handout/index.html",
    "title": "Lecture 7: Describing measurements II",
    "section": "",
    "text": "Last week we started learning about the tools we can use to describe data. Specifically, we learned about the mean, mode, and median. And we learned about how they are different ways of describing the typical value. But apart describing the typical value, we might also want a way to describe how spread out the data is around this value. We’ll start this week off by talking about ways to measure this spread. ## Measures of spread\nIf you look at Figure 1 you’ll see two data sets that are centered at the same value but have very different amounts of variability. Both sets of data have a mean of 0. But, as you can see, the values of one are spread out much more widely than the values of the other.\nFigure 1: Histogram of two distributions with equal means but different spreads. N = 10,000 in each case.\nThis is why, apart from measures of central tendency, we also need measures that tell us about the spread, or dispersion of a variable. Once again, there are several measures of spread available, and we’ll talk about five of them:"
  },
  {
    "objectID": "lectures/week07/handout/index.html#understanding-the-relationship-between-samples-and-populations",
    "href": "lectures/week07/handout/index.html#understanding-the-relationship-between-samples-and-populations",
    "title": "Lecture 7: Describing measurements II",
    "section": "Understanding the relationship between samples and populations",
    "text": "Understanding the relationship between samples and populations\nNow we have some tools for describing measurements, both in terms of where they are centered (the mean) and in terms of how spread out they are (the standard deviation). With these tools in hand, we can return to the problem we talked about last lecture. That is, the problem of knowing whether our sample resembles the population.\nIn the previous lecture, we saw that when we took samples from the population, sometimes the sample mean was higher than the population mean, and sometimes it was lower. But on average the sample mean was the same as the population mean.\nIn the previous lecture, I also said that we wouldn’t know whether a particular sample mean was higher, lower, close to, or far away from the population mean. We can’t know this, because we don’t know the value of the population mean. But one thing we can know, is how much, on average, the sample means will deviate from the population mean. To see what I mean by this, let’s say a look at the two plots in Figure 8. In Figure 8​a you can see the means of 10 different samples taken from the sample population. Sometimes the sample mean is higher than the population mean, sometimes it’s lower. But the thing I want you to notice is how spread out the values are. In Figure 8​b you can see the means of a different collection of 10 samples. Again, some are higher and some are lower. But notice the spread of the values. If we were to calculate the standard deviation for Figure 8​a, we would find that the sample means deviate from the population mean by an average of 6.8. And if we were to calculate the standard deviation for Figure 8​b, we would find that the sample means deviate from the population mean by an average of 10.48.\nNow we’re not using the standard deviation to tell us about the spread of the values in our sample. Instead, we’re using the idea to tell us about the spread of our sample means. This standard deviation, the standard deviations of the sample means from the population mean has a special name. It is called the standard error of the mean.\n\n\n\n\n\n\n\nFigure 8: (a) 10 samples with a standard deviation of 6.8 (b) 10 samples with a standard deviation of 10.48\n\n\n\n\nThe standard error of the mean will be an important concept. But to fully appreciate the idea we’ll first need to learn about the sampling distribution. And before we can get to the sampling distribution, we first need to understand the what distributions are, what they look like, and why they look the way they do.\n\n\n\nmutable summary = null\n\n\n\n\n\n\n\nround2 = (v) =&gt; Math.round(v * 100) / 100\n\n\n\n\n\n\n\nimport { set } from \"@observablehq/synchronized-inputs\"\n\n\n\n\n\n\n\nimport { dist } from \"@ljcolling/wasm-distributions\"\n\n\n\n\n\n\n\nmaketable = (data) =&gt; {\n  let headers = Object.keys(data[0]).map((v) =&gt; html.fragment`&lt;td&gt;${v}&lt;/th&gt;`);\n  let body = data.map((r) =&gt; {\n    let this_row = Object.values(r).map((v) =&gt; html.fragment`&lt;td&gt;${v}&lt;/td&gt;`);\n    return html.fragment`&lt;tr&gt;${this_row}&lt;/tr&gt;`;\n  });\n  return htl.html`&lt;table class=\"table table-striped\"&gt;&lt;thead class=\"thead-dark\"&gt;&lt;tr&gt;${headers}&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;${body}&lt;/tbody&gt;&lt;/table&gt;`;\n};\n\n\n\n\n\n\n\nhtml = htl.html\n\n\n\n\n\n\n\nmeasures = [\n  { name: \"Range\", tag: 0, fun: drawrange },\n  { name: \"Interquartile range\", tag: 1, fun: drawiqr },\n  { name: \"None\", tag: 2, fun: () =&gt; {} }\n]\n\n\n\n\n\n\n\nfunction data_mean_median(data, drawlines, varname) {\n  let height = 200;\n  let xAxisOffest = 0;\n  let radius = 10;\n  var this_summary = {};\n  const pin = (v) =&gt; {\n    return v;\n  };\n  const update = () =&gt; {\n    let data = svg.selectAll(\"circle\").data();\n    let mean = d3.mean(data.map((v) =&gt; v.x));\n    let median = d3.median(data.map((v) =&gt; v.x));\n    drawlines(marker, data);\n    return data;\n  };\n\n  function dragstarted(event, d) {\n    d3.select(this).attr(\"stroke\", \"green\").attr(\"stroke-width\", 5);\n  }\n\n  function dragged(event, d) {\n    d3.select(this)\n      .raise()\n      .attr(\"cx\", d.x = Math.round(event.x))\n      .attr(\"cy\", d.y = Math.round(event.y));\n    data = update();\n\n    var this_summary = {};\n    this_summary[varname] = {\n      mean: d3.mean(data.map((v) =&gt; v.x)),\n      median: d3.median(data.map((v) =&gt; v.x)),\n      data: data.map((v) =&gt; v.x),\n    };\n    summaryupdate(this_summary);\n  }\n\n  function dragended(event, d) {\n    d3.select(this).attr(\"stroke\", null);\n    d3.select(this).attr(\"r\", radius);\n    let data = svg.selectAll(\"circle\").data();\n\n    var this_summary = {};\n    this_summary[varname] = {\n      mean: d3.mean(data.map((v) =&gt; v.x)),\n      median: d3.median(data.map((v) =&gt; v.x)),\n      data: data.map((v) =&gt; v.x),\n    };\n    summaryupdate(this_summary);\n  }\n\n  this_summary[varname] = {\n    mean: d3.mean(data.map((v) =&gt; v.x)),\n    median: d3.median(data.map((v) =&gt; v.x)),\n    data: data.map((v) =&gt; v.x),\n  };\n\n  summaryupdate(this_summary); \n  let svg = d3\n    .create(\"svg\")\n    .attr(\"viewBox\", [0, 0, width, height])\n    .attr(\"stroke-width\", 2);\n  let marker = svg.append(\"g\");\n  let points = svg.append(\"g\");\n\n  points\n    .selectAll(\"circle\")\n    .data(data)\n    .join(\"circle\")\n    .attr(\"cx\", (d) =&gt; d.x)\n    .attr(\"cy\", (d) =&gt; d.y)\n    .attr(\"r\", radius)\n    .on(\"mouseover\", function (d, i) {\n      d3.select(this)\n        .transition()\n        .duration(\"50\")\n        .attr(\"opacity\", \".5\")\n        .attr(\"stroke\", \"blue\");\n    })\n    .on(\"mouseout\", function (d, i) {\n      d3.select(this)\n        .transition()\n        .duration(\"50\")\n        .attr(\"opacity\", \"1\")\n        .attr(\"stroke\", \"black\");\n    })\n    .call(\n      d3\n        .drag()\n        .on(\"start\", dragstarted)\n        .on(\"drag\", dragged)\n        .on(\"end\", dragended),\n    );\n\n  marker.selectAll(\"#labelline\").remove();\n  drawlines(marker, data);\n\n  var scale = d3.scaleLinear().domain([0, width]).range([0, width]);\n  var x_axis = d3.axisBottom().scale(scale);\n\n  const clicked = (event, d) =&gt; {\n    if (event.defaultPrevented) return; \n    let x = Math.round(event.offsetX);\n    let y = Math.round(event.offsetY);\n    data.push({ x: x, y: y });\n    var this_summary = {};\n    this_summary[varname] = {\n      mean: d3.mean(data.map((v) =&gt; v.x)),\n      median: d3.median(data.map((v) =&gt; v.x)),\n      data: data.map((v) =&gt; v.x),\n    };\n\n    summaryupdate(this_summary); \n\n    svg\n      .selectAll(\"circle\")\n      .data(data)\n      .join(\"circle\")\n      .attr(\"cx\", (d) =&gt; d.x)\n      .attr(\"cy\", (d) =&gt; d.y)\n      .attr(\"r\", radius)\n      .on(\"mouseover\", function (d, i) {\n        d3.select(this)\n          .transition()\n          .duration(\"50\")\n          .attr(\"opacity\", \".5\")\n          .attr(\"stroke\", \"blue\");\n      })\n      .on(\"mouseout\", function (d, i) {\n        d3.select(this)\n          .transition()\n          .duration(\"50\")\n          .attr(\"opacity\", \"1\")\n          .attr(\"stroke\", \"black\");\n      })\n      .call(\n        d3\n          .drag()\n          .on(\"start\", dragstarted)\n          .on(\"drag\", dragged)\n          .on(\"end\", dragended),\n      );\n\n    update();\n  };\n\n  svg.on(\"click\", clicked);\n  svg\n    .append(\"g\")\n    .attr(\"transform\", \"translate(\" + xAxisOffest + \", \" + height * 0.9 + \")\")\n    .call(x_axis);\n\n  return svg.node();\n}\n\n\n\n\n\n\n\nfunction summaryupdate(this_summary) {\n    mutable summary = Object.assign({}, mutable summary, this_summary);\n}\n\n\n\n\n\n\n\ndrawlines = {\n  return (marker, data) =&gt; {\n    let height = 200;\n    marker.selectAll(\"#labelline\").remove();\n    // draw the mean line\n    /*\n    svg\n      .append(\"line\")\n      .attr(\"id\", \"labelline\")\n      .attr(\"x1\", d3.mean(data.map((v) =&gt; v.x)) || -10)\n      .attr(\"x2\", d3.mean(data.map((v) =&gt; v.x)) || -10)\n      .attr(\"y1\", 0)\n      .attr(\"y2\", height * 0.9)\n      .attr(\"stroke\", \"red\")\n      .attr(\"stroke-width\", 5);\n*/\n    // draw the deviation lines\n    marker\n      .selectAll(\"line\")\n      .data(data)\n      .join(\"line\")\n      .attr(\"id\", \"labelline\")\n      .attr(\"x1\", (d) =&gt; d.x)\n      .attr(\"x2\", d3.mean(data.map((v) =&gt; v.x)) || -10)\n      .attr(\"y1\", (d) =&gt; d.y)\n      .attr(\"y2\", (d) =&gt; d.y)\n      .attr(\"stroke\", \"red\")\n      .style(\"stroke-dasharray\", \"3, 3\")\n      .attr(\"stroke-width\", 2);\n    marker\n      .append(\"line\")\n      .attr(\"id\", \"labelline\")\n      .attr(\"x1\", d3.mean(data.map((v) =&gt; v.x)) || -10)\n      .attr(\"x2\", d3.mean(data.map((v) =&gt; v.x)) || -10)\n      .attr(\"y1\", 0)\n      .attr(\"y2\", height * 0.9)\n      .attr(\"stroke\", \"red\")\n      .attr(\"stroke-width\", 5);\n  };\n  //end of drawlines\n}\n\n\n\n\n\n\n\ndrawrange = {\n  return (marker, data) =&gt; {\n    let height = 200;\n    marker.selectAll(\"#labelline\").remove();\n\n    const max = d3.max(data.map((v) =&gt; v.x)) || -10;\n    const min = d3.min(data.map((v) =&gt; v.x)) || -10;\n    const width = max - min || 0;\n\n    marker\n      .append(\"line\")\n      .attr(\"id\", \"labelline\")\n      .attr(\"x1\", min)\n      .attr(\"x2\", min)\n      .attr(\"y1\", 0)\n      .attr(\"y2\", height * 0.9)\n      .attr(\"stroke\", \"red\")\n      .attr(\"stroke-width\", 5);\n\n    marker\n      .append(\"line\")\n      .attr(\"id\", \"labelline\")\n      .attr(\"x1\", max)\n      .attr(\"x2\", max)\n      .attr(\"y1\", 0)\n      .attr(\"y2\", height * 0.9)\n      .attr(\"stroke\", \"red\")\n      .attr(\"stroke-width\", 5);\n\n    marker\n      .append(\"rect\")\n      .attr(\"id\", \"labelline\")\n      .attr(\"x\", d3.min(data.map((v) =&gt; v.x)))\n      .attr(\"y\", 0)\n      .attr(\"width\", width)\n      .attr(\"height\", height * 0.9)\n      .attr(\"stroke\", \"red\")\n      .attr(\"fill\", \"red\")\n      .attr(\"stroke-width\", 5)\n      .attr(\"opacity\", 0.5);\n  };\n}\n\n\n\n\n\n\n\ncalc_iqr_limits = (d) =&gt; {\n  const data = d.map((v) =&gt; v.x);\n  const quantiles = [0, 0.25, 0.5, 0.75, 1].map((q) =&gt; d3.quantile(data, q));\n  const widths = quantiles.slice(0, -1).map((v, i) =&gt; {\n    return quantiles[i + 1] - v;\n  });\n\n  const marks = new Map();\n\n  widths.map((v, i) =&gt; {\n    const start = quantiles[i];\n    const width = v;\n    marks.set(\"q\" + (i + 1) + \"_s\", start);\n    marks.set(\"q\" + (i + 1) + \"_w\", width);\n  });\n\n  return marks;\n}\n\n\n\n\n\n\n\ndrawiqr = {\n  return (marker, data) =&gt; {\n    let height = 200;\n    marker.selectAll(\"#labelline\").remove();\n\n    const max = d3.max(data.map((v) =&gt; v.x)) || -10;\n    const min = d3.min(data.map((v) =&gt; v.x)) || -10;\n    const width = max - min || 0;\n\n    const quantiles = calc_iqr_limits(data);\n\n    for (let i = 1; i &lt; 5; i++) {\n      const start = quantiles.get(\"q\" + i + \"_s\");\n      const width = quantiles.get(\"q\" + i + \"_w\");\n\n      marker\n        .append(\"rect\")\n        .attr(\"id\", \"labelline\")\n        .attr(\"x\", start)\n        .attr(\"y\", 0)\n        .attr(\"width\", width)\n        .attr(\"height\", height * 0.9)\n        .attr(\"fill\", i === 2 || i === 3 ? \"red\" : \"blue\")\n        .attr(\"stroke-width\", 0)\n        .attr(\"opacity\", 0.5);\n\n      marker\n        .append(\"line\")\n        .attr(\"id\", \"labelline\")\n        .attr(\"x1\", start)\n        .attr(\"x2\", start)\n        .attr(\"y1\", 0)\n        .attr(\"y2\", height * 0.9)\n        .attr(\"stroke\", i === 1 ? \"blue\" : \"red\")\n        .attr(\"stroke-width\", 2);\n    }\n\n    marker\n      .append(\"line\")\n      .attr(\"id\", \"labelline\")\n      .attr(\"x1\", max || -10)\n      .attr(\"x2\", max || -10)\n      .attr(\"y1\", 0)\n      .attr(\"y2\", height * 0.9)\n      .attr(\"stroke\", \"blue\")\n      .attr(\"stroke-width\", 2);\n  };\n\n}\n\n\n\n\n\n\n\nraw_data = {\n  replay_variance_1\n  replay_variance_2\n  replay_variance_3\n  let sample_size = 50;\n  let population_mean = 100;\n  let sd = 15;\n  return dist.rand_normal(population_mean, sd, sample_size, 100);\n}\n\n\n\n\n\n\n\nraw_data_ave = {\n  replay_variance_1\n  replay_variance_2\n  replay_variance_3\n\n  let sample_size = 50;\n  let population_mean = 100;\n  let sd = 15;\n  return dist.rand_normal(population_mean, sd, sample_size, 10000);\n}"
  },
  {
    "objectID": "lectures/week07/handout/index.html#check-your-understanding",
    "href": "lectures/week07/handout/index.html#check-your-understanding",
    "title": "Lecture 7: Describing measurements II",
    "section": "Check your understanding",
    "text": "Check your understanding\nUse this quiz to make sure that you’ve understood the key concepts."
  },
  {
    "objectID": "lectures/week08/handout/index.html",
    "href": "lectures/week08/handout/index.html",
    "title": "Lecture 8: Distributions",
    "section": "",
    "text": "In the previous lecture, we started talking about something called the standard error of the mean. We said that the standard error of the mean told us something about how spread out the sample means would be if we took multiple samples from the same population. That is, the standard error of the mean if the standard deviation of something called the sampling distribution. But before we can talk about the sampling distribution, we need to talk about distributions more generally, where they come from, and why they look the way that they do. That means that for now, we’ll put the standard error of the mean aside, but we will return to it later.\nUp until now, we’ve skirted around the idea of distributions. We’ve looked at histograms of data, but we haven’t really talked much about their shape. It turns out that there are some shapes that we’ll come across very often, and some of these shapes have properties that will make them very useful for statistics. But before we can get to that, we need to first understand where these shapes come from—that is, why distributions have the shape they do—and some language for describing these shapes. We’ll start off with the simplest distribution, the binomial distribution, before moving on to the normal distribution."
  },
  {
    "objectID": "lectures/week08/handout/index.html#the-binomial-distribution",
    "href": "lectures/week08/handout/index.html#the-binomial-distribution",
    "title": "Lecture 8: Distributions",
    "section": "The binomial distribution",
    "text": "The binomial distribution\nTo understand what the binomial distribution is, and where it comes from we’ll do a little thought experiment. In our thought experiment, we’ll take a coin, and we’ll flip it. When we flip a coin, one of two outcomes is possible. Either the coin will land showing heads, or it will land showing tails. We can say that there are two possible events or two possible sequences of events (sequences will make more sense when we add more coins) that can happen when we flip a coin.\nBut now let’s make it a little more complicated. Let’s flip two coins. Now there’s a greater number of possible sequences. We can list them:\n\nThe first coin shows heads, and so does the second (HH),\nThe first coin shows heads and the second shows tails (HT),\nThe first coin shows tails and the second shows heads (TH),\nand the first coins shows tails and the second shows tails (TT)\n\nNow there are four possible sequences. Let’s count up the number of sequences that lead to 0 heads, one head, two heads, etc. If we do this, we’ll see that one sequence leads to 0 heads (TT). Two sequences lead to 1 head (HT, and TH). And one sequence leads to 2 heads (HH).\nLet’s now add more coins. Things will get trickier from here, because the number of sequences rapidly goes up. With three coins there will be 8 possible sequence. With four coins there will be 16 sequences. And with five coins there will be 32 possible sequences. To make things easier, we’ll draw a plot. First, we’ll draw out a plot to trace the sequences. We’ll use different coloured dots to indicate heads and tails. We can do this in the form of a branching tree diagram shown in Figure 1.\nOnce we’ve visualised the sequences, it’s easy to count up how many sequences result in 0 heads, one head, two heads etc. For this, we’ll make a frequency plot, or histogram, just like we’ve seen before. On the x-axis, we’ll have the number of heads. And on the y-axis, we’ll have the number of sequences that result in that number of heads. This frequency plot is shown in Figure 2.\n\nviewof sequences = {\n  const div = document.createElement(\"div\");\n  div.value = new vega.View(parsedSpec).initialize(div).run();\n  return div;\n}\n\n\n\n\n\nFigure 1: Possible sequences after  coin flips\n\n\n\n\n\n\nviewof coin_flip_dist = Plot.plot({\n  y: {\n    label: \"Sequences\",\n    grid: true\n  },\n  x: {\n    label: \"Number of heads\"\n  },\n  marks: [Plot.barY(n_heads, { x: \"x\", y: \"y\" }), Plot.ruleY([0])]\n})\n\n\n\n\n\nFigure 2: Distribution of number of heads after  coin flips\n\n\n\n\n\n\nviewof coins = htl.html`&lt;input style=\"width:300px\" type=\"range\" id=\"coins\" min=\"1\" max=\"7\" value=\"1\" class=\"form-range\"&gt;`\n\ncoins_label = htl.html`&lt;label for=\"coins\" class= \"form-label\" width=\"100%\"&gt;Number of coin flips: ${\n  coins - 1\n}&lt;/label&gt;`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeither Figure 1 nor Figure 2 look very interesting at the moment. But that’s because we haven’t started flipping any coins. Move the slider above so that you can see what happens after we’ve flipped one coin, two coins, three coins etc.\nIncreasing the number of coin flips increases the number of possible sequences. And it changes the number of ways of getting one head, two heads, three heads and so on. Notice that as you adjust the slider and add more and more coin flips, the frequency plot takes on a characteristic shape. You can mathematically model the shape of this plot using a binomial distribution.\nThe binomial distribution is just an idealised representation of the process that generates sequences of heads and tails when we flip a coin. It tells us that if we flip a coin a certain number of times, and that coin lands head 50% of the time, then there is a particular number of sequences that will produce 0 heads, 1 head, 2 heads etc. And the shape of the distribution tells us how many sequences will produce each of those outcomes. We’ll encounter the binomial distribution again when we talk about probability, but for now hopefully the intuition make sense.\nI said it’s an idealised representation, but we can also see that stereotypical shape being produced by natural processes. One natural process that gives rise to this shape is the “bean machine”. In a bean machine, small steel balls fall from the top of the device to the bottom of the device. On their way down, they bump into pegs. When one of the balls hits a peg, it has a roughly equal chance of bouncing off to the left or the right. At the bottom of the device are equally-spaced bins for collecting the balls. If enough balls are dropped into the device, then the distribution of balls across the bins will start to take on the shape of the binomial distribution. Very few balls will be at the far edges, because this would require the balls to bounce left or right every time. Instead, most of the balls will tend to be clumped somewhere near the middle. You can see an example of a “bean machine” in Figure 3.\n\n\n\n\n\n\nFigure 3: Example of the bean machine\n\n\n\nFlipping coins might seem a long way off from anything you might want to study in psychology. However, the shape of the binomial distribution, might be something you’re more familiar with. This characteristic bell shape is also something we see in the normal distribution. And it’s the normal distribution which we’ll turn our attention to next."
  },
  {
    "objectID": "lectures/week08/handout/index.html#the-normal-distribution",
    "href": "lectures/week08/handout/index.html#the-normal-distribution",
    "title": "Lecture 8: Distributions",
    "section": "The normal distribution",
    "text": "The normal distribution\nThe normal distribution has a similar shape to the binomial distribution; however, there are a few key differences. First, the binomial distribution is bounded. One end represents 0 heads. And the other end represents all heads. That is, the distribution can only range from 0 to n (where n is the number of coins). The normal distribution, however, is unbounded. It can range from positive infinity to negative infinity. The second difference is that for the binomial distribution, the steps along the x-axis are discrete. You can have 0 heads, 1 head, 2 heads etc. But you can’t get 1.5 heads. But for the normal distribution, the steps are continuous.\nThe normal distribution is a mathematical abstraction, but we can use it as a model of real-life frequency distributions. That is, we can use it as a model of populations that are produced by certain kinds of natural processes. Because normal distributions are unbounded and continuous, nothing, in reality, is normally distributed. For example, it’s impossible to have infinity or negative infinity of anything. This is what is meant by an abstraction. But natural processes can give rise to frequency distributions that look a lot like normal distributions, which means that normal distributions can be used as a model of these processes. Before we talk more about the shape of the normal distribution, let us first examine some processes that give rise to normal distributions.\n\nProcesses that produce normal distributions\nTo see how a natural process can give rise to a normal distribution, let us play a board game! We won’t play it for real, but we’ll simulate it.\nIn this game, each player rolls the dice a certain number of times, and they move the number of spaces indicated by the dice. Not that dissimilar to what you’d do in a game of monopoly, or a similar board game! For example, if a player rolled the die three times, and they got 1, 4, 3, then they would move 8 (1 + 4 + 3 = 8) spaces along the board. At the end of one round of rolls we can take a look at how far from the start each player is. And we can draw a histogram of this data. You can explore the simulation in Explorable 1.\n\n\n\n\n\n\n\nExplorable 1 (Dice game simulation (adding))  \n\nIn the simulation, you can set the number of players and the number of rounds they play. We’ll start off by having 10 players, and each player will roll 1 dice/play 1 round. Next try setting the Number of rounds to 2 and then increasing the Number of players. What do you notice? You can also try increasing the Number of rounds so that it’s more than 2. What do you notice?\n\nsimple_dice_plot = Plot.plot({\n  x: {\n    label: \"places from start\",\n    domain: d3.sort(dicedata, (d) =&gt; d.value).map((d) =&gt; d.value)\n  },\n  y: {\n    label: \"number of players\"\n  },\n  marks: [\n    Plot.barY(dicedata, {\n      x: \"value\",\n      y: \"count\",\n      sort: { x: \"y\", reverse: true }\n    }),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\nFigure 4: Distribution of players’ position from the starting point\n\n\n\n\nviewof n_dice = Inputs.range([1, 2000], {\n  step: 1,\n  value: 1,\n  label: \"Number of rounds\"\n})\nviewof n_players = Inputs.range([1, 10000], {\n  step: 1,\n  value: 10,\n  label: \"Number of players\"\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen you have enough players, and they play enough rounds, then the distribution shown in Figure 4 starts to take on the shape of a normal distribution. What is it about the dice game that gives rise to the normal distribution shape? In the dice game each players position is determined by adding up the value of each roll. This adding up is the key feature that determines the shape of the distribution of the players’ scores. Notice that this is also what we did with coin flips. We added up the number of heads. It doesn’t matter whether we’re adding up the number of heads or the outcomes of dice rolls. What matters is that each final value (number of heads or places from start) is determined by adding up numbers.\nWe can imagine other processes that might work similarly to the dice game. For example, a developmental process might work similarly. Let’s say that we have a developmental process like height. At each point in time some value (growth) can be added on to a person’s current height. That is, at each time point a person can grow by some amount just like each players’ score can increase by some amount with each dice roll. If you have enough people (players), and some time for growth to happen (rolls), then the distribution of height will start to look like a normal distribution.\n\nProcesses that don’t produce normal distributions\nI’ve said that the key factor that gives rise to the normal distribution is that outcomes are determined by adding together numbers1. We can test out this idea by changing the rules of the dice game so that instead of adding together numbers we’ll multiply them.\nIn Explorable 2 we’ve changed the rules so that each players’ score is determined by multiplying together the values of their rolls. For example, under the new rules, if a player rolled 1, 4, 3 then their score would be 12 (1 × 4 × 3 = 12). Explore what happens to the shape of the distribution under these new rules.\n\n\n\n\n\n\n\nExplorable 2 (Dice game simulation (multiplying))  \n\nAs with the previous simulation you can set the number of players and the number of rounds they play. In the previous simulation when you had many players that played a lot of rounds you saw a distribution that was shaped like a normal distribution. For this simulation first try setting the number of players to the maximum (400), and then slowly increase the number of rounds they play from 1, to 2, to 3, etc. Notice how the shape of the distribution differs from the shape of the normal distribution.\n\ndice_plot_mult = Plot.plot({\n  x: { label: \"places from start\" },\n  y: { label: \"number of players\" },\n  marks: [\n    Plot.rectY(d, Plot.binX({ y: \"count\" }, { x: \"x\" })),\n    Plot.ruleY([0]),\n  ],\n});\n\n\n\n\n\n\n\nviewof n_dice_mult = Inputs.range([1, 20], {\n  step: 1,\n  value: 6,\n  label: \"Number of rounds\"\n})\nviewof n_players_mult = Inputs.range([5, 400], {\n  step: 1,\n  value: 200,\n  label: \"Number of players\"\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the new version of the dice game, the distribution of scores doesn’t look like a normal distribution. After only a few rounds you start seeing a few players with very high scores, but most of the players are clumped around near the start. You might notice that this plot looks a little bit like the plot we saw in Lecture 6 of the Average Annual Salary across a set of 79 countries. This is maybe not a total coincidence. Can you think of a process that might operate with similar rules as this game? How about interest, or returns on investments?\nWhen you get a 10% annual return on an investment, and you start with £10, then after 1 year you’ll have 10 × 1.10 or £11. After two years, you’ll have (10 × 1.10) × 1.10 or £12.10.\nGiven this, it’s not surprising to see wealth distributions (whether across countries in the form of national salaries or within countries) to be heavily skewed in a manner resembling the outcomes of the dice game in Explorable 2. It’s simply a consequence of the rules of the game where wealth accumulates more wealth by multiplication! And in our dice game, this skewed distribution happens even though each player is rolling the same set of dice. A lucky few rolls at the start of the game and then some players get so far ahead that they’ll never get caught by the remaining players stuck down near the bottom.\nIn psychology, we won’t study many processes that grow like this, but it is an interesting exercise thinking about these processes.\n\n\n\nDescribing normal distributions\nThe normal distribution has a characteristic bell shape, but not all normal distributions are identical. They can vary in terms of where they’re centered and how spread out they are. We’ve already talked about measures of central tendency and measures of spread. In particular, we’ve talked about the mean and standard deviation, and these are what we use to describe the shape of the normal distribution.\nThe mean (\\(\\mu\\)) determines where the centre is, and the standard deviation (\\(\\sigma\\)) determines how spread out it is. Use Explorable 3 to explore the normal distribution.\n\n\n\n\n\n\n\nExplorable 3 (Explore the normal distribution)  \n\nUse the sliders to make adjustments to \\(\\mu\\) and \\(\\sigma\\) and see how the plot of the normal distribution changes.\n\nnormal_plot_output = Plot.plot({\n  x: {\n    grid: true,\n    domain: [-4, 4]\n  },\n  y: {\n    grid: true,\n    domain: [0, 0.8]\n  },\n  marks: [\n  // Plot.line(\n  //   normal_plot(\n  //     mean_value - sd_value * s &gt; -4 ? mean_value - sd_value * s : -4,\n  //     mean_value + sd_value * s &lt; 4 ? mean_value + sd_value * s : 4,\n  //     mean_value,\n  //     sd_value\n  //   ),\n  //   {\n  //     x: \"x\",\n  //     y: \"y\",\n  //     strokeWidth: 1,\n  //     fill: \"blue\",\n  //     opacity: show ? 0.5 : 0\n  //   }\n  // ),\n  // Plot.line(fill_limits(s), {\n  //   x: \"x\",\n  //   y: \"y\",\n  //   fill: \"blue\",\n  //   strokeWidth: 1,\n  //   opacity: show ? 0.5 : 0\n  // }),\n    Plot.line(normal_plot(-4, 4, mean_value, sd_value), {\n      x: \"x\",\n      y: \"y\",\n      strokeWidth: 4\n    }),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\nnormal_sliders = htl.html`${mean_value_slider}${sd_value_slider}`\n\n\n\n\n\n\n\ntexmd`This plot shows a normal distribution with a mean ($\\mu$) of ${mean_value} and a standard deviation ($\\sigma$) of ${sd_value}.`\n\n\n\n\n\n\n\n\n\nWe’ll come to this point later again, but I’ll mention it briefly here. One important thing to note is that when we make adjustments to the \\(\\mu\\) and \\(\\sigma\\) of the normal distribution, the relative position of points (in units of \\(\\sigma\\)) doesn’t change, even though the absolute position of points on the plot do change.\n\nBut sometimes we see distributions that aren’t normal distributions. They’ve departed from this normal distribution shape in some way. There are specific terms for describing departures from the shape we see in the normal distribution. And it’s these terms that we turn our attention to next.\n\n\nDescribing departures from the normal distribution.\nWhen we looked at the example of the game where a players score was based on multiplying together the dice rolls, it produced a distribution that was skewed.\nSkew is actually a technical term that describes one way in which a distribution can deviate from the shape we see with normal distributions. The normal distribution is symmetrical, but a skew distribution is not. A left-skewed distribution has a longer left tail, and a right-skewed distribution has a longer right tail. Use Explorable 4 to explore skewness.\n\n\n\n\n\n\n\nExplorable 4 (Explore Skewness)  \n\nUse the slider to make adjustments to the skewness of the distribution. Slide it to the left to add left-skew and slide it to the right to add right-skew. See how the distribution changes. When the distribution has no skew then it is symmetrical just like a normal distribution.\n\nskew_normal_plot_output = Plot.plot({\n  x: {\n    grid: true\n  },\n  y: {\n    grid: true,\n    domain: [0, 0.8]\n  },\n  marks: [\n    Plot.line(skew_normal_plot(-4, 4, skew), {\n      x: \"x\",\n      y: \"y\",\n      strokeWidth: 4\n    }),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\n\n\nviewof skew = Inputs.range([-10, 10], { label: \"Skewness\", step: 0.5 })\n\n\n\n\n\n\n\nskew_desc = md`This distribution has a **skewness** of ${skew}. It is ${\n  skew == 0 ? \"symmetrical\" : skew &lt; 0 ? \"left-skewed\" : \"right-skewed\"\n}.`\n\n\n\n\n\n\n\n\n\nApart from skew, deviations from the normal distribution can occur when a distribution either has fatter or skinnier tails than the normal distribution. The tailedness of a distribution is given by its kurtosis. The kurtosis of a distribution is often specified with reference to the normal distribution. In this case, what is being reported is excess kurtosis. A distribution with positive excess kurtosis has a higher kurtosis value than the normal distribution, and a distribution with negative excess kurtosis has a lower kurtosis value than the normal distribution.\nDistributions with no excess kurtosis are called mesokurtic. Distributions with negative excess kurtosis are called platykurtic. And distributions with positive excess kurtosis are called leptokurtic.\nIn your research methods courses, you probably won’t come across many distributions that have negative excess kurtosis. However, the distribution that describes the outcomes of a single dice roll is one such distribution. You will encounter distributions with positive excess kurtosis more often. In particular, the t-distribution, a distribution with positive excess kurtosis, will be used in several of the statistical procedures that you will learn about next year. You can use Explorable 5 to explore excess kurtosis.\n\n\n\n\n\n\n\nExplorable 5 (Explore kurtosis)  \n\nUse the slider to adjust the excess kurtosis. A standard normal distribution with an excess kurtosis of 0 is shown for reference.\n\nkurtosis_plot = Plot.plot({\n  x: {\n    grid: true,\n    domain: [-3, 3]\n  },\n  y: {\n    grid: true,\n    domain: [0, 0.4]\n  },\n  marks: [\n    true\n      ? Plot.line(dists.standard_normal, {\n          x: \"x\",\n          y: \"y\",\n          stroke: \"grey\",\n          strokeWidth: 1\n        })\n      : null,\n    Plot.line(dists[Object.keys(kurtosis)[kurtosis_value]], {\n      x: \"x\",\n      y: \"y\",\n      strokeWidth: 4\n    }),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\nviewof kurtosis_value = Inputs.range([0, 6], {\n  step: 1,\n  value: 2,\n  label: \"Excess kurtosis\",\n  format: (d) =&gt; kurtosis_values[d]\n})\n\n\n\n\n\n\n\nkurtosis_desc = md`This distribution has an excess kurtosis of ${\n  kurtosis_values[kurtosis_value]\n}. ${\n  kurtosis_values[kurtosis_value] === 0\n    ? \"It is a **Mesokurtic distribution**.\"\n    : kurtosis_values[kurtosis_value] &gt; 0\n    ? \"It is a **Leptokurtic distribution**.\"\n    : \"It is a **Platykurtic distribution**.\"\n}`"
  },
  {
    "objectID": "lectures/week08/handout/index.html#distributions-and-samples",
    "href": "lectures/week08/handout/index.html#distributions-and-samples",
    "title": "Lecture 8: Distributions",
    "section": "Distributions and samples",
    "text": "Distributions and samples\nNow that we’ve talked a little bit about samples and populations (in Lecture 6 and Lecture 7) and we’ve also covered distributions, we’re going to start putting these ideas together. We’ve seen that whenever we look at the distribution of values where the values are produced by adding up numbers we got something that looked like a normal distribution.\nWhen we covered the sample mean in Lecture 6, the formula was as shown in Equation 1, below:\n\\[\\bar{x}=\\frac{\\displaystyle\\sum^{N}_{i=1}{x_i}}{N} \\tag{1}\\]\nThis formula can be re-written as shown in Equation 2, below:\n\\[\\bar{x}={\\displaystyle\\sum^{N}_{i=1}{\\frac{x_i}{N}}} \\tag{2}\\]\nWhat Equation 2 makes clear is that calculating a mean is just adding up numbers. Now let’s think about taking lots of samples from a population. And for each sample, we calculate the sample mean. If we had to plot these sample means, then what would the distribution look like? We can try it out. Let’s say that I have a population with a mean of 100 and a standard deviation of 15. From this population I can draw samples. Let’s say that each sample will be 25 values (that is, the sample size will be 25). After I’ve collected my sample I’ll work out the sample mean, and I’ll do this 100,000 times and plot the results. You can see this plot in Figure 5.\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nFigure 5: Sample means from 100,000 samples (sample size = 25)\n\n\n\n\nAs you can see in Figure 5 the distribution of sample means is shaped just like a normal distribution. This is exactly what you’d expect. To make it clear that it’s shaped like a normal distribution, in red I’ve drawn the outline of a normal distribution. This normal distribution has a mean of 100, which is just what the mean of our population is. It also has a standard deviation of 3. This distribution is an example of the sampling distribution of the mean. This means that the standard deviation of this distribution is the standard error of the mean because the standard error of the mean is just the standard deviation of the sampling distribution of the mean.\n\nThe central limit theorem\nBefore we get to how we work out the standard error of the mean I just want to assure you of something. You might think that in the example above the sampling distribution of the mean has a normal distribution because we’re drawing samples from a population that has a normal distribution. That is, you might think that because the parent distribution has a normal distribution, the sampling distribution will have a normal distribution.\nBut this is not the case. As long as we’re dealing with the sampling distribution of the mean (that is, a value we calculate by adding up numbers) then we’ll see a sampling distribution that has the shape of a normal distribution. You can explore this fact in Explorable 6\n\n\n\n\n\n\n\nExplorable 6 (The Central Limit Theorem)  \n\nYou can use the selector to select the population distribution. A histogram that matches the shape of the population distribution is shown in Figure 6.\n\nimport {viewof poptype} from \"@ljcolling/central-limit\"\nimport {viewof n} from \"@ljcolling/central-limit\"\nimport {pop_hist_plot} from \"@ljcolling/central-limit\"\nimport {sampling_distrubution} from \"@ljcolling/central-limit\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof poptype\n\n\n\n\n\n\n\npop_hist_plot\n\n\n\n\n\nFigure 6: Distribution of the population\n\n\n\nAfter you’ve selected the population distribution, then you can draw samples from that distribution. You can set the sample size for each sample you draw. We’ll always draw 50,000 samples, because that should be enough for the shape of the sampling distribution to become clear. You can see a plot of the sampling distribution of the mean in Figure 7. When the sample size is 1 then the sampling distribution should look just like the population distribution. But try increasing the sample size and see what happens. What do you notice?\n\nsampling_distrubution\n\n\n\n\n\nFigure 7: Sampling distribution of the mean (50, 000 samples)\n\n\n\n\nviewof n\n\n\n\n\n\n\nWhen the sample size increases then the sampling distribution takes on the shape of a normal distribution. The normal distribution that the sampling distribution approximates is also shown in Figure 7 as a solid blue line2."
  },
  {
    "objectID": "lectures/week08/handout/index.html#the-standard-error-of-the-mean",
    "href": "lectures/week08/handout/index.html#the-standard-error-of-the-mean",
    "title": "Lecture 8: Distributions",
    "section": "The standard error of the mean",
    "text": "The standard error of the mean\n\nIn Lecture 7 we started talking about the spread of sample means around the population mean that occurs when we repeatedly draw samples from the same population. I showed you an example where the average deviation was small (that is, on average the individual sample means were close to the population mean). And I showed you an example where the average deviation was larger (that is, on average the individual sample means were further away from the population mean). We also learned in Lecture 7 that variance is a measure of average (squared) deviation.\nI ended the lecture by asking you to think of two situations where the average deviation (of sample means around the population mean) would be small or 0. If you managed to think of those two situations, then great! But if you didn’t, then here they are:\nFirst, a feature of populations that will make the average deviation of sample means around the population mean be equal to 0. If the variance in the population is 0 then all sample means will be identical to the population mean. Why? Because if the variance in the population is 0 then all members of the population will be identical. The typical value (the mean) any sample will have to be identical to the population mean, because there’s only one value anything can take.\nSecond, a feature of the samples that will make the average deviation of samples means around the population mean be equal to 0. If the sample size is so large that it includes the entire population then the sample mean would, by definition, be equal to the population mean. So if all your samples are this large then all the sample means will be equal to the population mean. And this means that the average deviation of sample means around the population would be equal to 0.\nThis means that if we want to compute the average (squared) deviation of sample means around the population mean then it will depend on two factors. First, when the variance of the population (\\(\\sigma^2\\)) is small then the variance of sample means around the population mean will be small. Conversely, when the variance of the population is larger then we’d expect the variance of sample means around the population mean to be larger. Second, if the sample size (n) is large then the variance of the sample means around the population mean will be small. Conversely, when the sample is small then we’d expect the variance of sample means around the population mean to be larger.\nThe only way to combine \\(\\sigma^2\\) and \\(n\\) that matches these observations is as shown in Equation 3, below:\n\\[\\frac{\\sigma^2}{n} \\tag{3}\\]\nThis formula gives us the variance of the sample means around the population mean. But I said that the standard error of the mean is the standard deviation of the sample means around the population mean. That means that we just need to take the square root of the formula in Equation 3. And because we don’t actually know the value of \\(\\sigma\\) (the variance of the population) and only know our estimate of it, the sample variance (s), we need to replace \\(\\sigma\\) with s. This gives us the equation in Equation 4, below:\n\\[\\frac{s}{\\sqrt{n}} \\tag{4}\\]\nThis is exactly the formula for the standard error of the mean.\nNow this is, admittedly, a fairly long winded way to get to what is essentially a very simple formula. However, as I have alluded to several times, the standard error of the mean is a fairly misunderstood concept. Part of the reason for this is that it’s often just presented as a formula. I hope that getting there the long way has helped you to build a better intuition of what the standard error of the mean actually is.\n\n\nn_heads = jstat(0, coins - 1, coins + 1 - 1)[0].map((v) =&gt; {\n  return {\n    x: v,\n    y: jstat.binomial.pdf(v, coins - 1, 0.5) * 2 ** (coins - 1)\n  };\n})\n\n\n\n\n\n\n\nvega = require(\"https://cdn.jsdelivr.net/npm/vega@5/build/vega.js\")\n\n\n\n\n\n\n\ncoin_data = [\n  { name: \"START\", id: 1, parent: \"\", color: \"red\" },\n  ...d3.range(2, 2 ** coins).map((i) =&gt; {\n    return {\n      name: i % 2 ? \"T\" : \"H\",\n      id: i,\n      parent: Math.floor(i / 2)\n    };\n  })\n].map((x) =&gt; {\n  let colors = { H: \"black\", T: \"white\", START: \"red\" };\n  x.color = colors[x.name];\n  return x;\n})\n\n\n\n\n\n\n\nparsedSpec = {\n  return vega.parse(spec3);\n}\n\n\n\n\n\n\n\nspec3 = {\n  return {\n    $schema: \"https://vega.github.io/schema/vega/v5.0.json\",\n    padding: 0,\n    width: 500,\n    height: 100,\n    layout: {\n      padding: 0,\n      columns: 1\n    },\n    marks: [\n      {\n        type: \"group\",\n        encode: {\n          update: {\n            width: {\n              value: 1000\n            },\n            height: {\n              value: 130\n            }\n          }\n        },\n        data: [\n          {\n            name: \"tree\",\n            values: coin_data,\n            transform: [\n              {\n                type: \"stratify\",\n                key: \"id\",\n                parentKey: \"parent\"\n              },\n              {\n                type: \"tree\",\n                method: \"tidy\",\n                size: [500, 200],\n                as: [\"x\", \"y\", \"depth\", \"children\"]\n              }\n            ]\n          },\n          {\n            name: \"links\",\n            source: \"tree\",\n            transform: [\n              {\n                type: \"treelinks\",\n                key: \"id\"\n              },\n              {\n                type: \"linkpath\",\n                orient: \"horizontal\",\n                shape: \"line\"\n              }\n            ]\n          }\n        ],\n        scales: [\n          {\n            name: \"color\",\n            domain: [0, 1, 2, 3, 4, 5],\n            type: \"sequential\",\n            range: \"ramp\"\n          }\n        ],\n        marks: [\n          {\n            type: \"path\",\n            from: {\n              data: \"links\"\n            },\n            encode: {\n              update: {\n                path: {\n                  field: \"path\"\n                },\n                stroke: {\n                  value: \"black\"\n                }\n              }\n            }\n          },\n          {\n            type: \"symbol\",\n            from: {\n              data: \"tree\"\n            },\n            encode: {\n              enter: {\n                size: {\n                  value: 50\n                },\n                stroke: {\n                  value: \"black\"\n                }\n              },\n              update: {\n                x: {\n                  field: \"x\"\n                },\n                y: {\n                  field: \"y\"\n                },\n                fill: {\n                  field: \"color\"\n                }\n              }\n            }\n          },\n          {\n            type: \"text\",\n            from: {\n              data: \"tree\"\n            },\n            encode: {\n              enter: {\n                text: {\n                  field: \"name\"\n                },\n                fontSize: {\n                  value: 0\n                },\n                baseline: {\n                  value: \"bottom\"\n                }\n              },\n              update: {\n                x: {\n                  field: \"x\"\n                },\n                y: {\n                  field: \"y\"\n                }\n              }\n            }\n          }\n        ]\n      }\n    ]\n  };\n}\n\n\n\n\n\n\n\njstat = require(\"https://bundle.run/jstat@1.9.4\")\n\n\n\n\n\n\n\ndicedata = {\n  return d3.sort(\n    dist.six_dice_roll_histogram(n_dice, n_players).counts,\n    (d) =&gt; d.value\n  );\n}\n\n\n\n\n\n\n\nd = {\n  return Array(n_players_mult)\n    .fill(0)\n    .map((x) =&gt; {\n      return {\n        x: Number(\n          Array.from(dist.six_dice_roll(1, n_dice_mult)).reduce(\n            (state, item) =&gt; state * item\n          )\n        )\n      };\n    });\n}\n\n\n\n\n\n\n\nimport { dist } from \"@ljcolling/wasm-distributions\"\n\n\n\n\n\n\n\nsd_value_slider = Inputs.range([0.5, 2], {\n  value: 1,\n  step: 0.25,\n  label: htl.html`standard deviation &lt;br /&gt;&#x3C3;`\n})\n\n\n\n\n\n\n\nimport { texmd } from \"@kelleyvanevert/katex-within-markdown\"\n\n\n\n\n\n\n\nmean_value_slider = Inputs.range([-3, 3], {\n  value: 0,\n  step: 0.25,\n  label: htl.html`mean&lt;br /&gt;  &#x3BC`\n})\n\n\n\n\n\n\n\nsd_value = Generators.input(sd_value_slider)\n\n\n\n\n\n\n\nmean_value = Generators.input(mean_value_slider)\n\n\n\n\n\n\n\n// jStat.normal.pdf( x, mean, std )\nnormal_plot = (min, max, mean, sd) =&gt; {\n  // jStat.normal.pdf(x, mean, sd)\n\n  return d3.ticks(min, max, 501).map((v) =&gt; {\n    return {\n      x: v,\n      y: dnorm(v, mean, sd)\n    };\n  });\n}\n\n\n\n\n\n\n\nskew_normal_plot = (min, max, alpha) =&gt; {\n  // jStat.normal.pdf(x, mean, sd)\n\n  return d3.ticks(min, max, 201).map((v) =&gt; {\n    return {\n      x: v,\n      y: dsn(v, alpha)\n    };\n  });\n}\n\n\n\n\n\n\n\ndsn = (x, alpha) =&gt; {\n  // set the defaults\n\n  const xi = 0;\n  const omega = 1;\n  const tau = 0;\n\n  let z = (x - xi) / omega;\n\n  let logN = -Math.log(Math.sqrt(2 * Math.PI)) - 0 - Math.pow(z, 2) / 2;\n\n  let logS = Math.log(\n    jStat.normal.cdf(tau * Math.sqrt(1 + Math.pow(alpha, 2)) + alpha * z, 0, 1)\n  );\n\n  let logPDF = logN + logS - Math.log(jStat.normal.cdf(tau, 0, 1));\n\n  return Math.exp(logPDF);\n}\n\n\n\n\n\n\n\n// import jStat library\njStat = require(\"jStat\")\n\n\n\n\n\n\n\nkurtosis = {\n  return {\n    uniform: -(6 / 5),\n    raised_cosine: (6 * (90 - Math.PI ** 4)) / (5 * (Math.PI ** 2 - 6) ** 2),\n    standard_normal: 0,\n    t_dist30: 6 / (30 - 4),\n    t_dist20: 6 / (20 - 4),\n    t_dist10: 6 / (10 - 4),\n    t_dist7: 6 / (7 - 5),\n    t_dist5: 6 / (5 - 4)\n  };\n}\n\n\n\n\n\n\n\nkurtosis_values = Object.values(kurtosis).map((v) =&gt; Math.round(v * 100) / 100)\n\n\n\n\n\n\n\ndists = {\n  return {\n    raised_cosine: d3.ticks(-3, 3, 500).map((v) =&gt; {\n      return {\n        x: v,\n        y: dist.raised_cosine(v, 0, 2.5)\n      };\n    }),\n    standard_normal: d3.ticks(-3, 3, 500).map((v) =&gt; {\n      return {\n        x: v,\n        y: dnorm(v, 0, 1)\n      };\n    }),\n    t_dist30: d3.ticks(-3, 3, 500).map((v) =&gt; {\n      return {\n        x: v,\n        y: dist.dt(v, 30)\n      };\n    }),\n    t_dist20: d3.ticks(-3, 3, 500).map((v) =&gt; {\n      return {\n        x: v,\n        y: dist.dt(v, 20)\n      };\n    }),\n    t_dist10: d3.ticks(-3, 3, 500).map((v) =&gt; {\n      return {\n        x: v,\n        y: dist.dt(v, 10)\n      };\n    }),\n    t_dist7: d3.ticks(-3, 3, 500).map((v) =&gt; {\n      return {\n        x: v,\n        y: dist.dt(v, 7)\n      };\n    }),\n    t_dist5: d3.ticks(-3, 3, 500).map((v) =&gt; {\n      return {\n        x: v,\n        y: dist.dt(v, 5)\n      };\n    }),\n\n    uniform: d3.ticks(-2.1, 2.1, 500).map((v) =&gt; {\n      return {\n        x: v,\n        y: dist.dunif(v, -2, 2)\n      };\n    })\n  };\n}"
  },
  {
    "objectID": "lectures/week08/handout/index.html#check-your-understanding",
    "href": "lectures/week08/handout/index.html#check-your-understanding",
    "title": "Lecture 8: Distributions",
    "section": "Check your understanding",
    "text": "Check your understanding\nUse this quiz to make sure that you’ve understood the key concepts.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunction dnorm(x, mean, sd) {\n  return jstat.normal.pdf(x, mean, sd);\n}"
  },
  {
    "objectID": "lectures/week08/handout/index.html#footnotes",
    "href": "lectures/week08/handout/index.html#footnotes",
    "title": "Lecture 8: Distributions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOr subtracting, because subtracting is just adding negative numbers↩︎\nNote that the sampling distribution and the normal distribution shown in the plot won’t always match perfectly even when the sample size is large. This is because the normal distribution is an idealisation of what happens if you collect an infinite number of samples. But we’re not collecting an infinite number. We’re only collecting 50,000↩︎"
  },
  {
    "objectID": "lectures/week09/handout/index.html",
    "href": "lectures/week09/handout/index.html",
    "title": "Lecture 9: Transformation and comparisons",
    "section": "",
    "text": "In last week’s lecture we started learning a little bit about distributions. We learned about the normal distribution, and where it comes from. And we also learned a little bit about the sampling distribution, and why knowing the sampling distribution might be useful.\nThis week we’ll talk a little bit more about distributions and why the normal distribution is particularly useful."
  },
  {
    "objectID": "lectures/week09/handout/index.html#the-shape-of-things",
    "href": "lectures/week09/handout/index.html#the-shape-of-things",
    "title": "Lecture 9: Transformation and comparisons",
    "section": "The shape of things",
    "text": "The shape of things\nIf we were to measure the height of 1000 women and plot the values then we might get something like the plot in Figure 1. As you can see, the vast majority of the measured heights are in the 155–175 centimetre range. Only a small number of people fall outside of this range. You can also see that the distribution is roughly symmetrical around its mean (165 cm) and it has a shape characteristic of a normal distribution.\n\n\n\n\n\nFigure 1: Distribution of heights in a sample of 1000 women. Not real data.\n\n\n\n\nOf course it doesn’t look exactly like a normal distribution, because, as we saw in Lecture 8, a normal distribution is smooth line. Our plot is a histogram where we’ve just counted up the number of people that fall into each 5 centimetre bin. However, we could image measuring the heights of more and more people and making the bins narrower and narrower. In Figure 2 we can see what the histogram might look like if we were to measure 100,000 women.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\nFigure 2: Distribution of heights in a sample of 100,000 women (Not real data) and the corresponding normal distribution\n\n\n\n\nIn Figure 2 you can also see the what the corresponding normal distribution looks like. This idealised representation is a normal distribution with a mean of 165 and a standard deviation of 10. Although the normal distribution is an idealisation, or an abstraction, we can use it to do some very useful things."
  },
  {
    "objectID": "lectures/week09/handout/index.html#the-standard-normal-distribution",
    "href": "lectures/week09/handout/index.html#the-standard-normal-distribution",
    "title": "Lecture 9: Transformation and comparisons",
    "section": "The standard normal distribution",
    "text": "The standard normal distribution\nWhen we were first introduced to the normal distribution last week we saw that there were two parameters that we could change (\\(\\mu\\) and \\(\\sigma\\)) that changed where the normal distribution was centered and how spread out it was. When \\(\\mu=0\\) and \\(\\sigma=1\\), then the normal distribution is called the standard normal distribution. You can explore the normal distribution again in Explorable 1.\nI said in Lecture 8 that when you adjust the \\(\\mu\\) and \\(\\sigma\\) values then the absolute positions of points on the plot change, but the relative position doesn’t change.\nTo understand what I mean by this, we’ll use an example. Let’s take the heights of people that we plotted in Figure 1. In this example we measures height in centimeters. It should be pretty obvious that your height doesn’t change depending on the units you measure it in. You’re the same height whether you get measured in centimetres, metres, millimetres, feet, or inches.\nIf we measured height of the sample of women in metres instead of centimetres, the shape of the plot should remain the same. You can see this in Figure 3.\n\n\n\n\n\n\n\n(a) Measured in centimetres\n\n\n\n\n\n\n\n(b) Measured in metres\n\n\n\n\nFigure 3: Distribution of heights in a sample of 1000 women. Not real data.\n\n\nThe distribution in Figure 3 (a) has a standard deviation of 10 while the distribution in Figure 3 (b) has a standard deviation of 0.1. But as you can see, they’re they same distributions—they’re just displayed on different scales (centimetres versus metres).\n\n\n\n\n\n\nNote\n\n\n\nChanging the scale changes the standard deviation. This is why the standard deviation is sometimes referred to as the scale parameter for the distribution.\n\n\nWe’ve seen how we can change the scale of the distribution, by measuring it in metres instead of centimetres. But we can also change the where the distribution is centred. We can see an example of this in Figure 4.\n\n\n\n\n\n\n\n(a) Measured in centimetres\n\n\n\n\n\n\n\n(b) Measured in difference from the average height\n\n\n\n\nFigure 4: Distribution of heights in a sample of 1000 women. Not real data.\n\n\nIn Figure 4 (a) we can see the same distribution as before. But in Figure 4 (b) we can see a distribution that is now centred at 0. In this distribution we’ve just changed where the centred is located, but the distribution is still the same.\n\n\n\n\n\n\nNote\n\n\n\nChanging the mean changes where the centre of the distribution is located. This is why the mean is sometimes referred to as the location parameter for the distribution.\n\n\nThe fact that the relative positions of points don’t change is a useful property. In the standard normal distribution, ~68% of the distribution falls between -1 and +1. Or, put into relative terms, ±1 \\(\\sigma\\) from \\(\\mu\\). And ~68% of the distribution will always fall between ±1 \\(\\sigma\\) from \\(\\mu\\) no matter what value you set for \\(\\sigma\\) and \\(\\mu\\). You can explore this in Explorable 1.\n\n\n\n\n\n\n\nExplorable 1 (The normal distribution)  \n\n\nnormal_plot_output = Plot.plot({\n  x: {\n    grid: true,\n    domain: [-4, 4]\n  },\n  y: {\n    grid: true,\n    domain: [0, 0.8]\n  },\n  marks: [\n   Plot.line(\n     normal_plot(\n       mean_value - sd_value * s &gt; -4 ? mean_value - sd_value * s : -4,\n       mean_value + sd_value * s &lt; 4 ? mean_value + sd_value * s : 4,\n       mean_value,\n       sd_value\n     ),\n     {\n       x: \"x\",\n       y: \"y\",\n       strokeWidth: 1,\n       fill: \"blue\",\n       opacity: show ? 0.5 : 0\n     }\n   ),\n   Plot.line(fill_limits(s), {\n     x: \"x\",\n     y: \"y\",\n     fill: \"blue\",\n     strokeWidth: 1,\n     opacity: show ? 0.5 : 0\n   }),\n    Plot.line(normal_plot(-4, 4, mean_value, sd_value), {\n      x: \"x\",\n      y: \"y\",\n      strokeWidth: 4\n    }),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\nnormal_sliders = htl.html`${mean_value_slider}${sd_value_slider}`\n\n\n\n\n\n\n\n{\n  if(mean_value === 0 & sd_value === 1) {\n    return texmd`This plot shows the **standard normal** distribution.`\n  } else {\n    return texmd`This plot shows a normal distribution with a mean ($\\mu$) of ${mean_value} and a standard deviation ($\\sigma$) of ${sd_value}.`\n\n  }\n\n}\n\n\n\n\n\n\nYou can make adjustments to \\(\\mu\\) and \\(\\sigma\\) to change the centre and scale of the normal distribution.\n\nviewof show = Inputs.toggle({ label: \"Show coverage\", value: false })\n\n\n\n\n\n\n\nviewof s = Inputs.range([0, 3], { step: 0.5, value: 1 })\n\n\n\n\n\n\n\n\n\ntexmd`Approximately ~68% of the distribution will fall between ±1 $\\sigma$ of $\\mu$.\n\nToggle **Show coverage** to highlight the region corresponding to ±$ ${s} $\\sigma$ \nfrom $\\mu$. This covers ~${coverage(s)}% of the distribution.\n\nMake adjustments to $\\mu$ and $\\sigma$. See how the highlighted region covers\nthe region from ${mean_value - s * sd_value} to ${mean_value + s * sd_value}, or\n(${mean_value} - [${s} × ${sd_value}]) to (${mean_value} + [${s} × ${sd_value}]).\n`\n\n\n\n\n\n\n\nfunction coverage(s) {\nlet cov = jstat.normal.cdf(s, 0, 1) * 2 - 1\nreturn Math.round(cov * 100)\n}"
  },
  {
    "objectID": "lectures/week09/handout/index.html#transformations",
    "href": "lectures/week09/handout/index.html#transformations",
    "title": "Lecture 9: Transformation and comparisons",
    "section": "Transformations",
    "text": "Transformations\nIn Figure 3 and Figure 4 we saw how we could transform a variable so that the shape of the distribution stayed the same, but the mean and the standard deviation changed. These two kinds of transformations are known as centring and scaling.\n\nCentering\nCentring is performed by subtracting a fixed value from each observation in our dataset. This has the effect of shifting the distribution of our variable along the x-axis. You can technically centre a variable but subtracting any value from it but the most frequently used method is mean-centring.\nThis is shown in Equation 1, below:\n\\[x_i - \\bar{x} \\tag{1}\\]\n\n\n\n\n\n\nTip\n\n\n\nApplying this transformation results in shifting the variable so that it’s mean is at the zero point and the individual values of the mean-centred variable tell us how far that observation is from the mean of the entire variable.\n\n\nIt’s crucially important to understand that mean-centring does not alter the shape of the variable, nor does it change the scale at which the variable is measured. It only changes the interpretation of the values from the raw scores to differences from the mean.\n\n\nScaling\nScaling is performed by dividing each observation by some fixed value. This has the effect of stretching or compressing the variable along the x-axis.\nJust like centring, you can technically scaled a variable by dividing it by any value. For example, we created Figure 3 (b) by taking the values in Figure 3 (a) and dividing them by 100 to transform the height in centimetres to a height in metres. However, the most frequent method of scaling is by dividing values by the standard deviation of the dataset. This is shown in Equation 2, below:\n\\[\\frac{x_i}{s} \\tag{2}\\]\nJust like with centring, the fundamental shape of the variable’s distribution did not change as a result of scaling. After scaling the data by the standard deviation the values would now be measured in units of sd.\n\n\n\n\n\n\nTip\n\n\n\nUnlike centring, however, scaling does change the scale, or units, on which the variable is measured. After all, that’s why it’s called scaling.\n\n\n\n\nThe z-transform\nThe combination of first mean-centering a variable and then scaling the variable by its standard deviation is known as the z-transform. The formula for this is shown in Equation 3, below:\n\\[z(x) = \\frac{x_i - \\bar{x}}{s} \\tag{3}\\]\nWe can see an example of how to z-transform some data in Table 1. The 10 values in Table 1 have a mean of 5.7 and a standard deviation of 2.21. In the column labelled centred, the 5.7 has been subtracted from the raw values. If we were to work out the mean of this column the value would be 0. The column labelled scaled contains the values in centred but divided by 2.21. If you were to work out the mean of this column is would still be 0. And if you were to work out the standard deviation of this column it would now be 1.1\n\n\n\n\nTable 1: z transformed data\n\n\nRaw values\nCentred\nScaled\n\n\n\n\n4\n-1.7\n-0.77\n\n\n6\n0.3\n0.14\n\n\n6\n0.3\n0.14\n\n\n9\n3.3\n1.49\n\n\n1\n-4.7\n-2.12\n\n\n7\n1.3\n0.59\n\n\n5\n-0.7\n-0.32\n\n\n8\n2.3\n1.04\n\n\n5\n-0.7\n-0.32\n\n\n6\n0.3\n0.14\n\n\n\n\n\n\nWhen we’ve z-transformed data2 we can now interpret the data in terms of distance from the mean in units of standard deviation.\nBeing able to do this makes it easier to make comparisons. And making comparisons is where we turn our attention to next."
  },
  {
    "objectID": "lectures/week09/handout/index.html#making-comparisons",
    "href": "lectures/week09/handout/index.html#making-comparisons",
    "title": "Lecture 9: Transformation and comparisons",
    "section": "Making comparisons",
    "text": "Making comparisons\n\nComparing groups\nWhen we talk about comparing groups on some variable in the context of quantitative research we are most often talking about looking at the average difference in the variable between the groups. In other words, we are asking, how different the groups are on average.\nLet’s make it concrete with an example. Suppose we’re interested in amateur and professional sportspeople on a simple target detection task where participants have to press a button as quickly as possible when a particular stimulus appears on computer screen. We get 500 amateur and 500 professional sportspeople to participant in this experiment. The results of this experiment show that amateurs have a mean reaction time of 500 ms and professionals have a mean reaction time of 460 ms. Figure 5 shows the histogram of these data.\n\n\n\n\n\nFigure 5: Distribution of reaction times in a sample of amateur (green) and 500 professional (blue) sportspeople. Group means are indicated with the vertical lines.\n\n\n\n\nIn this example we can see that there is a lot of overlap between the two groups. However, we can also clearly see that there is a difference in the average reaction times between amateur and professional sportspeople. To quantify this difference, all we would need to do is to subtract the mean of one group from the mean of the other group.\nSo for our example in Figure 5, the mean of the amateurs is 500 ms and the mean of the professionals is 460, so the mean difference is just 500ms - 460ms = 40ms. Because this difference is positive, the amateurs have a reaction time that is 40ms higher (slower) than the professionals.\nThe sign indicates the direction of the difference. If the number is positive, that means that the first group’s mean is larger than that of the second group. If the number is negative, the opposite is true. Of course, it is completely arbitrary which group is first and which is second.\n\n\nComparison across groups\nIn the example above, the comparison was easy to make because the two measurements were measured on the same scale. But sometimes we want to compare measurements that are measured on different scales.\nSuppose you’re interested in comparing the performance of two children on a puzzle completion tasks. One child is 8 years old and the other is 14 years old. Because 8-year-olds and 14-years-olds are at different developmental stages there are two versions of the task that are scored in slightly different ways. Because we now have two tests that might have a different number of items, and that might be scored in different ways, we can’t just compare the two numbers to see which is bigger. So what do we do instead? We’ll explore this with and example.\nAhorangi is 8 years old, and she got a score of 86. Benjamin is 14 years old, and he got a score of 124. We can easily tell that Benjamin got a higher score than Ahorangi. But the scores are not directly comparable, because they’re measured on different scales. So how can we compare them? What we need to do instead is look at how each of them performed relative to their age groups. Is Ahorangi better performing relative to 8-year-olds than Benjamin is relative to 14-year-olds?\nTo answer this question we can use the z-scores we learned about earlier. By standardising the time variable across each group, we get variables that are on the same scale. Do do this, we’ll need to know the mean and standard deviation for each of the age groups. We can see these details in Table 2.\n\n\n\n\nTable 2: Means and Standard deviations for the 8-year-old and 14-year-old age groups\n\n\nAge group\nMean\nStandard deviation\n\n\n\n\n8-year-olds\n80\n2\n\n\n14-year-olds\n120\n8\n\n\n\n\n\n\nLet’s use this formula to calculate Ahorangi and Benjamin’s z-score. First, for Ahorangi:\n\\[3 = \\frac{86 - 80}{2}\\]\nAnd next for Benjamin:\n\\[0.5 = \\frac{124 - 120}{8}\\]\nSo we now know that Ahorangi’s z-score is 3, and that Benjamin’s z-score is 0.5. This means that Ahorangi’s score is 3 standard deviations higher than the average 8-year-old. Benjamin’s z-score is rb_z higher than the average 14-year-old. That means, that Ahorangi, despite having a lower score, actually scored very high for an 8-year-old. Benjamin, on the other hand, only scored a little higher than the average 14-year-old."
  },
  {
    "objectID": "lectures/week09/handout/index.html#making-comparisons-with-the-sampling-distribution",
    "href": "lectures/week09/handout/index.html#making-comparisons-with-the-sampling-distribution",
    "title": "Lecture 9: Transformation and comparisons",
    "section": "Making comparisons with the sampling distribution",
    "text": "Making comparisons with the sampling distribution\nThe final kind of comparison we’ll talk about is the comparison between our sample and the sampling distribution. Last week learned about the sampling distribution. And we learned that the sampling distribution of the mean will be centred at the population mean and have a standard deviation that is equal to the standard error of the mean.3\nAs I’ve already emphasised, you don’t know the value of the population mean, which means that we won’t know what the sampling distribution of the mean looks like for our particular population. So what use then is the sampling distribution?\nAlthough we won’t know the population mean we can generate a hypothesis about what we think the population mean might be. We can then use this hypothesis about the population mean value, together with the standard error, to generate a sampling distribution. The sampling distribution describes what would happen if we were to repeatedly take samples from the population and work out the mean.\nI’ll make this concrete by way of an example. Let’s say that interested in whether people are quicker at recognising the faces of family members versus the faces of celebrities. You get your participants to perform a task whether they are shown faces and they have to press a button whenever the recognise them. You find that the mean difference between these two conditions is 24.87ms. But this is just the mean difference is your sample. The mean difference in the population might be some other value that is smaller or larger.\nNow we don’t know the population mean difference, but we could hypothesise a value. For example, we might hypothesise that the population mean difference is 100 ms, 50 ms, 0 ms, or some other value. Once we have hypothesised a value, we can generate a sampling distribution. Our sampling distribution will be centered at our hypothesised value, and it’ll have a standard deviation equal to the standard error of the mean. For our experiment, we had 50 participants, so our sample size is 50. Our sample also had a standard deviation of 62.76. Together we can work out a standard error of the mean that is equal to \\(\\frac{62.76}{\\sqrt{50}}\\) or 8.88.\nNow that we have a hypothesis about the population mean, and we have an estimate of the standard error of the mean, we can create a sampling distribution that tells us what would happen if we were to run the experiment many times. In Figure 6 we can see what the sampling distribution would look like if the population mean were 0.\n\n\n\n\n\nFigure 6: The sampling distribution with a mean of 0 and a SEM of 8.88\n\n\n\n\nWe can now compare our particular sample mean of 24.87ms to this sampling distribution. Because the sampling distribution is a normal distribution we know that ~68% of the time the sample means will fall between ±1 SEM of the population mean. Or, between -8.88ms and 8.88ms. And ~95% of the time sample means will fall between -17.76ms and 17.76ms.\nIf we look at our particular mean we see that it falls 2.8 SEM from our hypothesised population mean. What can we make of this? From this we can conclude that if the population mean were in fact 0, then we have observed something rare. That is, if the population mean were in fact 0, it would be rare for a sample mean (calculated from a sample drawn from this population) to be that far away from the population mean. Observing something rare does not in itself tell us that our hypothesis about the population mean is wrong. After all rare events, like people winning the lottery, happen every day. On a more mundane level, we might be confident that men are (on average) taller than women. But as anybody who has attended a professional netball will tell you, it can certainly happen that you come across a group of people were all the women are taller than the men, although it would be rare and surprising.\nThe same goes for our sample. Observing something rare (according to our hypothesised population mean) might just mean we’ve observed some rare. But if we were to run our experiments again and again, and we continued to observe rare events then we would probably have a good reason to update our hypothesis.\nThis process, where we compared our sample value to the sampling distributed constructed from a hypothesised value is known as null hypothesis significance testing, will be a major topic that you’ll cover next year."
  },
  {
    "objectID": "lectures/week09/handout/index.html#check-your-understanding",
    "href": "lectures/week09/handout/index.html#check-your-understanding",
    "title": "Lecture 9: Transformation and comparisons",
    "section": "Check your understanding",
    "text": "Check your understanding\nUse this quiz to make sure that you’ve understood the key concepts.\n\nvega = require(\"https://cdn.jsdelivr.net/npm/vega@5/build/vega.js\")\n\n\n\n\n\n\n\njstat = require(\"https://bundle.run/jstat@1.9.4\")\n\n\n\n\n\n\n\nimport { dist } from \"@ljcolling/wasm-distributions\"\n\n\n\n\n\n\n\nsd_value_slider = Inputs.range([0.5, 2], {\n  value: 1,\n  step: 0.25,\n  label: htl.html`standard deviation &lt;br /&gt;&#x3C3;`\n})\n\n\n\n\n\n\n\nimport { texmd } from \"@kelleyvanevert/katex-within-markdown\"\n\n\n\n\n\n\n\nmean_value_slider = Inputs.range([-3, 3], {\n  value: 0,\n  step: 0.25,\n  label: htl.html`mean&lt;br /&gt;  &#x3BC`\n})\n\n\n\n\n\n\n\nsd_value = Generators.input(sd_value_slider)\n\n\n\n\n\n\n\nmean_value = Generators.input(mean_value_slider)\n\n\n\n\n\n\n\n// jStat.normal.pdf( x, mean, std )\nnormal_plot = (min, max, mean, sd) =&gt; {\n  // jStat.normal.pdf(x, mean, sd)\n\n  return d3.ticks(min, max, 501).map((v) =&gt; {\n    return {\n      x: v,\n      y: dnorm(v, mean, sd)\n    };\n  });\n}\n\n\n\n\n\n\n\nskew_normal_plot = (min, max, alpha) =&gt; {\n  // jStat.normal.pdf(x, mean, sd)\n\n  return d3.ticks(min, max, 201).map((v) =&gt; {\n    return {\n      x: v,\n      y: dsn(v, alpha)\n    };\n  });\n}\n\n\n\n\n\n\n\ndsn = (x, alpha) =&gt; {\n  // set the defaults\n\n  const xi = 0;\n  const omega = 1;\n  const tau = 0;\n\n  let z = (x - xi) / omega;\n\n  let logN = -Math.log(Math.sqrt(2 * Math.PI)) - 0 - Math.pow(z, 2) / 2;\n\n  let logS = Math.log(\n    jStat.normal.cdf(tau * Math.sqrt(1 + Math.pow(alpha, 2)) + alpha * z, 0, 1)\n  );\n\n  let logPDF = logN + logS - Math.log(jStat.normal.cdf(tau, 0, 1));\n\n  return Math.exp(logPDF);\n}\n\n\n\n\n\n\n\n// import jStat library\njStat = require(\"jStat\")\n\n\n\n\n\n\n\nkurtosis = {\n  return {\n    uniform: -(6 / 5),\n    raised_cosine: (6 * (90 - Math.PI ** 4)) / (5 * (Math.PI ** 2 - 6) ** 2),\n    standard_normal: 0,\n    t_dist30: 6 / (30 - 4),\n    t_dist20: 6 / (20 - 4),\n    t_dist10: 6 / (10 - 4),\n    t_dist7: 6 / (7 - 5),\n    t_dist5: 6 / (5 - 4)\n  };\n}\n\n\n\n\n\n\n\nkurtosis_values = Object.values(kurtosis).map((v) =&gt; Math.round(v * 100) / 100)\n\n\n\n\n\n\n\ndists = {\n  return {\n    raised_cosine: d3.ticks(-3, 3, 500).map((v) =&gt; {\n      return {\n        x: v,\n        y: dist.raised_cosine(v, 0, 2.5)\n      };\n    }),\n    standard_normal: d3.ticks(-3, 3, 500).map((v) =&gt; {\n      return {\n        x: v,\n        y: dnorm(v, 0, 1)\n      };\n    }),\n    t_dist30: d3.ticks(-3, 3, 500).map((v) =&gt; {\n      return {\n        x: v,\n        y: dist.dt(v, 30)\n      };\n    }),\n    t_dist20: d3.ticks(-3, 3, 500).map((v) =&gt; {\n      return {\n        x: v,\n        y: dist.dt(v, 20)\n      };\n    }),\n    t_dist10: d3.ticks(-3, 3, 500).map((v) =&gt; {\n      return {\n        x: v,\n        y: dist.dt(v, 10)\n      };\n    }),\n    t_dist7: d3.ticks(-3, 3, 500).map((v) =&gt; {\n      return {\n        x: v,\n        y: dist.dt(v, 7)\n      };\n    }),\n    t_dist5: d3.ticks(-3, 3, 500).map((v) =&gt; {\n      return {\n        x: v,\n        y: dist.dt(v, 5)\n      };\n    }),\n\n    uniform: d3.ticks(-2.1, 2.1, 500).map((v) =&gt; {\n      return {\n        x: v,\n        y: dist.dunif(v, -2, 2)\n      };\n    })\n  };\n}\n\n\n\n\n\n\n\nfill_limits = (mult) =&gt; {\n  let s = sd_value * mult;\n  return [\n    { x: mean_value - s &gt; -4 ? mean_value - s : -4, y: 0 },\n    normal_plot(\n      mean_value - s &gt; -4 ? mean_value - s : -4,\n      mean_value - s &gt; -4 ? mean_value - s : -4,\n      mean_value,\n      sd_value\n    )[0],\n    normal_plot(\n      mean_value + s &lt; 4 ? mean_value + s : 4,\n      mean_value + s &lt; 4 ? mean_value + s : 4,\n      mean_value,\n      sd_value\n    )[0],\n    { x: mean_value + s &lt; 4 ? mean_value + s : 4, y: 0 }\n  ];\n}\n\n\n\n\n\n\n\nfunction dnorm(x, mean, sd) {\n  return jstat.normal.pdf(x, mean, sd);\n}"
  },
  {
    "objectID": "lectures/week09/handout/index.html#footnotes",
    "href": "lectures/week09/handout/index.html#footnotes",
    "title": "Lecture 9: Transformation and comparisons",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI won’t include the R code for how you might z score some data in a tibble. Instead, I’ll leave that are an exercise for you to figure out↩︎\nAlso called standardising, because the mean is 0 and the standard deviation is 1, just like the standard normal distribution↩︎\nThis is a slight simplification. The standard error of the mean is an estimate of the standard deviation of the sampling distribution. This estimate is biased, so in reality you would make a slight modification to the normal distribution to correct for this bias. You’ll learn about this next year when you cover the t-test.↩︎"
  },
  {
    "objectID": "lectures/week11/handout/index.html",
    "href": "lectures/week11/handout/index.html",
    "title": "Lecture 11: Introduction to probability",
    "section": "",
    "text": "A good understanding of probability is important not only for understanding science but also for understanding and making sense of the world. Unfortunately, probability is poorly understood and, as a result, people tend to reason quite poorly about probabilities. Some of this faulty reasoning can have real world impacts. Therefore, it’s important that you understand probability correctly, so that you don’t also fall for these fallacies.\nWe’ll start off this lecture by asking a seemingly simple question."
  },
  {
    "objectID": "lectures/week11/handout/index.html#different-views-of-probability",
    "href": "lectures/week11/handout/index.html#different-views-of-probability",
    "title": "Lecture 11: Introduction to probability",
    "section": "Different views of probability",
    "text": "Different views of probability\nWhat do we mean by “probability”?\nIt might seem like there’s an easy answer to this question, but there’s at least three senses of probability.\nThese different senses after often employed in different contexts, because they make more sense in some contexts and not others\nThe three I’ll cover are:\n\nThe classical view of probability\nThe frequency view of probability\nThe subjective view of probability\n\n\nThe classical view of probability\nThe classical view is often used in the context of games of chance like roulette and lotteries\nWe can sum it up as follows:\n\nIf we have an (exhaustive) list of events that can be produce by some (exhaustive) list of equally possible outcomes (the number of events and outcomes need not be the same), the probability of a particular event occurring is just the proportion of outcomes that produce that event.\n\nTo make it concrete we can think about flipping coins. If we flip two coins then the possible outcomes that can occur are:\n\nHeads and then heads\nHeads and then tails\nTails and then heads\nTails and then tails\n\nIf we’re interested in a particular event–for example, the event of “obtaining at least one head from two flips”—then we just count the number of outcomes that produce that event. For example, let’s take the four outcomes above and see which of them lead to at least one head.\n\nHeads and then heads: 2 heads\nHeads and then tails: 1 head\nTails and then heads: 1 head\nTails and then tails: 0 heads\n\nThree out of four outcomes would produce the event of “at least one head”, so the probability is \\(\\frac{3}{4}\\) or 0.75.\nIf you’re viewing probability like this, it’s very important to be clear about what counts as a possible outcome. For example, when you’re playing the lottery, how many outcomes are there?\nIs it two? Either you pick the correct numbers or you don’t? So the probability of winning is \\(\\frac{1}{2}\\)? Of course not! There’s 45,057,474 possible outcomes. And 1 leads to you winning, with 45,057,473 leading to you not winning!\n\n\nThe frequency view of probability\nWhen you take a frequency view of probability you’re making a claim about how often, over some long period of time some event occurs. The frequency view is often the view that we take in science. The frequency view of probability is also the view of probability that we most often use in the context of sampling distribution.\nThink about the following statement: There’s 95% probability that the sample mean will be less than 2 standard errors from the population mean.\nWhat this statement means is that if you draw lots of samples from the same population then 95% of the time the sample mean will be within 2 standard errors of the population mean.\nOr, for example, consider assigning a probability to the claim “drug X lowers depression”, we can’t just think of each possible outcomes that could occur when people take Drug X and then count up how many lead to lower depression and how many do not, as we would do with the classical view. Because there’s no way to make an exhaustive list of every possible outcome! So instead what we need to do is to run an experiment where we give Drug X and see whether it lowers depression. And we can repeat this many many times. After this we count up the proportion of experiments in which depression was lowered, and this is then the probability that Drug X lowers depression.\n\n\nThe subjective view of probability (credences)\nFinal view we’ll discuss is the subjective view of probability where probabilities refer to credences. To understand what this means, consider the following statements:\n\nThe Australian cricket team will lose the upcoming test series against South Africa.\n\nThere is a sense in which you can assign a probability to this. But it isn’t the classical kind—we can’t just enumerate all the possible outcomes that lead to this event. Nor is it the frequency kind—we can’t repeat the 2022/2023 cricket tour over and over and see how often Australia lose.\nWhen we talk about probability in this context mean something like degree of belief, credence, or subjective probability. Probability in this context is the answer to the question “how sure are you that the Australian cricket team will lose the upcoming test series against South Africa?”\nThe viewing probabilities are credences is something that is common in our everyday thinking. For example, consider jurors that are required to make a decision about the guilt or innocence of a defendant. To make this decision jurors need to assign probabilities to the two propositions: 1) The defendant is guilty or 2) The defendant is innocent. And, in the case of criminal trials, the probability assigned to 1 must be greater than 2 by some threshold amount. 1\nThe classical view of probability and the frequency view of probability are, in many respects, similar to each other, at least when compared to the subjective view. But you might ask, why do these differences matter?\nOne reason for discussing these differences is that the frequency view is the view you’ll most commonly encounter within what’s known as Frequentist statistics.This is the kind of statistics you’ll be learning in your undergraduate courses2. The frequency view is, however, not that common in our every day thinking. As the juror example is meant to demonstrate, the credence/subjective view is more common. As a result this can lead to some confusions. Specifically, people get confused and think that the results of statistical tests tell people what they should “believe”—that is, what subjective probabilities they should assign to hypotheses. But they don’t. At least not by themselves. We can use them to help us form beliefs about hypotheses, but only with the help of some extra information that comes, for example, from scientific theories and so on."
  },
  {
    "objectID": "lectures/week11/handout/index.html#calculating-with-probability",
    "href": "lectures/week11/handout/index.html#calculating-with-probability",
    "title": "Lecture 11: Introduction to probability",
    "section": "Calculating with probability",
    "text": "Calculating with probability\nThe different views of probability have got to do with what the numbers mean, but once we have the numbers there’s no real disagreements about how we do calculations with those numbers3.\n\nSome properties of probabilities\nThere are some rule that probabilities need to obey. When we attach numbers to probabilities those numbers must range from 0 to 1. We assign a probability of 0 to an event if that event is impossible (that is, it will never occur). And we assign 1 to an event if it’s guaranteed (that is, it will always occur)\nThese two simple rules can help us to check our calculations with probabilities. If we get a value more than 1 or a value less than 0, then something has gone wrong!\n\n\n\n\n\n\nA first note about notation.\n\n\n\nThere’s lots of notation that goes along with probability theory. We’ll learn more about this notion as we go long. But for now, we’ll just start off simple.\nIt’s common to use P or Pr to refer to probability. To refer to the probability of some event the P is followed by brackets containing a symbol. For example, if you wanted to refer to the probability of getting Heads on a coin flip then you might right P(Heads). Or if you wanted to refer to the probability that somebody is sick then you might right P(Sick).\n\n\n\n\nThe addition law\nThe addition law states that whenever two events are mutually exclusive:\n\nThe probability that at least one them occurs is the sum of the their individual probabilities\n\nIf we flip a coin, one of two things can happen. It can land Heads, or it can land Tails. It can’t land heads and tails (this is what is meant by mutually exclusive), and one of those things must happen (it’s a list of all possible events).\nWhat’s the probability that at least one of the those events happens? Since one of those events must happen the probability must be 1. But we can also work it out from the individual probabilities using the addition law.\n\n\\(\\frac{1}{2}\\) possible outcomes produce Heads—P(Heads) = 0.50\n\\(\\frac{1}{2}\\) possible outcomes produce Tails—P(Tails) = 0.50\n\nThe probabilities of at least one of Heads or Tails occurring is 0.5 + 0.5 = 1\n\n\n\n\n\n\nAnother note about notation\n\n\n\nAnother set of symbols that you’ll see when dealing with probability are \\(\\cup\\) (union) and \\(\\cap\\) (intersection).\nIf we have two events \\(A\\) and \\(B\\) that occur with \\(P(A)\\) and \\(P(B)\\) then the probability or either A or B occurring is \\(P(A \\cup B)\\).\nThe addition law tells us that if A and B are mutually exclusive (they can’t both occur at the same time) then \\(P(A \\cup B) = P(A) + P(B)\\).\nWhile we use \\(P(A \\cup B)\\) to denote A or B occurring we use \\(\\cap\\) to denote A and B occurring. That is, the probability of A and B occurring is denoted as \\(P(A \\cap B)\\). If A and B are mutually exclusive then \\(P(A \\cap B) = 0\\).\n\n\nYou can explore mutually exclusive events in Explorable 1.\n\n\n\n\n\n\n\nExplorable 1 (Mutually exclusive events)  \n\n\nviewof plot = html`\n&lt;svg class=\"image\" xmlns=\"http://www.w3.org/2000/svg\" width=\"${\n  (44 * 10) + 44\n  }\" height=\"${\n  40 * Math.ceil((44 + 44 + 44) / 10) \n}\"&gt;\n${[\n  full_grid.slice(0, red).map((pt) =&gt; circle(pt[0], pt[1], \"red\", \"red\")),\n  full_grid\n    .slice(red, red + blue)\n    .map((pt) =&gt; circle(pt[0], pt[1], \"blue\", \"blue\")),\n  full_grid\n    .slice(red + blue, red + blue + green)\n    .map((pt) =&gt; circle(pt[0], pt[1], \"green\", \"green\"))\n].join(\"\")}\n&lt;/svg&gt;`\n\n\n\n\n\n\n\nviewof red = Inputs.range([0, 30], {\n  label: \"Number of red cirlces\",\n  step: 1,\n  value: 10\n})\n\nviewof blue = Inputs.range([0, 30], \n  {label: \"Number of blue circles\",\n  step: 1, \n  value: 10})\n\nviewof green = Inputs.range([0, 30], {\n  label: \"Number of green circles\",\n  step: 1,\n  value: 10\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof t3 = md`The display shows ${red} red circles, ${blue} circles, and ${green} circles.\nThis means that there's a total of ${red + blue + green} circles. If we were to \nselect one circle at random then we can work out the probability that it'll\nbe green, blue or red.\n\n- The probability of selecting a red circle is ${tex`\\frac{${red}}{${\n  red + green + blue\n}}`} or ${round3(red / (red + green + blue))}\n\n- The probability of selecting a blue circle is ${tex`\\frac{${blue}}{${\n  red + green + blue\n}}`} or ${round3(blue / (red + green + blue))}\n\n- The probability of selecting a green circle is ${tex`\\frac{${green}}{${\n  red + green + blue\n}}`} or ${round3(green / (red + green + blue))}\n\nSelecting **blue** and selecting **red** are **mutually exclusive**. \nThis means that if you select **only one circle**, that circle can't be both \nblue **and** red. But we could ask about the probability of selecting, \nfor example, a circle that is blue **or** red.\n\nTo work out this probability we apply the **addition rule**.\n\nMathematically, we can say:\n${tex`P (A \\cup B) = P(A) + P(B)`}\n\nLet's put some numbers to it:\\\n`\n\n\n\n\n\n\n\nviewof colors = Inputs.checkbox(\n  [\"red\", \"blue\", \"green\"],\n  { value: [\"red\", \"blue\"], label: \"Colours\"}, { value: [\"red\", \"blue\"]}\n)\n\n\n\n\n\n\n\nfunction give_selection(x) {\n\n  if(_.isEmpty(x)) {\n    return \"**None**\"\n  }\n\n  return colors.map((x, i) =&gt; `${i &gt;= 1 ? \" and \" : \"\"}**${x}**`)\n\n}\n\n\n\n\n\n\n\nmd`Select two colours using the selectors above. You've selected: ${give_selection(colors)}.\n\n${\n  _.isEmpty(colors)\n    ? \"\"\n    : md`The probability of selecting ${colors.map(\n        (x, i) =&gt; `${x}${i + 1 &lt; colors.length ? \" **or** \" : \" is:\"}`\n      )}`\n}\n\n${\n  _.isEmpty(colors)\n    ? \"Use the checkboxes above to select the colours.\"\n    : md`${tex`P(${colors\n        .map((x) =&gt; `\\\\mathrm{${x}}`)\n        .join(\"\\\\cup{}\")})`}  = ${colors\n        .map((x) =&gt; round3(getvalue(x) / (red + blue + green)))\n        .join(\" + \")} = ${round3(\n        _.sum(colors.map((x) =&gt; getvalue(x))) / (red + blue + green)\n      )}`\n} \n\n\n\n${\n  _.isEmpty(colors)\n    ? \"\"\n    : md`\nWe can work this out just by counting:\n\n${colors.map((x) =&gt; `${round3(getvalue(x))} (number of ${x} circles)`).join(\" + \")} = ${_.sum(\n        colors.map((x) =&gt; getvalue(x))\n      )}\n\n${_.sum(colors.map((x) =&gt; getvalue(x)))} ÷ ${\n        red + blue + green\n      } (total number of circles) = ${round3(\n        _.sum(colors.map((x) =&gt; getvalue(x))) / (red + blue + green)\n      )}\n`\n}\n\n`"
  },
  {
    "objectID": "lectures/week11/handout/index.html#mutually-and-non-mutually-exclusive-events",
    "href": "lectures/week11/handout/index.html#mutually-and-non-mutually-exclusive-events",
    "title": "Lecture 11: Introduction to probability",
    "section": "Mutually and non mutually exclusive events",
    "text": "Mutually and non mutually exclusive events\nWhen we flip a coin the two outcomes are mutually exclusive. That is, they can’t both happen at the same time. But not everything is like this. Consider drawing a card from a deck of cards:\n\nWhat is the probability of pulling out a Spade ♠ or a Club ♣?\nWhat is the probability of pulling out a Spade ♠ or an Ace 🃁?\n\nIn situation (1) the events are mutually exclusive (or disjoint). A card can’t be a Spade and a Club. It will either be a Spade, a Club, or something else. In this case, the addition law applies.\nIn situation (2) the events are not mutually exclusive. A card can be both a Spade and an Ace. Because a card can be both a Spade and an Ace we have to make sure that we don’t double count these cards. So we just modify the addition law so that we have Equation 1 below:\n\\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\tag{1}\\]\nWe can put numbers to this for the card example:\nThere are 52 cards in a deck of cards. Of these, 13 are s. So the probability of selecting a is \\(P(\\mathrm{Spade}) = \\frac{13}{52}\\). Exactly 4 of the 52 will be s, so the probability of selecting an is \\(P(\\mathrm{Ace}) = \\frac{4}{52}\\). And finally, exactly 1 card is both an and a . So the probability of selecting a card that is both is \\(P(\\mathrm{Ace} \\cap \\mathrm{Spade}) = \\frac{1}{52}\\). With all these we can now work out the probability of selecting a card that is an Ace or a Spade.\n\\(P(\\mathrm{Ace} \\cup \\mathrm{Spade}) = P(\\mathrm{Ace}) + P(\\mathrm{Spade}) - P(\\mathrm{Ace} \\cap \\mathrm{Spade})\\)\n\\(P(\\mathrm{Ace} \\cup \\mathrm{Spade}) = 0.08 + 0.25 - 0.02\\)\nYou can explore non-mutually exclusive events in Explorable 2.\n\n\n\n\n\n\n\nExplorable 2 (Non-mutually exclusive events)  \n\nIn Explorable 1 we had circles of three different colours (red, blue, and green). In this example, we’ll still have coloured circles, but some of the circles will also have a white dot. We’ll only have two colours (blue and red), but some blue circles will have white dots and others will not, and some red circles will have white dots and others will not.\nFirst we can set how many of each colour we have.\n\nviewof blue2_ = Inputs.bind(\n  Inputs.range([1, 30], { label: \"Number of blue\", step: 1, value: 15 }),\n  viewof blue2\n)\n\n\nviewof red2_ = Inputs.bind(\n  Inputs.range([1, 30], { label: \"Number of red\", step: 1, value: 15 }),\n  viewof red2\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThen we can set how many of each color will have white dots.\n\nviewof blue2_with_ = Inputs.bind(\n  Inputs.range([1, blue2], { label: \"Number of blue with dots\", step: 1 }),\n  viewof blue2_with\n)\nviewof red2_with_ = Inputs.bind(\n  Inputs.range([1, red2], {\n    label: \"Number of red with dots\",\n    step: 1\n  }),\n  viewof red2_with\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmd`\nThis display has ${red2} red circles and ${blue2} blue circles.\nOut of the ${red2} red circles, ${red2_with} also have a white dot.\nOut of the ${blue2} blue circles, ${blue2_with} also have a white dot.\n`\n\n\n\n\n\n\n\nviewof plot2 = html`\n&lt;svg class=\"image\" xmlns=\"http://www.w3.org/2000/svg\" width=\"${\n  (44 * 10) + 44\n  }\" height=\"${\n  25 * Math.ceil((44 + 44 + 44) / 10) \n}\"&gt;\n${[\n  full_grid\n    .slice(0, red2_with)\n    .map((pt) =&gt; circle(pt[0], pt[1], colors2.red.w[0], colors2.red.w[1])),\n  full_grid\n    .slice(red2_with, red2)\n    .map((pt) =&gt; circle(pt[0], pt[1], colors2.red.n[0], colors2.red.n[1])),\n  full_grid\n    .slice(red2, red2 + blue2_with)\n    .map((pt) =&gt; circle(pt[0], pt[1], colors2.blue.w[0], colors2.blue.w[1])),\n  full_grid\n    .slice(red2 + blue2_with, red2 + blue2)\n    .map((pt) =&gt; circle(pt[0], pt[1], colors2.blue.n[0], colors2.blue.n[1]))\n].join(\"\")}\n&lt;/svg&gt;`\n\n\n\n\n\n\nWe can now as a question like: What is the probability of selecting a circle that is Red or has a white dot.\n\ntexmd`\nFirst we need to know P(Red). To do this, we just count\nup the number of circles that are red. \n${red2} of the ${red2 + blue2} cirlces are red, \nso $P(\\mathrm{Red})$ = ${frac(red2/(red2 + blue2),red2 + blue2)}.\n\nNext we need to know P(Dot). To do this, we just count\nup the number of circles that have white dot.\n${red2_with + blue2_with} of the ${red2 + blue2} \ncircles have white dots, so $P(\\mathrm{Dot})$ = \n${frac((red2_with + blue2_with) / (red2 + blue2), red2 + blue2)}.\n\nWe can't just add these two numbers, because we'll double count some \nof the circles.\n\nClick the toggle below to see which circles get counted twice.\n`\n\n\n\n\n\n\n\nviewof show_double_count = Inputs.toggle({ label: \"Show double counted\", value: false })\n\n\n\n\n\n\n\ntexmd`Because some the red circles with white dots get counted twice, we need to \nsubtract this amount.\nFirst we work out $P(\\mathrm{Red})$ + $P(\\mathrm{Dot})$.Using the numbers above\nthis gives us ${frac(((red2 + red2_with + blue2_with))/(blue2 + red2), blue2 + red2)}.\nThen we subtract ${frac(red2_with / (blue2 + red2), blue2 + red2)}. This gives us\n$P(\\mathrm{Red} \\cup \\mathrm{Dot})$ = ${frac((red2 + blue2_with)/(blue2 + red2), blue2 + red2)}.\n`\n\n\n\n\n\n\nBut if all that maths is too difficult, that we can just work out the probability by counting! All we need to do is to count up all the circles that are either Red or have a dot. And we just divide that by the total number of circles. If you click the toggle below, then you can see which circles you need to count.\n\nviewof show_count = Inputs.toggle({ label: md`Show Red ∪ Dot`, value: false })\n\n\n\n\n\n\n\ncolors2 = {\n  let c = {\n    \"red\" : { \"w\" : [ \"red\", \"white\" ], \"n\" : [ \"red\", \"red\" ] },\n    \"blue\" : { \"w\" : [ \"blue\", \"white\" ], \"n\" : [ \"blue\", \"blue\" ] }\n  }\n\n  if(show_double_count) {\n    c.red[\"n\"] = [\"white\",\"white\"]\n    c.blue[\"w\"] = [\"white\", \"white\"]\n    c.blue[\"n\"] = [\"white\", \"white\"]\n  }\n  if(show_count) {\n    c.red[\"n\"] = [\"red\", \"red\"]\n    c.red[\"w\"] = [\"red\",\"white\"]\n    c.blue[\"w\"] = [\"blue\", \"white\"]\n    c.blue[\"n\"] = [\"grey\", \"grey\"]\n  }\n\n\n  return c\n\n}"
  },
  {
    "objectID": "lectures/week11/handout/index.html#two-or-more-events",
    "href": "lectures/week11/handout/index.html#two-or-more-events",
    "title": "Lecture 11: Introduction to probability",
    "section": "Two or more events",
    "text": "Two or more events\nIn the previous example we were only making one selection, but the things we were selecting from had two features: The circles had a colour and they could have a dot (or not). But sometimes we want to deal with two or more selections. A simple example of this is when we flip coins multiple times. For example, if we flip a coin three times, we might want to work out the probability of getting, for example, Heads, then Tails, and then Heads again.\nWhen we have a problem like this, we can’t just add up the probabilities. If we did, then we’d get \\(\\frac{1}{2}\\) + \\(\\frac{1}{2}\\) + \\(\\frac{1}{2}\\) = \\(\\frac{3}{2}\\). But, remember, probabilities have to be between 0 and 1, so obviously this answer is wrong.\nBefore we get to how to work it out mathematically, we’ll just work it out by counting. In Figure 1, we can see the possible sequences of events if we flip a coin a particular number of times. Set the slider to 3 to see what happens if we flip the coin 3 times. In Figure 1 the black circles mean getting Heads, and the white circles mean getting Tails. Now just count up how many sequences go Black, White, Black. And now count up how many sequences there are in total. And that’s the probability of getting Heads, Tails, Heads in three coin flips.\n\nviewof sequences = {\n  const div = document.createElement(\"div\");\n  div.value = new vega.View(parsedSpec).initialize(div).run();\n  return div;\n}\n\n\n\n\n\nFigure 1: Possible sequences after  coin flips\n\n\n\n\nviewof coins = htl.html`&lt;input style=\"width:300px\" type=\"range\" id=\"coins\" min=\"1\" max=\"7\" value=\"1\" class=\"form-range\"&gt;`\n\ncoins_label = htl.html`&lt;label for=\"coins\" class= \"form-label\" width=\"100%\"&gt;Number of coin flips: ${\n  coins - 1\n}&lt;/label&gt;`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you don’t want to count, and you just want to work it out mathematically, then you can do this by just multiplying together the probability for each of the event. Doing this gives us the following: \\(\\frac{1}{2}\\) × \\(\\frac{1}{2}\\) × \\(\\frac{1}{2}\\) = \\(\\frac{1}{8}\\)."
  },
  {
    "objectID": "lectures/week11/handout/index.html#independence-and-non-independence",
    "href": "lectures/week11/handout/index.html#independence-and-non-independence",
    "title": "Lecture 11: Introduction to probability",
    "section": "Independence and non-independence",
    "text": "Independence and non-independence\nThe coin flipping example, the three flips are independent. Independent means that, for example, the probability of getting Heads/Tails on the second flip doesn’t change depending on what you get on the first flip. This means that if I were to ask you to work out the probability of getting heads on the second flip you answer wouldn’t change if I told you what I got on the first flip.\nBut it is often the case the finding out a bit of information does change your probability calculation. To see how, let’s change things up a bit. Let’s say that we’re going to roll a dice. But instead of just rolling a dice, we’ll first select one of two dice. The set up is as follows: First, pick either a 20-sided dice (D-20) or a 6-sided dice (D-6). Second, roll the dice.\nUnlike the coin flip example, where knowing what happened on the first flip won’t change how you calculate the probability for the second flip, knowing whether I picked a D-6 or D-20 will change your calculations. For example, the probability of rolling a 20 will depend on which dice I pick in the first step. If I told you that I picked a D-6, then the probability that I rolled a 20 would be 0, because it would be impossible! If told you that I picked a D-20, then the probability that I rolled a 20 would be \\(\\frac{1}{20}\\).\nIn a situation like this, we say that the probability of rolling a 20 is conditional on the selection in the first step.\n\n\n\n\n\n\nConditional probability notation.\n\n\n\nWe deal with probability of one event that is conditional on other event, then we call this a conditional probability. There is a special notation for this. Using the dice roll example above, we’d use \\(P(\\mathrm{Roll\\ 20})\\) to refer to the probability of rolling a 20. And we’d use \\(P(\\mathrm{Pick\\ 6-sided})\\) to refer to the probability of selecting the 6-sided dice.\nIf wanted to refer to the probability of rolling a 20 given that (conditional on) we selected a 6-sided dice then we’d use the following notation:\n\\[P(\\mathrm{Roll\\ 20}|\\mathrm{Pick\\ 6-sided})\\]\nThe \\(|\\) is read as “conditional on” or “given that”."
  },
  {
    "objectID": "lectures/week11/handout/index.html#working-with-conditional-probabilities",
    "href": "lectures/week11/handout/index.html#working-with-conditional-probabilities",
    "title": "Lecture 11: Introduction to probability",
    "section": "Working with conditional probabilities",
    "text": "Working with conditional probabilities\nWe often encounter conditional probabilities in every day life. However, reasoning about conditional probabilities can be difficult and as a result people make a lot of mistakes when dealing with them.\nThe most common mistake that you’ll encounter is the confusion being P(A|B) and P(B|A). Or, as in the dice example, P(Roll 20 | Pick 20-sided) and P(Pick 20-sided | Roll 20). We know from above that P(Roll 20 | Pick 20-sided) is \\(\\frac{1}{20}\\). But what is the probability of P(Pick 20-sided | Roll 20). In our scenario, where we could either of rolled a D-20 or a D-6, then given the fact that we rolled a 20, it must have been the D-20 that we rolled. So P(Pick 20-sided | Roll 20) must be 1!\nThe other typical confusion is confusing the condition probabilities P(A|B) and P(B|A) for the unconditional probabilities P(A) and P(B). To see this problem with this, consider the following statement:\n\nWhat is the probability that a randomly selected person lives in London?\n\nThere’s about 9 Billion people in the world and only 9 Million of them live in London. So the probability is pretty low at only 0.1%. But now consider this statement:\n\nWhat is the probability that a randomly selected person lives in London given that the person’s names is King Charles III?\n\nThe answer to this is obviously 1, because King Charles III lives in London. So, conditional on the randomly selected person being King Charles III then that person must live in London.\nIn the first statement we’re working out P(Lives in London) and in the second we’re working P(Lives in London | Is King Charles). We could also work out P(Is King Charles | Lives in London). There’s 9 Million people in London but only 1 of them is King Charles. So the probably for this is very small at less than 0.0001%!\nThere’s a mathematical formula that relates P(A|B) to P(B|A). This formula is known as Bayes theorem and it’s incredibly useful for helping us work out the probability of something once we’ve been given a new bit of information."
  },
  {
    "objectID": "lectures/week11/handout/index.html#bayes-theorem",
    "href": "lectures/week11/handout/index.html#bayes-theorem",
    "title": "Lecture 11: Introduction to probability",
    "section": "Bayes theorem",
    "text": "Bayes theorem\nThe classic example that is usually used to introduce Bayes theorem is a problem like the following:\nThere is a test for an illness. The test has the following properties.\n\nAbout 80% of people that actually have the illness will test positive.\nOnly about ~5% of people that don’t have the illness will test positive\n\nSomebody, who may be sick or healthy, takes the test and tests positive. Is that person actually sick?\nBefore we get to Bayes theorem remember that we can work out probabilities just by counting. So we’ll first try to get an answer to our question that way before we do the maths. We can explore this question in Explorable 3 below.\n\n\n\n\n\n\n\nExplorable 3 (Is the person sick or healthy?)  \n\n\nviewof figure = html`\n&lt;svg class=\"image\" xmlns=\"http://www.w3.org/2000/svg\" width=\"420\" height=\"${320}\"&gt;\n${[\n  functions.full_grid\n    .slice(0, _.sum(data.g.slice(0, 1)))\n    .map((pt) =&gt;\n      functions.circle(pt[0], pt[1], data.colours[0][0], data.colours[0][1])\n    ),\n  functions.full_grid\n    .slice(_.sum(data.g.slice(0, 1)), _.sum(data.g.slice(0, 2)))\n    .map((pt) =&gt;\n      functions.circle(pt[0], pt[1], data.colours[1][0], data.colours[1][1])\n    ),\n  functions.full_grid\n    .slice(_.sum(data.g.slice(0, 2)), _.sum(data.g.slice(0, 3)))\n    .map((pt) =&gt;\n      functions.circle(pt[0], pt[1], data.colours[2][0], data.colours[2][1])\n    ),\n  functions.full_grid\n    .slice(_.sum(data.g.slice(0, 3)), _.sum(data.g.slice(0, 4)))\n    .map((pt) =&gt;\n      functions.circle(pt[0], pt[1], data.colours[3][0], data.colours[3][1])\n    )\n].join(\"\")}\n&lt;/svg&gt;`\n\n\n\n\n\n\nIn this example, the Red circle represent people that are sick and the Green circles represent people that are healthy.\n\nmd`\nIf a circle has a black dot, then that means the person tested positive for the\ndisease. I said that 80% of people that actually have the disease will\ntest positive. Select the option below to *only show the sick people*.\nNow just count up how many of the Red circles have Black dots. \nThere are ${data.g[0] + data.g[1]} sick people and ${functions.frac(\n  data.conds.dot_red,\n  data.g[0] + data.g[1]\n)} or ${functions.round3(\n  data.conds.dot_red * 100\n)}% of the\nsick people test **positive**`\n\n\n\n\n\n\n\nmd`\nNext we want to count up the healthy people that test positive.\nSelect the option below to *only show the healthy people*.\nNow just count up how many of the Green circles have Black Dots.\nThere are ${data.g[2] + data.g[3]} healthly people, and only\n ${functions.frac(\n  data.conds.dot_green,\n  data.g[2] + data.g[3]\n)} or ${functions.round3(\n  data.conds.dot_green * 100\n)}% of healthy people test **postive**.\n `\n\n\n\n\n\n\nThis matches the information I gave you about how the test works. But what we really want to know is, given somebody tests positive, what’s the probability that they’re sick?. To work this out, we need to focus our attention only on the people that test positive. Select the option below to only show the positive tests.\n\nmd`\n\nThere are ${data.g[1] + data.g[2]} people that test positive. Of these \n${functions.frac(\n  data.g[1] / (data.g[1] + data.g[2]),\n  data.g[1] + data.g[2]\n)} or ${functions.round3(\n  (data.g[1] / (data.g[1] + data.g[2])) * 100\n)}% are sick and ${functions.frac(\n  data.g[2] / (data.g[1] + data.g[2]),\n  data.g[1] + data.g[2]\n)} or ${functions.round3((data.g[2] / (data.g[1] + data.g[2])) * 100)}%\nare healthy.\nThis means that a person that tests **positive** is more likely to be ${\n  data.g[1] / (data.g[1] + data.g[2]) &gt; data.g[2] / (data.g[1] + data.g[2])\n    ? \"**sick**\"\n    : \"**healthy!**\"\n}`\n\n\n\n\n\n\n\nviewof positive_tests = Inputs.radio(\n  new Map([\n    [\"All people\", \"all\"],\n    [\"Only sick people\", \"sick\"],\n    [\"Only healthy people\", \"health\"],\n    [\"Only positive tests\", \"pos\"]\n  ]),\n  { value: \"all\", label: \"Show\" }\n)\n\n\n\n\n\n\n\n{\n  if(data.conds.red &lt; 0.5) {\n    return md` This might seem like an odd conclusion! But it's right \n    there in the circles and dots that you counted. To make sense of\n    it, I'll give you another key bit of information`\n  }\n  return md``\n\n}\n\n\n\n\n\n\n\nmd`The key bit of information I didn't tell you is that \nin this example, the disease is **${\n  data.conds.red &gt; 0.5 ? \"COMMON\" : \"RARE\"\n}** with an incidence of ${data.conds.red * 100}%.\n\nBut we can change this. Use the selection below to change whether the \ndisease is **COMMON** or **RARE** and then read through the\nexample again.\n`\n\n\n\n\n\n\n\nviewof incidence = Inputs.radio(\n  new Map([\n    [\"Common\", \"common\"],\n    [\"Rare\", \"rare\"]\n  ]),\n  { value: \"rare\", label: \"Incidence\" }\n)\n\n\n\n\n\n\nNote that when you change the disease from RARE to COMMON the facts about the test don’t change. It’s still the case that 80% of sick people will get a positive test and about 5% of healthy people will get a positive test. But what we can can conclude from a positive test (whether a person receiving a positive test is sick or healthy) changes dramatically between the two examples. In one we conclude that’s it’s more likely that the person is healthy and in the other we conclude that it’s more likely that the person is sick.\n\nfunctions = {\n  return {\n    circle: (x, y, fill, stroke) =&gt; {\n      return `&lt;circle cx=\"${x}\" cy=\"${y}\" r=\"14\" stroke=\"none\" fill=\"${fill}\" stroke-width=\"0\"&gt;&lt;/circle&gt;\n&lt;circle cx=\"${x}\" cy=\"${y}\" r=\"5\" stroke=\"${fill}\" fill=\"${stroke}\" stroke-width=\"0\"&gt;&lt;/circle&gt;`;\n    },\n    full_grid: _.flatten(\n      _.range(10).map((y) =&gt;\n        _.range(10)\n          .map((x) =&gt; 30 * (x + 1))\n          .map((x) =&gt; [x, 30 * (y + 1)])\n      )\n    ),\n    round3: (x) =&gt; Math.round(x * 1000) / 1000,\n    frac: (decimal, den) =&gt; {\n      const n = Math.round(decimal * den);\n      return md`${tex`\\frac{${n}}{${den}}`}`;\n    }\n  };\n}\n\n\n\n\n\n\n\ndata = {\n  let proportions = incidence === \"rare\" ? [5, 95] : [80, 20];\n  let percentages = incidence === \"rare\" ? [4 / 5, 1 / 20] : [8 / 10, 6 / 90];\n  let g = [\n    Math.round(proportions[0] * (1 - percentages[0])),\n    Math.round(proportions[0] * percentages[0]),\n    Math.round(proportions[1] * percentages[1]),\n    Math.round(proportions[1] * (1 - percentages[1]))\n  ];\n\n  let all = [\n    [\"red\", \"red\"],\n    [\"red\", \"black\"],\n    [\"green\", \"black\"],\n    [\"green\", \"green\"]\n  ];\n\n  let pos = [\n    [\"white\", \"white\"],\n    [\"red\", \"black\"],\n    [\"green\", \"black\"],\n    [\"white\", \"white\"]\n  ];\n\n  let sick = [\n    [\"red\", \"red\"],\n    [\"red\", \"black\"],\n    [\"white\", \"white\"],\n    [\"white\", \"white\"]\n  ]\n\n  let health = [\n    [\"white\", \"white\"],\n    [\"white\", \"white\"],\n    [\"green\", \"black\"],\n    [\"green\", \"green\"]\n  ];\n\n\n  let colors = {\"all\" : all, \"sick\": sick, \"pos\": pos, \"health\": health}\n\n  let conds = {\n    red_dot: g[1] / (g[1] + g[2]),\n    dot_red: g[1] / (g[1] + g[0]),\n    red: (g[0] + g[1]) / _.sum(g),\n    dot: (g[1] + g[2]) / _.sum(g),\n    green: 1 - (g[0] + g[1]) / _.sum(g),\n    dot_green: g[2] / (g[3] + g[2])\n  };\n\n  return {\n    proportions: proportions,\n    percentages: percentages,\n    g: g,\n    conds: conds,\n    colours: colors[positive_tests]\n  };\n}\n\n\n\n\n\n\n\n\n\nAlthough worked out the answer to our question just by counting the dots, we can also use Bayes theorem to do it. Bayes theorem is given as the Equation 2, below:\n\\[P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)} \\tag{2}\\]\nInstead of using As and Bs, we’ll make it a little more readable by using ✅ to indicate that the person tested positive and 🤮 to indicate that the person is actually sick. It now looks like Equation 3.\n\\[P(🤮\\ |\\ ✅) = \\frac{P(✅\\ |\\ 🤮) \\times P(🤮)}{P(✅)} \\tag{3}\\]\n\\(P(✅\\ |\\ 🤮)\\) is the probability that a sick person tests positive. \\(P(🤮)\\) is the incidence of the illness (whether is it common or rare). And \\(P(✅)\\) is the probability of testing positive irrespective of whether you’re sick or healthy. I didn’t tell you this number, but you can work it out using the equation in Equation 4.\n\\[P(✅) = P(✅\\ |\\ 🤮) × P(🤮) + P(✅\\ |\\ 😁) × P(😁) \\tag{4}\\]\nWhere 😁 means that the person is healthy.\nNow we can put numbers to it. In the case where the disease was rare we had the following values:\n\nP(😁) = 95/100 (95% of people are healthy)\nP(🤮) = 5/100 (5% of people are sick)\nP(✅ | 😁) = 5/95 (5.26% of healthy people test positive)\nP(✅ | 🤮) = 4/5 (80% of sick people test positive)\n\nNow we can put those numbers together to work out the value we wanted to know— that is, P(🤮 | ✅).\nYou can work it out yourself, and if you do then you’ll get 44.44% just as we did above.\nWe can also do it for the case where the disease is common. Then we can the following values.\n\nP(😁) = 20/100 (20% of people are healthy)\nP(🤮) = 80/100 (80% of people are sick)\nP(✅ | 😁) = 1/20 (5% of healthy people test positive)\nP(✅ | 🤮) = 64/80 (80% of sick people test positive)\n\nNow we can put those numbers together to work out the value we wanted to know— that is, P(🤮 | ✅). If you do then you’ll get 98.46% just as we did above.\nReasoning about conditional probabilities can be difficult because people often forget about the P(🤮) part. But if we ignore it can be any to make mistakes, as we saw in the example above. But these kinds of mistakes are common and they can have dangerous consequences. I could a number of examples to show this, but I’ll just pick three.\n\nErrors in reasoning about conditional probabilities\nYou might have the following statistic in the media/online.\n\n50% of people that die from Covid have been vaccinated\n\nI’ve seen this figure on social media along with the claim that it shows that the Covid vaccine doesn’t work. Let’s assume that the statistic is accurate. Does this mean that the vaccine doesn’t work?\nFirst, we have to think about what this figure refers to and whether it is what we want to know. If it’s not, then we have to work out what it is that we do what to know.\nFirst, what is this figure? To get this number we’re looking only at the people that have died. It’s a probability conditional on the person dying. That means this figure refers to P(Vaccinated | Death). But if we want to know if the vaccine works, then what we actually want to know is, if the person is vaccinated, then what is the probability that they will die. That is, we want to know the probability conditional on them being vaccinated, or P(Death | Vaccinated).\nSo straight away, we know that this 50% probability is not actually the number we want to know. But more importantly, this 50% is perfectly consistent with an effective vaccine if the vaccination rate is high. If the vaccine is effective, but the vaccination rate is low, this this number would be lower. So the 50% doesn’t give us the information we need to make the judgement about whether the vaccine is effective. We’d also need to know the vaccinate rate and the probability of dying if you contract Covid irrespective of your vaccine status.\nThe next example concerns a policy that has been proposed in the USA. Essentially the policy states that welfare recipients should be drug tests, and their benefits removed if the test comes back positive. But does a positive test mean that the person is actually a drug user? The answer to this question all depends on the rates of drug use among welfare recipients. It turns out that this is actually rather low. So let’s put some numbers to it.\nFirst, we’ll set the properties of the test. We’ll say it’s fairly accurate, so the probability of testing positive given drug use, or P(✅|💉), will be high.\n\nviewof p_d = Inputs.range([0, 1], {step: 0.01, value: 0.99, label: \"P(✅|💉)\"})\n\n\n\n\n\n\nAnd we’ll say that it’s not that common for somebody to test positive if they’re not a drug user. So we can set P(✅|😇) to a low number.\n\nviewof p_n = Inputs.range([0, 1], {step: 0.05, value: 0.05, label: \"P(✅|😇)\"})\n\n\n\n\n\n\nFinally, because drug use is not common amongst welfare receipts we’ll set P(💉) to a low number.\n\nviewof d = Inputs.range([0, 1], {step: 0.01, value: 0.01, label: \"P(💉)\"})\n\n\n\n\n\n\nWith all this information we can work out the number we want to know. That is, P(💉|✅). We’ll use Bayes theorem as follows:\n\\[P(💉|✅) = \\frac{P(✅|💉) × P(💉)}{P(✅)}\\]\nBut since I haven’t given you P(✅), we’ll just work it out as follows:\n\\[P(✅|💉) × P(💉) + P(✅|😇) × (1 - P(💉))\\]\nOr with numbers:\n\ntexmd`\nP(✅) = ${p_d} × ${d} + ${p_n} × (1 - ${d}) = ${Math.round(p_d * d + p_n * (1 - d) * 100)/100}\n`\n\n\n\n\n\n\nAnd now the full formula:\n\ntexmd`P(💉|✅)=(${p_d} × ${d}) ÷ ${Math.round(p_d * d + p_n * (1 - d) * 100)/100} = \n${Math.round((p_d * d) / (p_d * d + p_n * (1 - d)) * 100)/100}\n\nThis means that given our settings there's only a ${Math.round((p_d * d) / (p_d * d + p_n * (1 - d)) * 100)}%\nchance that somebody that tests postive for drugs is drug user. Obviously there's\na clear ethical/moral arguments to be made against removing the welfare benefits \nfrom somebody who tests positive for drugs. But if our settings are correct,\nthen there's also a clear **mathematical** argument to be made for it being \na bad idea.\n`\n\n\n\n\n\n\nThe final argument concerns a (now retracted) paper that was published a few years ago. According to the study, there was no racial bias in police shooting. What was the evidence for this claim? The researchers looked at a large sample of police shooting and showed that in this sample of police shootings a higher proportion of victims were White than Black.\nThis was picked by by the conservative media (e.g., Fox news) to show that organisations like BLM were fighting against a problem that didn’t exist. But is the reasoning correct, and do the data actually show what the authors claim?\nAs you’ve probably guessed the results certainly don’t support the claim. But why not?\nFirst, let’s look at the data the authors present. I’ll simply it somewhat, so that it’s easier for use to do the calculations, but the general gist is the same. In Figure 2 we can see the data they present. These are all the victors of the police shootings. The probability that a person is White (P(White|Shot)) is \\(\\frac{20}{30}\\) or 66.67% and the probability that a person is Black (P(Black|Shot)) is \\(\\frac{10}{30}\\) or 33.33%. These are the two probabilities that the authors looked at. You’ll notice that they’re actually two conditional probabilities. But are they the correct ones?\n\n\n\n\n\nFigure 2: Sample of police shooting. Pink circles correspond to White victims and Black circles to Black victims\n\n\n\n\nLet’s add some additional data. In Figure 3 we can see all the people that have encounters with the police, but not get shot. Black people are a racial minority in the USA, so we would expect police to encounter police less often on their day-to-day rounds.\n\n\n\n\n\nFigure 3: Sample of all people encountered by the police without getting shot. Pink circles correspond to White people and Black circles to Black people\n\n\n\n\nNow let’s put it all together. In Figure 4 we’ve just combined the data from Figure 2 and Figure 3. This shows all the people that come into contact with police including those people that fall victim to a police shoot (red dot) and those that do not (no dot)\n\n\n\n\n\nFigure 4: Sample of all people encountered by the police. Pink circles correspond to White people and Black circles to Black people. Red dots correspond to shooting victims.\n\n\n\n\nWith everything on a single plot we can now focus in on the numbers we actually want to know. We want to know, given a person is a particular race what is the probability of them being a victim of a police shooting. That is, we want to know P(Shot | White) and P(Shot | Black), not P(White | Shot) and P(Black | Shot) as reported in the study.\nTo this, we’ll just focus on people who are Black. We can see this in Figure 5. This allows us to see P(Shot|Black), which gives \\(\\frac{10}{20}\\) or 50%.\n\n\n\n\n\nFigure 5: Sample of all Black people encountered by the police. Red dots correspond to shooting victims.\n\n\n\n\nNext, we’ll just focus on the people who are White. We can see this in Figure 6. This allows use to see P(Shot|White), which gives \\(\\frac{20}{80}\\) or 25%.\n\n\n\n\n\nFigure 6: Sample of all White people encountered by the police. Red dots correspond to shooting victims.\n\n\n\n\nBecause P(Shot | Black) is higher than P(Shot | White) then this suggests that there is in fact racial bias in police shooting. However, to reach this conclusion I had to make an assumption that wasn’t reported in the paper. I had to assume that people come across more White people than Black people in their daily activities. This is reasonable given that Black people are a racial minority. But it might not actually be true. It might be the case that police encounter Black people more often. For example, the real data might look like Figure 7. Comparing the first example, which gave P(Shot|Black) = 0.5 and P(Shot|White) = 0.25 this new data gives P(Shot|Black) = 0.14 and P(Shot|White) = 0.67. So this is consistent with a racial bias against White people.\n\n\n\n\n\nFigure 7: Sample of all people encountered by the police. Pink circles correspond to White people and Black circles to Black people. Red dots correspond to shooting victims.\n\n\n\n\nThe point here isn’t to argue whether there is or isn’t a racial bias in police shooting. Rather the point is that the data presented in this study is consistent with both conclusions. And because the authors reasoning was faulty they didn’t collect the data they needed to actually answer the question they posed.\nWhat makes matter worse is that this study was used to delegitimise protest movements against what is a very real problem. It had very real negative consequences. This study has now been retracted, but not before it did a lot of damage.\nI hope this serves as a sobering message for just how important research methods (including probability theory) is in your training. You might one day be in the position to make policies for governments so I hope you don’t fall victim to faulty reasoning when you do!\n\n\nfunction circle(x, y, fill, stroke) {\n  return `&lt;circle cx=\"${x}\" cy=\"${y}\" r=\"20\" stroke=\"none\" fill=\"${fill}\" stroke-width=\"0\"&gt;&lt;/circle&gt;\n&lt;circle cx=\"${x}\" cy=\"${y}\" r=\"10\" stroke=\"${fill}\" fill=\"${stroke}\" stroke-width=\"0\"&gt;&lt;/circle&gt;`;\n}\n\n\n\n\n\n\n\nfull_grid = _.flatten(\n  _.range(10).map((y) =&gt;\n    _.range(10)\n      .map((x) =&gt; 44 * (x + 1))\n      .map((x) =&gt; [x, 44 * (y + 1)])\n  )\n)\n\n\n\n\n\n\n\nfunction round3(x) {\n  return Math.round(x * 1000) / 1000;\n}\n\n\n\n\n\n\n\nfunction getvalue(x) {\n  if (x === \"red\") {\n    return red;\n  }\n\n  if (x === \"blue\") {\n    return blue;\n  }\n\n  if (x === \"green\") {\n    return green;\n  }\n}\n\n\n\n\n\n\n\nviewof blue2 = Inputs.input(10)\n\n\n\n\n\n\n\nviewof red2 = Inputs.input(10)\n\n\n\n\n\n\n\nviewof blue2_with = Inputs.input(5)\n\n\n\n\n\n\n\nviewof red2_with = Inputs.input(5)\n\n\n\n\n\n\n\nfunction frac(decimal, den) {\n  const n = Math.round(decimal * den);\n\n  return md`${tex`\\frac{${n}}{${den}}`}`;\n}\n\n\n\n\n\n\n\nimport { texmd } from \"@kelleyvanevert/katex-within-markdown\"\n\n\n\n\n\n\n\nparsedSpec = {\n  return vega.parse(spec3);\n}\n\n\n\n\n\n\n\nspec3 = {\n  return {\n    $schema: \"https://vega.github.io/schema/vega/v5.0.json\",\n    padding: 0,\n    width: 500,\n    height: 100,\n    layout: {\n      padding: 0,\n      columns: 1\n    },\n    marks: [\n      {\n        type: \"group\",\n        encode: {\n          update: {\n            width: {\n              value: 1000\n            },\n            height: {\n              value: 130\n            }\n          }\n        },\n        data: [\n          {\n            name: \"tree\",\n            values: coin_data,\n            transform: [\n              {\n                type: \"stratify\",\n                key: \"id\",\n                parentKey: \"parent\"\n              },\n              {\n                type: \"tree\",\n                method: \"tidy\",\n                size: [500, 200],\n                as: [\"x\", \"y\", \"depth\", \"children\"]\n              }\n            ]\n          },\n          {\n            name: \"links\",\n            source: \"tree\",\n            transform: [\n              {\n                type: \"treelinks\",\n                key: \"id\"\n              },\n              {\n                type: \"linkpath\",\n                orient: \"horizontal\",\n                shape: \"line\"\n              }\n            ]\n          }\n        ],\n        scales: [\n          {\n            name: \"color\",\n            domain: [0, 1, 2, 3, 4, 5],\n            type: \"sequential\",\n            range: \"ramp\"\n          }\n        ],\n        marks: [\n          {\n            type: \"path\",\n            from: {\n              data: \"links\"\n            },\n            encode: {\n              update: {\n                path: {\n                  field: \"path\"\n                },\n                stroke: {\n                  value: \"black\"\n                }\n              }\n            }\n          },\n          {\n            type: \"symbol\",\n            from: {\n              data: \"tree\"\n            },\n            encode: {\n              enter: {\n                size: {\n                  value: 50\n                },\n                stroke: {\n                  value: \"black\"\n                }\n              },\n              update: {\n                x: {\n                  field: \"x\"\n                },\n                y: {\n                  field: \"y\"\n                },\n                fill: {\n                  field: \"color\"\n                }\n              }\n            }\n          },\n          {\n            type: \"text\",\n            from: {\n              data: \"tree\"\n            },\n            encode: {\n              enter: {\n                text: {\n                  field: \"name\"\n                },\n                fontSize: {\n                  value: 0\n                },\n                baseline: {\n                  value: \"bottom\"\n                }\n              },\n              update: {\n                x: {\n                  field: \"x\"\n                },\n                y: {\n                  field: \"y\"\n                }\n              }\n            }\n          }\n        ]\n      }\n    ]\n  };\n}\n\n\n\n\n\n\n\nvega = require(\"https://cdn.jsdelivr.net/npm/vega@5/build/vega.js\")\n\n\n\n\n\n\n\ncoin_data = [\n  { name: \"START\", id: 1, parent: \"\", color: \"red\" },\n  ...d3.range(2, 2 ** coins).map((i) =&gt; {\n    return {\n      name: i % 2 ? \"T\" : \"H\",\n      id: i,\n      parent: Math.floor(i / 2)\n    };\n  })\n].map((x) =&gt; {\n  let colors = { H: \"black\", T: \"white\", START: \"red\" };\n  x.color = colors[x.name];\n  return x;\n})"
  },
  {
    "objectID": "lectures/week11/handout/index.html#footnotes",
    "href": "lectures/week11/handout/index.html#footnotes",
    "title": "Lecture 11: Introduction to probability",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn criminal trials the threshold is termed “beyond reasonable doubt”. In civil trials, the probability of one must just be greater than the probability of the other, and there is no requirement that the probability of one exceeds the probability of the other by some threshold amount.↩︎\nAn alternative to Frequentist statistics is an approach known as Bayesian statistics. You won’t learn Bayesian statistics in your undergraduate courses (at least not in very much detail), but if you are interested in learning more then I do teach a course on it at Masters level, which you’ll be able to take in a few years time.↩︎\nProbabilities don’t always have to have numbers attached. There is a sense in which something can be more probable than something else without numbers being attached.↩︎"
  },
  {
    "objectID": "lectures/week01/slides/index.html#plan-for-today",
    "href": "lectures/week01/slides/index.html#plan-for-today",
    "title": "Psychology As A Science",
    "section": "Plan for today",
    "text": "Plan for today\n\nMeet the teaching team\nTips on doing well\nFinding the information you need\nKnowing how to get help\nResearch methods in your degree\nStructure of this module\n\nAssessments\nLecture topics"
  },
  {
    "objectID": "lectures/week01/slides/index.html#introductions",
    "href": "lectures/week01/slides/index.html#introductions",
    "title": "Psychology As A Science",
    "section": "Introductions",
    "text": "Introductions\nMain teaching is three people:\nDr Lincoln Colling (Lecture content convenor)\n\n\nCognitive Neuroscientist and software developer\n\nWork on Social Cognition, Numerical Cognition, and Philosophy of Cognitive Neuroscience\nDevelop statistics software\n\nResponsibilities\n\nLecture content\nFinal Exam"
  },
  {
    "objectID": "lectures/week01/slides/index.html#introductions-1",
    "href": "lectures/week01/slides/index.html#introductions-1",
    "title": "Psychology As A Science",
    "section": "Introductions",
    "text": "Introductions\n\n\nDr Jenny Terry (practical content convenor)\nEducational psychology researcher and psychometrician\n\nMeasures of Maths and Statistics anxiety\nStatistics education\n\nResponsibilities\n\nPractical content\nAssessments related to R\nLab report"
  },
  {
    "objectID": "lectures/week01/slides/index.html#introductions-2",
    "href": "lectures/week01/slides/index.html#introductions-2",
    "title": "Psychology As A Science",
    "section": "Introductions",
    "text": "Introductions\n\n\nDr Vlad Costin (practical co-lead)\nSocial Psychologist\n\nExistential psychology and subjective evaluations of meaning in life\nBelief systems and worldviews\n\nResponsibilities\n\nPractical content\nLab report"
  },
  {
    "objectID": "lectures/week01/slides/index.html#introductions-3",
    "href": "lectures/week01/slides/index.html#introductions-3",
    "title": "Psychology As A Science",
    "section": "Introductions",
    "text": "Introductions\nIn addition to the three main people there’s also:\n\nDr Bryan Singer who is responsible for the special Research Ethics lecture and the assessment that goes with it\nAn entire team of doctoral tutors who are there to help you in the practical classes and who will be responsible for marking your lab report\n\nKnowing who is responsible for which sections of the course will help to make sure that you contact the correct person\nWe spend a lot of time just re-directing queries, so just directing queries at the correct person will save everyone a lot of time"
  },
  {
    "objectID": "lectures/week01/slides/index.html#tips-for-doing-well",
    "href": "lectures/week01/slides/index.html#tips-for-doing-well",
    "title": "Psychology As A Science",
    "section": "Tips for doing well",
    "text": "Tips for doing well\nThe most important thing you can do to make sure you do well is attend all the classes\nComing to the lectures each week means:\nYou get exposed to all the lecture content without even trying!\n\nIt might be tempting to just watch the recordings, but it’s really easy to fall behind\n\nBut more importantly so much of what makes university great is the stuff that happens around the classes\n\nYou get to meet me and talk to me after class\nYou’ll get to meet to your classmates\nYou’ll get more of an authentic university experience"
  },
  {
    "objectID": "lectures/week01/slides/index.html#tips-for-doing-well-1",
    "href": "lectures/week01/slides/index.html#tips-for-doing-well-1",
    "title": "Psychology As A Science",
    "section": "Tips for doing well",
    "text": "Tips for doing well\nApart from showing up every week, the other thing you should do is:\nDo all the assessments!\n\nThis is particularly true to the assessments related to the practical content!\nEach of these assessments builds on the previous one, so if you miss too many it can be difficult to catch up.\n\nFor all the assessments you’ll do during the course, whether it’s the\n\nAssessments related to R\nOr the Lab Report\n\nit’ll be the first time you’ll be doing anything like them\nThis means they might appear daunting, but you’re all in the same boat, and if you stick with it, I know you can be successful"
  },
  {
    "objectID": "lectures/week01/slides/index.html#tips-for-doing-well-2",
    "href": "lectures/week01/slides/index.html#tips-for-doing-well-2",
    "title": "Psychology As A Science",
    "section": "Tips for doing well",
    "text": "Tips for doing well\nPsychology as a Science is not a difficult course, but most of the material we’ll be covering will be very new…\nFor example, I’ve mentioned R several times, but all you are probably thinking “what is R?”\nI don’t expect any of you will know what R is, but that is something you’ll learn about in this course!\n\n\nDon’t feel bad if you don’t know what R is or if you’ve never come across any other material in this course. If you knew all the material in this course, then this course will be pointless!\n\n\nYou’re here to learn new stuff and we don’t assume any background knowledge1\nNot even A-Level Psychology is assumed knowledge"
  },
  {
    "objectID": "lectures/week01/slides/index.html#finding-the-information-you-need",
    "href": "lectures/week01/slides/index.html#finding-the-information-you-need",
    "title": "Psychology As A Science",
    "section": "Finding the information you need",
    "text": "Finding the information you need\nAll the information you need is on canvas"
  },
  {
    "objectID": "lectures/week01/slides/index.html#finding-the-information-you-need-1",
    "href": "lectures/week01/slides/index.html#finding-the-information-you-need-1",
    "title": "Psychology As A Science",
    "section": "Finding the information you need",
    "text": "Finding the information you need\nClick through all the links and read the information"
  },
  {
    "objectID": "lectures/week01/slides/index.html#finding-the-information-you-need-2",
    "href": "lectures/week01/slides/index.html#finding-the-information-you-need-2",
    "title": "Psychology As A Science",
    "section": "Finding the information you need",
    "text": "Finding the information you need\n\nKeep a notebook handy while you read through the information on canvas\nNote down anything you think is particularly important\nOr bookmark pages you think are important\nCheck out the Frequently Asked Questions"
  },
  {
    "objectID": "lectures/week01/slides/index.html#knowing-who-to-contact",
    "href": "lectures/week01/slides/index.html#knowing-who-to-contact",
    "title": "Psychology As A Science",
    "section": "Knowing who to contact",
    "text": "Knowing who to contact\nOne thing that can be tricky when you’re first starting at university is knowing the correct person to contact if you need help.\nAdmin queries\n\nIf you’re meant to have an extension but it hasn’t been applied correctly\nIf you’re unable to submit your lab report because you’re unwell\n\nIf you have any of these kinds of queries then you should email the admin staff at psychology@sussex.ac.uk\n\n\nPlease don’t email teaching staff asking for extensions, because we can’t do anything about it.\nSee the handout (and canvas for more information about absences and extensions)"
  },
  {
    "objectID": "lectures/week01/slides/index.html#knowing-who-to-contact-1",
    "href": "lectures/week01/slides/index.html#knowing-who-to-contact-1",
    "title": "Psychology As A Science",
    "section": "Knowing who to contact",
    "text": "Knowing who to contact\nQueries about course content\nThe lecturers are your point of contact if you have queries about the course content\n\nFor queries about the lecture material and final exam I’m the best person to contact\nFor queries about the practical content or the lab report, you should take to Dr Terry or Dr Costin\n\n\n\nThe best way to get in touch with me is to talk to me after class!\n\n\nI’ll always hang around after class to answer any questions"
  },
  {
    "objectID": "lectures/week01/slides/index.html#asking-for-help",
    "href": "lectures/week01/slides/index.html#asking-for-help",
    "title": "Psychology As A Science",
    "section": "Asking for help",
    "text": "Asking for help\nApart from asking me a question after/during class, the best place to ask for help is on the Discord\nSign up here: https://discord.gg/TBf8Zju2xv\n\n\nYou can also book into a drop in session on Canvas:\n\nBook me for lecture questions\nDrs Terry or Costin for practical / lab report questions\nAnybody for general R queries"
  },
  {
    "objectID": "lectures/week01/slides/index.html#asking-for-help-1",
    "href": "lectures/week01/slides/index.html#asking-for-help-1",
    "title": "Psychology As A Science",
    "section": "Asking for help",
    "text": "Asking for help\nThe best ways to get help are to talk to us directly before/after class, by using the Discord, or by coming to the drop in sessions.\n\n\nYou shouldn’t need to email us unless there is a matter that you’d like to raise confidentially.\n\n\nThere’s lots of ways to get help, so make sure you make use of these methods first"
  },
  {
    "objectID": "lectures/week01/slides/index.html#research-methods-in-psychology",
    "href": "lectures/week01/slides/index.html#research-methods-in-psychology",
    "title": "Psychology As A Science",
    "section": "Research methods in Psychology",
    "text": "Research methods in Psychology\nPsychology as a Science is only the first in a series of research methods modules you’ll do during your degree.\n\n\nFollowing this module you’ll take the following modules:\n\nAnalysing Data (next term)\nDiscovering Statistics (year 2)\nQuantitative and Qualitative Methods (year 2)\n\nAll these courses build on each other and prepare you for your research dissertation in your final year\nThey’re also a great way to learn a lot of transferable skills that are useful outside university, for example: 1) how to analyse data; 2) how to make sense of statistics; 3) computer programming/coding skills."
  },
  {
    "objectID": "lectures/week01/slides/index.html#why-research-methods",
    "href": "lectures/week01/slides/index.html#why-research-methods",
    "title": "Psychology As A Science",
    "section": "Why research methods?",
    "text": "Why research methods?\n\nDominant approach to training psychologists is the scientist practitioner model\nDoing research is integral to this approach!\n\nJust like medical doctors, who not only deliver treatments but also develop treatments, psychologists also apply and produce knowledge.\nYou want to do what works and being able to read, critique, and conduct research will help you know what works and allow you to develop evidence-based care\nEven for those that don’t become psychologists, research methods is a useful skills that can prepare you for careers in, data science, civil service, consultancy, and more"
  },
  {
    "objectID": "lectures/week01/slides/index.html#structure-of-this-module",
    "href": "lectures/week01/slides/index.html#structure-of-this-module",
    "title": "Psychology As A Science",
    "section": "Structure of this module",
    "text": "Structure of this module\nLike most research methods modules PAAS is made up of three main activities:\n\nWeekly lectures\n\nOne hour each week.\nCovers research methods, statistics, and theory\nNote that you’ll also have a special lecture one evening in Week 6 that will cover research ethics. Check your timetable!\n\n\nNote, that Psychology as a Science does not have an additional reading list, because you’ll have enough R homework for the practicals to keep you busy"
  },
  {
    "objectID": "lectures/week01/slides/index.html#structure-of-this-module-1",
    "href": "lectures/week01/slides/index.html#structure-of-this-module-1",
    "title": "Psychology As A Science",
    "section": "Structure of this module",
    "text": "Structure of this module\n\nTutorials/Practical preparation homework\n\nAbout an hour a week.\nDone in R as preparation for the practical class.\n\nPractical classes\n\nTwo hours a week.\nGives you hands-on experience with the R programming language.\n\n\nYou’ll find out more about the exact structure of the tutorials and practicals in the practical sessions."
  },
  {
    "objectID": "lectures/week01/slides/index.html#lecture-topics",
    "href": "lectures/week01/slides/index.html#lecture-topics",
    "title": "Psychology As A Science",
    "section": "Lecture topics",
    "text": "Lecture topics\nLecture topics divided into two sections\nIn part one you’ll do:\n\n\n\n\n\nWeek\nTopic\n\n\n\n\n1\nIntroduction to Psychology as a Science\n\n\n2\nWhat is this thing called \"Science\"?\n\n\n3\nApproaches to Research\n\n\n4\nIntroduction to study design\n\n\n5\nOpen Science\n\n\n\n\n\n\n\nThese lectures are all about the research process, and they’ll prepare you for the lab report"
  },
  {
    "objectID": "lectures/week01/slides/index.html#lecture-topics-1",
    "href": "lectures/week01/slides/index.html#lecture-topics-1",
    "title": "Psychology As A Science",
    "section": "Lecture topics",
    "text": "Lecture topics\nIn part two you’ll do:\n\n\n\n\n\nWeek\nTopic\n\n\n\n\n6\nDescribing measurements I\n\n\n7\nDescribing measurements II\n\n\n8\nDistributions\n\n\n9\nTransformation and comparisons\n\n\n10\nVisual summaries of data\n\n\n11\nIntroduction to probability\n\n\n\n\n\n\n\nThese lectures focus on statistical concepts, and they’ll prepare you for learning about statistical testing in Semester two"
  },
  {
    "objectID": "lectures/week01/slides/index.html#lecture-topics-2",
    "href": "lectures/week01/slides/index.html#lecture-topics-2",
    "title": "Psychology As A Science",
    "section": "Lecture topics",
    "text": "Lecture topics\nThe first set of lectures will cover big picture ideas. These lectures will probably be most useful in helping you to prepare for the report.\n\nWhat are scientific theories?\nWhat issues do we need to consider when we’re measuring phenomena?\nWhat does it mean to operationalise our variables?\nWhat are different approaches you can take when conducting a study?\nWhat are some sources of bias in psychology studies and publishing of psychology studies, and how might we be able to ameliorate some of these biases?"
  },
  {
    "objectID": "lectures/week01/slides/index.html#lecture-topics-3",
    "href": "lectures/week01/slides/index.html#lecture-topics-3",
    "title": "Psychology As A Science",
    "section": "Lecture topics",
    "text": "Lecture topics\nThe second set of lectures are all about preparing you for learning about statistics and working with data.\nIn these lectures you’ll learn the underlying theory of statistical testing. You’ll learn how to reason about statistics and data, and the relationship between scientific hypotheses and statistical hypotheses.\nDoing statistics isn’t like following a recipe. It’s not about just picking the “correct” statistical test out of a list.\nIt involves thinking about what you want to know, why you want to know it, and how statistics can help you to know it. So we spend a bit of time this term just learning about this reasoning before you actually learn about statistical tests next term."
  },
  {
    "objectID": "lectures/week01/slides/index.html#closing-thoughts",
    "href": "lectures/week01/slides/index.html#closing-thoughts",
    "title": "Psychology As A Science",
    "section": "Closing thoughts",
    "text": "Closing thoughts\n\nCan’t emphasise enough that this course is not inherently difficult\nMost people do very well\nBUT you’ll only do well if you keep up with the work\n\nThis means:\n\nComing to all the classes (Lectures and Practicals)\nDoing all the assessments (particularly the R assessments)\nDoing all the R tutorial activities\nComing to drop in sessions if you’re stuck or confused about anything\n\nThe better you do in this course, the easier you’ll find the methods courses that follow\nAnd the better you do at methods the better you’ll do in your final year dissertation"
  },
  {
    "objectID": "lectures/week02/slides/index.html#philosophy-of-science",
    "href": "lectures/week02/slides/index.html#philosophy-of-science",
    "title": "Psychology As A Science",
    "section": "Philosophy of Science",
    "text": "Philosophy of Science\nThe title of this course is “Psychology as a Science”\n\nBut what is this thing called “Science”?\nIs it a special way of learning about the world?\nAnd if so, then what makes it special?"
  },
  {
    "objectID": "lectures/week02/slides/index.html#the-common-sense-view-of-science",
    "href": "lectures/week02/slides/index.html#the-common-sense-view-of-science",
    "title": "Psychology As A Science",
    "section": "The common-sense view of science",
    "text": "The common-sense view of science\nThe common-sense view might go something like this:\n\nScience is special because it’s knowledge based on facts\n\nScience is often contrasted with other forms of knowledge that might be:\n\nbased on authority (e.g., celebrities, religious and political leaders)\nrevelation (e.g., personal religious or spiritual experiences)\nsuperstition (e.g., “knowledge of the ancients”)\n\nBut this raises two questions:\n\nIf science is based on facts, then where do “facts” come from?\nHow is knowledge then derived from these facts"
  },
  {
    "objectID": "lectures/week02/slides/index.html#where-do-facts-come-from",
    "href": "lectures/week02/slides/index.html#where-do-facts-come-from",
    "title": "Psychology As A Science",
    "section": "Where do facts come from?",
    "text": "Where do facts come from?\nThe common-sense view of science was formalised by two schools of thought: The empiricists and the positivists\nTogether they held a view that went something like this:\n\nKnowledge should be derived from the facts of experience\n\nWe can break this idea down:\n\nCareful and unbiased observers can directly access facts through the senses/observation.\nFacts come before and are independent of, theories.\nFacts form a firm and reliable base for scientific knowledge.\n\nBut is this true?"
  },
  {
    "objectID": "lectures/week02/slides/index.html#facts-through-the-senses",
    "href": "lectures/week02/slides/index.html#facts-through-the-senses",
    "title": "Psychology As A Science",
    "section": "Facts through the senses",
    "text": "Facts through the senses\n\n\n\n\n\n\n\n\n\n\n\n\nA simple story of how the senses work might go like this: There are some external physical causes (light, sound waves etc.) that produce some physical changes in our sense organs (e.g., our eyes) that are then registered by the brain.\nThis account implies direct and unmediated access to the world through our senses. But is this actually the case?\n\nThis image could be seen as an old woman or a young woman. Some of you might see one and not the other while some might see both and switch between them.\nThe physical causes (the light hitting our eyes) is more-or-less the same for everyone, but you might “see” different things."
  },
  {
    "objectID": "lectures/week02/slides/index.html#observation-is-not-theory-free",
    "href": "lectures/week02/slides/index.html#observation-is-not-theory-free",
    "title": "Psychology As A Science",
    "section": "Observation is not theory-free",
    "text": "Observation is not theory-free\nThe previous example is just a toy example, but it reveals a larger point:\nTwo scientists might “observe” something different even when looking at the same thing.\nIn some fields, being able to make “observations” actually requires training. For example,\n\nTraining in how to observe stuff through a microscope\nTraining in how to distinguish different kinds of behaviour\nTraining in how to read an x-ray\n\nSo a simple claim that observations are “unbiased” or “straightforwardly given by the senses” seems to be false."
  },
  {
    "objectID": "lectures/week02/slides/index.html#but-what-do-we-even-mean-by-facts",
    "href": "lectures/week02/slides/index.html#but-what-do-we-even-mean-by-facts",
    "title": "Psychology As A Science",
    "section": "But what do we even mean by facts?",
    "text": "But what do we even mean by facts?\nWhen we think of a “fact” there are two things we could mean\n\n“Fact” could refer to some external state of the world\nOr “fact” could refer to statements about those external states\n\nThe fact that this university is in East Sussex could refer to this actual university and its actual being in East Sussex\nor it could refer to the statement: “This university is in East Sussex.”\nWhen we talk about “facts” as the basis for science, we’re talking about these statements.\nWe’ll call this type of fact an “observation statement.”"
  },
  {
    "objectID": "lectures/week02/slides/index.html#do-facts-come-before-theories",
    "href": "lectures/week02/slides/index.html#do-facts-come-before-theories",
    "title": "Psychology As A Science",
    "section": "Do facts come before theories?",
    "text": "Do facts come before theories?\nThink of a child learning the word apple:\n\nThey might initially imitate the word “apple” when shown an apple by their parent.\nNext, they might use the word “apple” when pointing to apples.\nBut then one day they might see a tennis ball and say “apple”. The parent would then correct them, and show them that a tennis ball isn’t an apple because you can’t, for example, bite into it like an apple.\nBy the time the child can make accurate “observation statements” about the presence of apples they might already know a lot about the properties of apples (have an extensive “theory of apples”)\n\nThe same goes for scientists: formulating observation statements might require substantial background knowledge or a conceptual framework to place them in.\nThey aren’t completely independent of theory."
  },
  {
    "objectID": "lectures/week02/slides/index.html#not-just-facts-but-relevant-facts",
    "href": "lectures/week02/slides/index.html#not-just-facts-but-relevant-facts",
    "title": "Psychology As A Science",
    "section": "Not just facts, but relevant facts",
    "text": "Not just facts, but relevant facts\nLet’s say that we’ve been able to acquire some facts. Will any old facts do?\nLet’s take a simple example:\n\nYou observe that grass grows longer among the cowpats in a field.\nYou think this is because the dung traps moisture that helps the grass grow.\nYour friend thinks this is because the dung acts as a fertiliser.\n\nObservations alone can’t distinguish these. To tell which is correct you need to intervene on the situation.\nFor example, grind up the dung so that it still fertilises the ground or use something else to trap the moisture.\nIntervening, for example, through experiment allows you to tell what the relevant facts of your observation are."
  },
  {
    "objectID": "lectures/week02/slides/index.html#active-observation-and-intervention",
    "href": "lectures/week02/slides/index.html#active-observation-and-intervention",
    "title": "Psychology As A Science",
    "section": "Active observation and intervention",
    "text": "Active observation and intervention\nBy intervening on the system, we can tell which facts are relevant\nBut scientific theories may play a part in helping to determine what is and isn’t relevant\nAn example from the history of cognitive psychology:\n\nIn certain kinds of reading tasks psychologists thought it was relevant that people made errors, but they didn’t think the exact nature of the errors was relevant.\nBut after certain kinds of theories were developed (ones based on neural network models) they came to realise that the particular kinds of errors (e.g., if people swapped letters between words) was relevant.\n\nIn short, observations can’t be completely divorced from theories.\nAnd experiments will presume the truth of certain theories (e.g., brain imaging experiments assume the validity of certain theories about brain function)."
  },
  {
    "objectID": "lectures/week02/slides/index.html#objectivity",
    "href": "lectures/week02/slides/index.html#objectivity",
    "title": "Psychology As A Science",
    "section": "“Objectivity”",
    "text": "“Objectivity”\n\nFacts don’t care about your feelings\n\n — Guy on the internet\nThe idea that science is objective in a simple sense of “objectivity” is misleading.\nYour conceptual framework, and theoretical assumptions, and even your knowledge and training can play a part in what kinds of observations you can make or what types of observation statements you can formulate\n“Objectivity” doesn’t mean observations free from theoretical assumptions (“the view from nowhere”)"
  },
  {
    "objectID": "lectures/week02/slides/index.html#deriving-theories-from-facts",
    "href": "lectures/week02/slides/index.html#deriving-theories-from-facts",
    "title": "Psychology As A Science",
    "section": "Deriving theories from facts",
    "text": "Deriving theories from facts\nThe final part of the common-sense view of science is that scientific knowledge is derived from facts.\nUsually this idea of derived means something like logically derived. We might sum up the view like this:\n\nScience = Facts + Logic\n\n — Guy on the internet\nTo understand what it might mean to logically derive scientific knowledge we need to know a bit about logic."
  },
  {
    "objectID": "lectures/week02/slides/index.html#induction",
    "href": "lectures/week02/slides/index.html#induction",
    "title": "Psychology As A Science",
    "section": "Induction",
    "text": "Induction\nThe process of induction allows us to construct arguments of the following form:\nPremises\n\nEmily the swan is white\nKevin the swan is white\n… the swan is white\n\nConclusion\nAll swans are white\nBut the problem with arguments like this is that all the premises may be true and yet the conclusion can be false.\nMaybe we just haven’t observed the one swan that isn’t white?"
  },
  {
    "objectID": "lectures/week02/slides/index.html#collecting-observations",
    "href": "lectures/week02/slides/index.html#collecting-observations",
    "title": "Psychology As A Science",
    "section": "Collecting observations",
    "text": "Collecting observations\nBut surely there are good and bad inductive arguments?\n\nMore observations are better than fewer observations—but how many is enough?\nObservations in many different contexts—but what makes a context different and what makes differences relevant?\n\nDifferent contexts should be novel in some sense\nThat is, it should not just be trivial changes\n\nNo contradicting observations—but what about probabilistic phenomena?\n\nClear and simple rules aren’t easy to come by.\nBut the bigger problem is induction can never establish truth.\nSo how do we ever prove anything for certain in science. The short answer is, we don’t.\nWe can never be certain of truth."
  },
  {
    "objectID": "lectures/week02/slides/index.html#using-induction-and-deduction",
    "href": "lectures/week02/slides/index.html#using-induction-and-deduction",
    "title": "Psychology As A Science",
    "section": "Using induction and deduction",
    "text": "Using induction and deduction\nInstead of just collecting confirmations we can employ induction and deduction together\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollect observations and use induction to come up with general laws and theories from particular observations\nUse deduction to figure out what logically follows from these general laws and theories\n\n\n\nThis approach nicely captures the idea of testability\nOur theories should make predictions about what we expect to find, and we can test these predictions with more observations"
  },
  {
    "objectID": "lectures/week02/slides/index.html#deduction-and-knowing-what-is-false",
    "href": "lectures/week02/slides/index.html#deduction-and-knowing-what-is-false",
    "title": "Psychology As A Science",
    "section": "Deduction and knowing what is false",
    "text": "Deduction and knowing what is false\nThe philosopher Karl Popper saw trouble with relying on induction. He wanted to put science on a firmer logical footing.\nHe proposed that while you can’t use deduction to figure out what is true, you can use deduction to figure out what is false\nHe said a key quality of scientific theories is that they should be falsifiable\n\n\nTheories can come into existence through any means (wild speculation, guesses, dreams, or whatever), but once a theory has been proposed it has to be rigorously and ruthlessly tested"
  },
  {
    "objectID": "lectures/week02/slides/index.html#degrees-of-falsifiability",
    "href": "lectures/week02/slides/index.html#degrees-of-falsifiability",
    "title": "Psychology As A Science",
    "section": "Degrees of falsifiability",
    "text": "Degrees of falsifiability\nGood theories are falsifiable, better theories are more falsifiable\n\n\nThree theories:\n\nMars moves in an elliptical orbit\nMars and Venus move in elliptical orbits\nPlanets move in elliptical orbits\n\n\nOf these three theories, (1) is the least falsifiable and (3) is the most falsifiable. Why? For theory (1) only an observation of Mars could falsify it. But for theory (3), an observation of Mars, Venus, Saturn, Neptune, or any other yet undiscovered planet would falsify it.\n\n\nBad theories are ones that can seemingly accommodate any observation\nIf two outcomes are possible and the theory can explain outcome one and outcome two then this is bad. What would be evidence against the theory?\nGood theories are broad in their applicability but precise in their predictions"
  },
  {
    "objectID": "lectures/week02/slides/index.html#encountering-a-falsifier",
    "href": "lectures/week02/slides/index.html#encountering-a-falsifier",
    "title": "Psychology As A Science",
    "section": "Encountering a falsifier",
    "text": "Encountering a falsifier\nWhat happens when you make an observation that falsifies a theory?\nThat is, you observe something that contradicts the theory you’re testing. What do you do?\nYour options:\n\nYou could abandon the theory\n\nBut what about probabilistic theories?\nAnd what about auxiliary assumptions?\n\nYou could modify or amend the theory\n\nBut are their better ways and worse ways to do this?"
  },
  {
    "objectID": "lectures/week02/slides/index.html#probabilistic-theories",
    "href": "lectures/week02/slides/index.html#probabilistic-theories",
    "title": "Psychology As A Science",
    "section": "Probabilistic theories",
    "text": "Probabilistic theories\nTheories in psychology tend to be probabilistic. They make claims about how things are on average, not claims about how things are in every case.1\nMuch of what we do with statistics is figuring out how to test and specify probabilistic claims. For example:\n\nWhat does it mean for things to be different on average?\nHow many cases do you have to observe before you have evidence for a probabilistic claim?\nHow many cases do you have to observe before you have evidence against a probabilistic claim (that you might previously have believed)?\n\nBut putting that aside, a single contradictory observation can’t falsify a probabilistic claim because we will sometimes expect contradictions with probabilistic claims.\nA probabilistic claim might say something like on average “men are taller than women”, but of course there are shorter men and taller women."
  },
  {
    "objectID": "lectures/week02/slides/index.html#abandoning-the-theory",
    "href": "lectures/week02/slides/index.html#abandoning-the-theory",
    "title": "Psychology As A Science",
    "section": "Abandoning the theory",
    "text": "Abandoning the theory\nPutting aside the probabilistic nature of claims (or assuming you’ve seen enough contradictory examples), should these contradictory observations lead you to abandon the theory?\nExperiments don’t test one theory in isolation, but rely on a range of auxiliary assumptions and other support theories.\nFor example, an experiment on memory using brain imaging is also making assumptions about the truth of theories related to:\n\nThe physics of how brain imaging machines work\nTheories about how brains work by, for example, requiring oxygen during neural processing\n\nAnd possibly many other theories"
  },
  {
    "objectID": "lectures/week02/slides/index.html#revising-and-amending-theories",
    "href": "lectures/week02/slides/index.html#revising-and-amending-theories",
    "title": "Psychology As A Science",
    "section": "Revising and amending theories",
    "text": "Revising and amending theories\nBut if we decide to amend a theory, then how do we do this?\nTheory: All bread is nourishing\nObservation: Bread eaten in a French village in 1951 was not nourishing1\nAd-hoc modification\nAll bread except bread eaten in a French village in 1951 is nourishing\nModification has fewer tests: Original theory can be tested by eating any bread. Modified theory can be tested by eating any bread except that particular bread.\nAcceptable modification\nAll bread except bread made with flour containing ergot fungus is nourishing\nModification leads to new tests: 1) Test the bread for the presence of fungus; 2) Cultivate fungus and make bread with it and test whether it nourishes; 3) Analyse fungus for poisons, etc\nhttps://en.wikipedia.org/wiki/1951_Pont-Saint-Esprit_mass_poisoning"
  },
  {
    "objectID": "lectures/week02/slides/index.html#problems-with-poppers-falsificationism",
    "href": "lectures/week02/slides/index.html#problems-with-poppers-falsificationism",
    "title": "Psychology As A Science",
    "section": "Problems with Popper’s falsificationism",
    "text": "Problems with Popper’s falsificationism\nPopper’s focus on falsifying theories leads to a couple of problems:\n\nIt can be difficult to figure out when to abandon theories and when to amend theories.\n\nAre all parts of the theoretical web of the same status?\n\nIt can be difficult to compare two theories to see which is “better”\n\nFor example, if you have Theory A and Theory B and neither has been falsified, which is the better theory? The one with more confirming observations? But then won’t trivial theories always win?\n\n\nThe philosopher Imre Lakatos developed his idea of research programmes1 as a reaction to these two problems.\nA similar idea was developed by the philosopher Thomas Kuhn, but Kuhn used the term paradigms for his idea."
  },
  {
    "objectID": "lectures/week02/slides/index.html#research-programmes",
    "href": "lectures/week02/slides/index.html#research-programmes",
    "title": "Psychology As A Science",
    "section": "Research programmes",
    "text": "Research programmes\nOne key aspect of Lakatos’s idea of research programmes is that not all parts of a science are on par\n\nSome laws or principles are so fundamental they might be considered a defining part of the science.\nOther parts might be more peripheral\n\nLakatos called these fundamental parts the hard core and the more peripheral parts the protective belt\nHe suggested that the hard core is resistant to falsification, so when an apparent falsifier is observed the blame is placed on theories in the protective belt\nResearch programmes are defined by what is in their hard core"
  },
  {
    "objectID": "lectures/week02/slides/index.html#working-within-a-research-programme",
    "href": "lectures/week02/slides/index.html#working-within-a-research-programme",
    "title": "Psychology As A Science",
    "section": "Working within a research programme",
    "text": "Working within a research programme\nOn Lakatos’s view, scientists work within a research programme.\nHe split guidelines for working within a research programme into a negative and positive heuristic, specifying what scientists shouldn’t do but also what they should do\n\nThe negative heuristic includes things like not abandoning the hard core\nThe positive heuristic is harder to specify exactly, but it includes suggestions on how to supplement the protective belt to develop the research programme further\n\nThat is, it should specify a programme of research\nThe research programme should identify problems to solve"
  },
  {
    "objectID": "lectures/week02/slides/index.html#progressive-degenerating-programmes",
    "href": "lectures/week02/slides/index.html#progressive-degenerating-programmes",
    "title": "Psychology As A Science",
    "section": "Progressive / degenerating programmes",
    "text": "Progressive / degenerating programmes\nLakatos was also interested in comparing research programmes, something that is difficult to do on a strictly falsificationist account.\nHe divided research programmes into those that are progressive and those that are degenerating\n\nProgressive research programmes are coherent (i.e., have minimal contradictions)\n\nAnd progressive research programmes make novel predictions that follow naturally from theories that are part of the programme\nThese predictions are then confirmed by experiments\n\nDegenerating research programmes are those that have faced so many falsifications that they have been modified to the point of being incoherent\n\nAt this point, it’s no longer sustainable to carry on modifying the protective belt, and instead, the hard core must be abandoned"
  },
  {
    "objectID": "lectures/week02/slides/index.html#final-thoughts",
    "href": "lectures/week02/slides/index.html#final-thoughts",
    "title": "Psychology As A Science",
    "section": "Final thoughts",
    "text": "Final thoughts\nIn thinking about the nature of science, the status of science, and the processes we use to create and justify knowledge, there are a lot of things to talk about.\nThis one course doesn’t have time to do them all justice.\nIn particular, I would’ve liked to talk about Feminist perspectives on science, the status of indigenous epistemologies (theories of knowledge), and how race, racism, and colonialism impact science.\nIf any of these topics interest you check out the links in the handout"
  },
  {
    "objectID": "lectures/week03/slides/index.html#approaches-to-research",
    "href": "lectures/week03/slides/index.html#approaches-to-research",
    "title": "Psychology As A Science",
    "section": "Approaches to research",
    "text": "Approaches to research\nDoing research is an integral part of your training as a psychologist.\nBut before you can start thinking about doing research you need to be aware of the different approaches that are available to you.\nIn today’s lecture, we’ll cover approaches from the two traditional divisions:\n\nqualitative methods and\nquantitative methods\n\nAnd we’ll finish off by talking about how computer simulation can be used in psychology research"
  },
  {
    "objectID": "lectures/week03/slides/index.html#qualitative-and-quantitative-methods",
    "href": "lectures/week03/slides/index.html#qualitative-and-quantitative-methods",
    "title": "Psychology As A Science",
    "section": "Qualitative and Quantitative methods",
    "text": "Qualitative and Quantitative methods\nWe can split approaches to research into two broad categories\nWe can give some simple descriptions of these categories:\n\nQuantitative methods collect numbers/numerical data and use statistical tools\nQualitative methods collect words, pictures, and artefacts\n\nSome researchers also adopt both approaches (mixed-methods) or apply quantitative methods to qualitative style data.\nQuantitative methods are probably easier to group together, because many approaches can be grouped under qualitative methods.\nThis course focuses on quantitative methods, but you’ll learn more about qualitative methods in later years."
  },
  {
    "objectID": "lectures/week03/slides/index.html#qualitative-methods",
    "href": "lectures/week03/slides/index.html#qualitative-methods",
    "title": "Psychology As A Science",
    "section": "Qualitative methods",
    "text": "Qualitative methods\nQualitative methods are extremely varied with different methodologies, underlying theoretical assumptions, and intellectual histories\nI can’t do them all justice in one short lecture, so we’ll only cover a few:\n\nVerbal protocol analysis\nEthnographic methods\nDiscourse analysis\nPhenomenology\n\nHowever, there are many more, including Case Studies, Grounded Theory, Participatory Research, Focus Groups…\nI’ll try to draw out some contrasts between qualitative and quantitative methods more generally and highlight strengths and weaknesses of each approach."
  },
  {
    "objectID": "lectures/week03/slides/index.html#issues-in-qualitative-research",
    "href": "lectures/week03/slides/index.html#issues-in-qualitative-research",
    "title": "Psychology As A Science",
    "section": "Issues in qualitative research",
    "text": "Issues in qualitative research\nUnlike quantitative methods than might use printed questionnaires or computers to record and measure responses, in qualitative research, the researcher is the instrument\n\nimportant for researchers to reflect on their values, assumptions, biases, and beliefs to understand how these might impact the research\nthe research instrument (i.e., the researcher) can change. For example, in ethnographic research, the changes in the researchers experience might alter how they record and observe behaviours.\n\nThere are parallels to validity (internal and external), reliability, and “objectivity” in qualitative research1\nThese are Credibility, Transferability, Dependability, and Confirmability\nWe’ll touch on these topics today, but you’ll also learn more about these concepts in coming lectures"
  },
  {
    "objectID": "lectures/week03/slides/index.html#issues-in-qualitative-research-1",
    "href": "lectures/week03/slides/index.html#issues-in-qualitative-research-1",
    "title": "Psychology As A Science",
    "section": "Issues in qualitative research",
    "text": "Issues in qualitative research\n\nCredibility: Can the data support the claims. Can be established through prolonged engagement, discussions with other researchers/participants, and critical self-reflection\nTransferability: Can the findings be transferred to similar contexts. Requires extensive, detailed, and careful descriptions of the research context (“thick descriptions”).\nDependability: Ensuring that researchers maintain a record of changes in the research process or research instrument (i.e., themselves) over time.\nConfirmability: Concerned with ensuring that the data used to support the conclusions are verifiable."
  },
  {
    "objectID": "lectures/week03/slides/index.html#quantitative-methods",
    "href": "lectures/week03/slides/index.html#quantitative-methods",
    "title": "Psychology As A Science",
    "section": "Quantitative methods",
    "text": "Quantitative methods\n\nAs the name suggests, a key aspect of quantitative methods is quantification.\nQuantification means putting numbers to the thing we’re interested in studying so that it can be measured.\nThe motivation behind measuring phenomena is that measurements are publicly available and verifiable (e.g., scientists can check or verify your measurements).\nUnlike qualitative research where researchers try to simultaneously study many aspects of a single phenomenon, quantitative research tries to condense a phenomenon down into a single (or a few) dimension(s).\nThe first step in quantitative research is often figuring out how to quantify the phenomenon of interest. This involves choosing a proxy (something measurable) that can stand-in for the phenomenon"
  },
  {
    "objectID": "lectures/week03/slides/index.html#quantitative-methods-and-causation",
    "href": "lectures/week03/slides/index.html#quantitative-methods-and-causation",
    "title": "Psychology As A Science",
    "section": "Quantitative methods and causation",
    "text": "Quantitative methods and causation\nQualitative research studies phenomena in the wild\n\n\nBut Quantitative approaches instead try to exert a lot of control over phenomena.\n \nControl allows researchers to make claims about causation and give causal explanations\n\n\nThere are a few ways to understand causation:\n\nAs a difference that makes a difference\nIn terms of manipulation\nIn terms of probability"
  },
  {
    "objectID": "lectures/week03/slides/index.html#causation-and-confounds",
    "href": "lectures/week03/slides/index.html#causation-and-confounds",
    "title": "Psychology As A Science",
    "section": "Causation and confounds",
    "text": "Causation and confounds\nIn the examples above they are all examples of possible causes\nTo be justified in claiming a causal relationship other conditions must usually be met\nBut causal claims are not always black and white. Sometimes we can only be more or less sure about causal relationships.\nWhat are some of the other conditions that need to be met?"
  },
  {
    "objectID": "lectures/week03/slides/index.html#qualitative-vs-quantitative-methods",
    "href": "lectures/week03/slides/index.html#qualitative-vs-quantitative-methods",
    "title": "Psychology As A Science",
    "section": "Qualitative vs Quantitative methods",
    "text": "Qualitative vs Quantitative methods\nIn qualitative research, you study phenomena in context while in quantitative research you aim for control.\nBut you can use either approach to study the same phenomena/psychological processes. Let’s say you’re interested in memory:\nHow could you study memory from:\n\nA qualitative perspective?\nA quantitative perspective?"
  },
  {
    "objectID": "lectures/week03/slides/index.html#quantitative-study-of-memory",
    "href": "lectures/week03/slides/index.html#quantitative-study-of-memory",
    "title": "Psychology As A Science",
    "section": "Quantitative study of memory",
    "text": "Quantitative study of memory\nYou could use experiments in a lab where you give people lists of words to remember. You could manipulate aspects of the words—for example, their emotional salience—and measure performance (accuracy scores) to try to understand something about memory and emotional salience.\nEnsure that the only thing that differs between the words on each list is the emotional salience.\nControl for possible confounds like:\n\nWord length: make sure that one list doesn’t contain long words and the other short words\nWord order: make sure some people get the lists in one order and some in the other order, because maybe people get tired by the end and that influences memory."
  },
  {
    "objectID": "lectures/week03/slides/index.html#qualitative-study-of-memory",
    "href": "lectures/week03/slides/index.html#qualitative-study-of-memory",
    "title": "Psychology As A Science",
    "section": "Qualitative study of memory",
    "text": "Qualitative study of memory\nFor a qualitative approach you don’t want to study memory in the lab—you want to study it in the wild. This allows you to ask different kinds of questions.\nYou could use an ethnographic approach with, for example, bartenders:\n\nYou might do fieldwork in a bar observing bartenders.\nThrough this, you might see that bartenders structure their environment in a particular way—e.g., put certain types of glasses or bottles in particular places.\n\nThis might lead you to form the hypothesis that bartenders structure their environment to support their memory—i.e., placing certain bottles and glasses together helps them remember what goes in what kinds of cocktails.\n\nConduct follow-up interviews or observe the training of bartenders for further evidence for\nEngage in bartending and critically reflect on your own experience"
  },
  {
    "objectID": "lectures/week03/slides/index.html#computer-simulation-and-formal-methods",
    "href": "lectures/week03/slides/index.html#computer-simulation-and-formal-methods",
    "title": "Psychology As A Science",
    "section": "Computer simulation and formal methods",
    "text": "Computer simulation and formal methods\nQualitative and quantitative methods try to understand phenomena by studying the phenomena themselves. The data they use comes from the phenomena.\nIn approaches like computer simulation and formal/mathematical modelling researchers instead generate the data.\nResearchers try to build systems that replicate or reproduce some aspects of systems or phenomena they are studying.\n\nThis might allow them to gain new insights into these systems.\nComparing the behaviour of their artificial systems with the natural system allows researchers to test theories about the processes that produce phenomena"
  },
  {
    "objectID": "lectures/week03/slides/index.html#final-thoughts",
    "href": "lectures/week03/slides/index.html#final-thoughts",
    "title": "Psychology As A Science",
    "section": "Final thoughts",
    "text": "Final thoughts\nThere are a lot of different methodological approaches available to you in psychology, and this lecture has just touched on a few\nYou won’t cover computer simulation again in undergrad, but if you stay on to do a Masters degree (or a third year project with me) then you might get the chance\nNext year, you’ll learn more about qualitative methods…\n… But for the rest of this course we’ll dive more deeply into quantitative methods"
  },
  {
    "objectID": "lectures/week04/slides/index.html#the-how-of-quantitative-research",
    "href": "lectures/week04/slides/index.html#the-how-of-quantitative-research",
    "title": "Psychology As A Science",
    "section": "The “how” of quantitative research",
    "text": "The “how” of quantitative research\nThe conclusions that we can draw from research depends on how the knowledge was generated\nFor any piece of research we plan (or any research we read), we must be able to answer:\n\nHow do we actually test hypotheses appropriately?\nHow do we generalise our findings?\nHow do we quantify seemingly unquantifiable things?\n\nThe answer to these questions lies in research design"
  },
  {
    "objectID": "lectures/week04/slides/index.html#the-how-of-quantitative-research-1",
    "href": "lectures/week04/slides/index.html#the-how-of-quantitative-research-1",
    "title": "Psychology As A Science",
    "section": "The “how” of quantitative research",
    "text": "The “how” of quantitative research\nResearch designs can vary on lots of different dimensions:\n\nSome designs have some kind of manipulation and others don’t\nSome designs involve multiple measurements from the same people and some design compare groups\nSome designs take all their measurement at one point in time and others follow participants across time\n\nThe design we choose depends on:\n\nOur hypothesis\nThe resources we have (time, money, facilities)\nLogistical considerations\nEthical considerations"
  },
  {
    "objectID": "lectures/week04/slides/index.html#an-example-ice-cream-and-murder",
    "href": "lectures/week04/slides/index.html#an-example-ice-cream-and-murder",
    "title": "Psychology As A Science",
    "section": "An example: Ice cream and murder",
    "text": "An example: Ice cream and murder\n\nFigure 1: The relationship between the murder rate and ice cream sales in New York City"
  },
  {
    "objectID": "lectures/week04/slides/index.html#an-example-ice-cream-and-murder-1",
    "href": "lectures/week04/slides/index.html#an-example-ice-cream-and-murder-1",
    "title": "Psychology As A Science",
    "section": "An example: Ice cream and murder",
    "text": "An example: Ice cream and murder\nWe might decide to conduct some research into this relationship between ice cream and murder to see whether there’s actually a causal relationship1\n\n\n\nHow would we actually go about this?\n\n\n\n\nWe start with a research question\n\nThis is the question we hope our research will answer\nWe might have something like the following:\n\nDoes eating ice cream make you more prone to murderous tendencies?\n\nFor now, we’ll just ignore all the ethical issues"
  },
  {
    "objectID": "lectures/week04/slides/index.html#an-example-ice-cream-and-murder-2",
    "href": "lectures/week04/slides/index.html#an-example-ice-cream-and-murder-2",
    "title": "Psychology As A Science",
    "section": "An example: Ice cream and murder",
    "text": "An example: Ice cream and murder\n\nNext we come up with a hypothesis\n\nIn our hypothesis we specify the outcome we expect\nWe might have something like the following:\n\nEating ice cream increases the desire to commit murder\n\nTo test this hypothesis we’ll design an experiment…"
  },
  {
    "objectID": "lectures/week04/slides/index.html#features-of-good-study-design",
    "href": "lectures/week04/slides/index.html#features-of-good-study-design",
    "title": "Psychology As A Science",
    "section": "Features of good study design",
    "text": "Features of good study design\nIn a well designed experiment, we can be confident in saying our manipulation caused a change in our outcome\nBut this isn’t the case with our study, because we’re missing a lot of things (or at least we haven’t specified them yet). Including:\n\nControls\nRandomisation\nBlinding\nA theoretical framework"
  },
  {
    "objectID": "lectures/week04/slides/index.html#types-of-experimental-studies",
    "href": "lectures/week04/slides/index.html#types-of-experimental-studies",
    "title": "Psychology As A Science",
    "section": "Types of experimental studies",
    "text": "Types of experimental studies\nWe’ve already talked a bit about experimental designs, but experiments actually come in different types\n\nTrue experiments\nQuasi-experiments\nAnd natural experiments\n\nSometimes it’s not logistically, or ethically, possible to do a true experiment, so that’s where quasi-experiments and natural experiments come in handy"
  },
  {
    "objectID": "lectures/week04/slides/index.html#issues-in-measurement",
    "href": "lectures/week04/slides/index.html#issues-in-measurement",
    "title": "Psychology As A Science",
    "section": "Issues in measurement",
    "text": "Issues in measurement\nWhenever we’re trying to measure something there are some issues that we need to be aware of1\n\nConstruct validity\n\nIn psychology we measure lots of things that are difficult to observe directly\n\nThis includes things like happiness, cognitive ability, and aspects of personality\n\nWe try to measure these things using a range of tools including questionnaires, and experimental tasks\nWe design these tools using the theoretical underpinnings behind the constructs we’re trying to measure\nConstruct validity is the extent to which a tool can be justifiably trusted to actually measure the construct it is supposed to measure.\n\nWe touched on these last week, but we’ll talk about them in more detail here"
  },
  {
    "objectID": "lectures/week04/slides/index.html#levels-of-measurement",
    "href": "lectures/week04/slides/index.html#levels-of-measurement",
    "title": "Psychology As A Science",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nThe last couple of things we’ll cover in this lecture will be about the jargon we use to talk about the nature of the measurements we’re taking\n\nThe first set of terms describe the kind of information we’re working with\nWe call this the level of measurement\nThere are four levels of measurement\n\nNominal/categorical\nOrdinal\nInterval\nRatio\n\n\nSometimes a construct can fall into many of these levels, and it’s on the researcher to decide what measurement level is the most appropriate to use."
  },
  {
    "objectID": "lectures/week04/slides/index.html#levels-of-measurement-1",
    "href": "lectures/week04/slides/index.html#levels-of-measurement-1",
    "title": "Psychology As A Science",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nNominal/categorical\nRefers to names, categories, labels, or group membership.\n\nSome examples include:\n\nEye colour (e.g., green, brown, blue)\nOccupation status (e.g., FT employed, PT employed, unemployed, student…)\nStudy condition (control, experimental);\nEven age can be nominal if we wanted it to be (under 50s vs over 50s).\n\nCan’t compare the different groups in any quantifiable way\n\nE.g., It doesn’t make sense to say that green is more blue when it comes to eye colour."
  },
  {
    "objectID": "lectures/week04/slides/index.html#variable-data-types",
    "href": "lectures/week04/slides/index.html#variable-data-types",
    "title": "Psychology As A Science",
    "section": "Variable / data types",
    "text": "Variable / data types\nWhen we represent variables with numbers we can have different types depending on the type of data\nContinuous variables can contain any numerical value within a certain range\n\nE.g., time, height, and weight\n\nDiscrete variables can only contain some values\n\nE.g., The number of children (only whole numbers, because there’s not such thing as 2.5 children)\n\nBinary variables can only take one of two possible values (Special case of discrete variables )\n\nE.g., Head / Tails, Pass / Fail\n\nOur IVs and DVs can be any type (continuous, discrete, binary) or any level of measurement (nominal, ordinal, interval, ratio). It all depends on the study!"
  },
  {
    "objectID": "lectures/week05/slides/index.html#outline-for-today",
    "href": "lectures/week05/slides/index.html#outline-for-today",
    "title": "Psychology As A Science",
    "section": "Outline for today",
    "text": "Outline for today\nToday’s lecture aims to provide you with information about the lab report, and some of the motivations behind why the lab report is designed in the way it is.\nToday’s lecture is in two parts:\nPart I\n\nThe replication crisis, pre-registration, and reproducibility\n\nPart II\n\nThe lab report itself"
  },
  {
    "objectID": "lectures/week05/slides/index.html#some-terminology",
    "href": "lectures/week05/slides/index.html#some-terminology",
    "title": "Psychology As A Science",
    "section": "Some terminology",
    "text": "Some terminology\nReplication and Reproducibility? What’s the difference?\nReproducibility\nReproducibility refers to the idea of taking a dataset (which another researcher may have collected) and running the same analysis as that researcher and getting the same results.\nThis might sound like it’s trivial, but it turns out that it isn’t! One of the reasons you’re learning R and Quarto in this course is so that you can learn how to do reproducible science.\nReplicability\nReplicability refers to the idea of taking the methods (research design, stimuli, etc) from a previously run study, re-running the study, and getting the same results.\nThe lecture will mainly focus on replicability/replication, but I’ll also touch on reproducibility."
  },
  {
    "objectID": "lectures/week05/slides/index.html#a-spectre-is-haunting-psychology",
    "href": "lectures/week05/slides/index.html#a-spectre-is-haunting-psychology",
    "title": "Psychology As A Science",
    "section": "A spectre is haunting psychology…",
    "text": "A spectre is haunting psychology…\nThe spectre of failed replications.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeveral large-scale replication attempts have shown that many classic findings in the psychology literature can not be replicated\nSome estimates suggest that &gt; 50% of findings aren’t replicable\nThis has prompted some to claim that psychology is in a state of crisis!"
  },
  {
    "objectID": "lectures/week05/slides/index.html#what-is-the-cause-of-this-crisis",
    "href": "lectures/week05/slides/index.html#what-is-the-cause-of-this-crisis",
    "title": "Psychology As A Science",
    "section": "What is the cause of this crisis?",
    "text": "What is the cause of this crisis?\nThere are likely to be several causes of this crisis. These might include:\n\nHow statistics and statistical procedures are used and abused in psychology\nBias in which studies get published and which do not\nThe typical use of small sample sizes in psychology\nLack of clearly defined theories in psychological science\n\nThese causes probably aren’t independent but are likely to be interconnected and related to each other.\nWhen we designed the psychology methods courses at Sussex, many of these issues were at the forefront of our minds.\nIn this lecture, I’ll focus on the causes that are most relevant in motivating the design of the lab report."
  },
  {
    "objectID": "lectures/week05/slides/index.html#bias-in-publishing",
    "href": "lectures/week05/slides/index.html#bias-in-publishing",
    "title": "Psychology As A Science",
    "section": "Bias in publishing",
    "text": "Bias in publishing\n\n\nIf we look specifically at the psychology literature we’ll notice something odd\nThe vast majority of published papers in psychology journals report findings that support the tested hypothesis\nBut how is this possible?\n\nMaybe psychology researchers as psychic and they always test hypotheses that turn out to be true…\n\nMaybe the hypotheses they’re testing a trivial…\n\nMaybe there is some sort of bias in publishing…\n\nOr maybe they only report the results that support their hypothesis"
  },
  {
    "objectID": "lectures/week05/slides/index.html#bias-in-publishing-1",
    "href": "lectures/week05/slides/index.html#bias-in-publishing-1",
    "title": "Psychology As A Science",
    "section": "Bias in publishing",
    "text": "Bias in publishing\nOne source of bias in publishing of psychology studies is that journal editors and peer reviewers might not want to publish studies when they don’t support the tested hypotheses\n\nThis might especially be the case when new studies don’t show support for a famous or influential theory\nEditors/reviewers might be more likely to suspect there’s some kind of a problem with the new study\nResearchers might also choose not to submit studies for publication if they don’t support the tested hypothesis"
  },
  {
    "objectID": "lectures/week05/slides/index.html#bias-in-research-practices",
    "href": "lectures/week05/slides/index.html#bias-in-research-practices",
    "title": "Psychology As A Science",
    "section": "Bias in research practices",
    "text": "Bias in research practices\nIt is very easy for researchers to engage in certain practices that invalidate their results\nThese practices make it so that researchers are more likely to find results that support a tested theory even if that theory isn’t true\nSome examples include:\n\nRunning a statistical test, looking at the result, collecting more data, re-running the statistical test… rinse, repeat.. until you find the desired result\nCollecting data under many different conditions and only reporting the conditions that produce the desired result"
  },
  {
    "objectID": "lectures/week05/slides/index.html#combating-bias",
    "href": "lectures/week05/slides/index.html#combating-bias",
    "title": "Psychology As A Science",
    "section": "Combating bias",
    "text": "Combating bias\nBut if these are problems, then what is the solution?\nOne solution that has been proposed is pre-registration\nThe idea of pre-registration has been covered in popular media. For example, it’s been written about in The Guardian on several occasions (see the handout for more details)"
  },
  {
    "objectID": "lectures/week05/slides/index.html#pre-registration-and-combating-bias",
    "href": "lectures/week05/slides/index.html#pre-registration-and-combating-bias",
    "title": "Psychology As A Science",
    "section": "Pre-registration and combating bias",
    "text": "Pre-registration and combating bias\n\nPre-registration can get around publication bias by allowing editors and reviewers to judge whether a study is likely to produce reliable results before the results are known\nPre-registration can also get around certain kinds of experimenter and statistical biases by making researchers specify their statistical and study methods in advance"
  },
  {
    "objectID": "lectures/week05/slides/index.html#pre-registration-and-combating-bias-1",
    "href": "lectures/week05/slides/index.html#pre-registration-and-combating-bias-1",
    "title": "Psychology As A Science",
    "section": "Pre-registration and combating bias",
    "text": "Pre-registration and combating bias\nPreregistration means that before conducting a study, researchers plan their study in detail\n\nSpecifying the theory they plan to test and all of their hypotheses\n\n\nThis means they can’t change their hypothesis to make it fit whatever their data happened to show (think about falsification and infinitely flexible theories!)\nThey can’t cherry-pick their data or engage in subtle procedures to make the data fit their hypotheses"
  },
  {
    "objectID": "lectures/week05/slides/index.html#pre-registration-and-combating-bias-2",
    "href": "lectures/week05/slides/index.html#pre-registration-and-combating-bias-2",
    "title": "Psychology As A Science",
    "section": "Pre-registration and combating bias",
    "text": "Pre-registration and combating bias\n\nBy outlining their plans in detail, reviewers can judge\n\nWhether the methods are scientifically rigorous\nWhether the study is likely to produce clear (rather than ambiguous results)\n\n\nAnd they have to do this all before seeing the results, which might otherwise bias their decision\nIn a special form of pre-registration known as a registered report, a journal actually agrees to publish a study before the data are collected.\nThis is possible because the pre-registration plan gives enough detail for editors/ reviewers to judge whether the study is scientifically sound\n\n\nTo see how a registered report works in practice I’ll take you through an example from my our research…"
  },
  {
    "objectID": "lectures/week05/slides/index.html#pre-registration-in-action",
    "href": "lectures/week05/slides/index.html#pre-registration-in-action",
    "title": "Psychology As A Science",
    "section": "Pre-registration in action…",
    "text": "Pre-registration in action…\n\nIn 2003 a paper was published claiming to show that merely looking at numbers would cause a shift in attention to either the left or right side of space.\n\nThis finding was very influential with more than 700 subsequent studies citing this finding or building on it.\nSome published studies tried to replicate it. Most showed successful replications and very few failed replications."
  },
  {
    "objectID": "lectures/week05/slides/index.html#the-reproducibility-crisis",
    "href": "lectures/week05/slides/index.html#the-reproducibility-crisis",
    "title": "Psychology As A Science",
    "section": "The reproducibility crisis",
    "text": "The reproducibility crisis\nThis lecture is primarily about the replication crisis but there might also be a reproducibility crisis on the way!\nReproducibility is one of the reasons you’re learning about R, RStudio and Quarto in the practical sessions.\nWe can say a study is reproducible if:\n\nWe can take a dataset (from a published journal article) and re-run the analysis described in that journal article and get the same numbers\n\n\nThis seems like it should be fairly simple, but is it?\n\nIt’s difficult to test because researchers don’t typically share their data (so you can’t re-analyze it)\nBut data sharing is becoming more common, which means we might be able to test it!"
  },
  {
    "objectID": "lectures/week05/slides/index.html#the-lab-report",
    "href": "lectures/week05/slides/index.html#the-lab-report",
    "title": "Psychology As A Science",
    "section": "The lab report",
    "text": "The lab report\nThe lab report is designed to be part of your training to do better science by introducing you to the idea of pre-registration!\n\nThe lab report will present a research plan for an experiment\nThe expected length with be around 1000–1500 words (with a maximum allowable length of 2000 words)\n\nThe research plan will address one of two questions\n\nIs buying “green” (i.e., environmentally friendly) products driven by status motives?\nDo women find men more attractive in conjunction with the colour red?\n\nLinks to two studies that have addressed this question can be found on Canvas"
  },
  {
    "objectID": "lectures/week06/slides/index.html#todays-lecture",
    "href": "lectures/week06/slides/index.html#todays-lecture",
    "title": "Psychology As A Science",
    "section": "Today’s lecture",
    "text": "Today’s lecture\n\nSo far we’ve talked about quantitative methods in the abstract\nWe’ve said quantitative methods is all about putting numbers to things, but we haven’t talked about what to do with the numbers\n\nIn this lecture, and the one that follows we’re going to start talking about the techniques we can use for describing sets of measurements\nWe’ll use these tools when we start learning the basics of statistical models, and learn about the sampling distribution"
  },
  {
    "objectID": "lectures/week06/slides/index.html#measures-of-central-tendency",
    "href": "lectures/week06/slides/index.html#measures-of-central-tendency",
    "title": "Psychology As A Science",
    "section": "Measures of central tendency",
    "text": "Measures of central tendency\nWhen we have a set of measurements the first thing we might want to do is work out what the typical value is\n\n\n\n\n\nFigure 1: National average annual salary [source: worlddata.info]\n\n\n\nWhat we mean by typical value is not always clear\n\nWhat is the typical average income in the set of countries shown in Figure 1?\nLot’s of countries have an income below USD 30,000, but some have incomes higher than USD 100,000"
  },
  {
    "objectID": "lectures/week06/slides/index.html#measures-of-central-tendency-1",
    "href": "lectures/week06/slides/index.html#measures-of-central-tendency-1",
    "title": "Psychology As A Science",
    "section": "Measures of central tendency",
    "text": "Measures of central tendency\nMaybe we should pick the bracket with the most countries in it?\n\nThen the typical salary is between $0 and USD 10,000 per year.\n\nMaybe we should pick the value where half the countries have a lower average salary and half the countries have a higher one?\n\nThen the typical salary is $12,855 USD per year.\n\nDepending on how we define the most typical value, we get different answers.\nWe’ll cover the three main ways of defining the typical value or average\nTogether, these ways of describing the typical or average value are known as measures of central tendency."
  },
  {
    "objectID": "lectures/week06/slides/index.html#sample-means-and-population-means",
    "href": "lectures/week06/slides/index.html#sample-means-and-population-means",
    "title": "Psychology As A Science",
    "section": "Sample means and population means",
    "text": "Sample means and population means\n\nSo far we’ve just talked about describing the typical value in a set of measurements that we have—our sample\nBut we want to do with statistics is to make inferences about populations from the information that we get from samples\nIf you’re interested in the average height of people in the UK the “easy” way to find an answer to this question is to measure all the people in the UK and then work out the average height\n\nBut if you can’t measure everyone in the UK, then what do you do?\n\nYou could instead select a smaller group, or subset, of people from the UK. Measure the height of people in this group, and then try to use this information to figure out plausible values for the average height of people in the UK.\n\nIn this example, the group (or groups) you’re making claims about is the population, and the sample is a subset of this population"
  },
  {
    "objectID": "lectures/week06/slides/index.html#from-samples-to-populations",
    "href": "lectures/week06/slides/index.html#from-samples-to-populations",
    "title": "Psychology As A Science",
    "section": "From samples to populations",
    "text": "From samples to populations\nLet’s say we have defined our population as all people in the UK\nOur sample is a subset of this\n\nWe really want to know about the population. E.g., What is the average (mean) height of people in the UK?\nBut all we have is our sample. I.e., The average (mean) height of people in our sample.\n\nIf we want to go from our sample to the population then ideally our sample mean should resemble our population mean\nBut if real life situations we don’t know the population mean, so how would we know whether our sample mean resembles it?"
  },
  {
    "objectID": "lectures/week06/slides/index.html#looking-forward",
    "href": "lectures/week06/slides/index.html#looking-forward",
    "title": "Psychology As A Science",
    "section": "Looking forward",
    "text": "Looking forward\n\nSo far we’ve covered measures of central tendency for samples\nAnd we’ve covered the idea of the sample mean (\\(\\bar{x}\\)) and population mean (\\(\\mu\\))\nWe saw that although we don’t know whether a particular sample mean is the same as the population mean, we do know that on average they will be the same\nIn the coming lectures we’ll learn how to describe how spread out our sample is, and how spread out the population is\n\nWhen we plotted the individual sample means we saw that they were spread out around the population mean\nWe’ll finally put ideas about means and spread together to finally work out how to quantify this spread\nBut all that is for later…"
  },
  {
    "objectID": "lectures/week07/slides/index.html#outline-for-today",
    "href": "lectures/week07/slides/index.html#outline-for-today",
    "title": "Psychology As A Science",
    "section": "Outline for today",
    "text": "Outline for today\n\nMeasures of spread\n\nRange\nInterquartile range\nDeviation\nVariance\n\nSample Variance and Population Variance\n\nStandard Deviation\n\nThe relationship between samples and populations"
  },
  {
    "objectID": "lectures/week07/slides/index.html#measures-of-spread",
    "href": "lectures/week07/slides/index.html#measures-of-spread",
    "title": "Psychology As A Science",
    "section": "Measures of spread",
    "text": "Measures of spread\nLast week we started learning about the tools we can use to describe data. Specifically, we learned about the mean, mode, and median. And we learned about how they are different ways of describing the typical value.\nBut apart describing the typical value, we might also want a way to describe how spread out the data is around this value."
  },
  {
    "objectID": "lectures/week07/slides/index.html#measures-of-spread-1",
    "href": "lectures/week07/slides/index.html#measures-of-spread-1",
    "title": "Psychology As A Science",
    "section": "Measures of spread",
    "text": "Measures of spread\nIn Figure 1 you can see two sets of data. Both the datasets have a mean 0, but they have different amounts of spread.\n\nFigure 1: Histogram of two distributions with equal means but different spreads. N = 10,000 in each case."
  },
  {
    "objectID": "lectures/week07/slides/index.html#the-relationship-between-samples-and-populations",
    "href": "lectures/week07/slides/index.html#the-relationship-between-samples-and-populations",
    "title": "Psychology As A Science",
    "section": "The relationship between samples and populations",
    "text": "The relationship between samples and populations\nNow that we have tools for describing the centre / typical value of a set of measurements (mean) and the spread of a set of measurements (variance / standard deviation) we can these two ideas together.\n\nIn lecture 6 we saw that individual sample means were spread out around the population mean\nWe can quantify that spread using the idea of the standard deviation\n\n\nBut we’re now no longer calculating the spread of our sample or even the spread of the population\nWe’re now calculating the spread of sample means around the population mean\nThis kind of standard deviation has a special name. It’s called:\nThe standard error of the mean"
  },
  {
    "objectID": "lectures/week07/slides/index.html#the-standard-error-of-the-mean",
    "href": "lectures/week07/slides/index.html#the-standard-error-of-the-mean",
    "title": "Psychology As A Science",
    "section": "The standard error of the mean",
    "text": "The standard error of the mean\n\nThe standard error of the mean in technical terms is the standard deviation of the sampling distribution of the mean\nTo fully appreciate the concept of the standard error of the mean we’ll need to understand the concept of the sampling distribution\nAnd to understand the sampling distribution we’ll first need to understand what distributions are, what they look like, and why they look the way they do\n\nBut we get to that I want to return to the problem I left you with last week"
  },
  {
    "objectID": "lectures/week07/slides/index.html#the-standard-error-of-the-mean-1",
    "href": "lectures/week07/slides/index.html#the-standard-error-of-the-mean-1",
    "title": "Psychology As A Science",
    "section": "The standard error of the mean",
    "text": "The standard error of the mean\n\nLast week I got you to start thinking about the problem how you can know how close sample means will be to the population mean on average\nYou should now be able to recognise this question is about the standard deviation between sample means and the population mean\nOr more technically, it’s a question about the standard error of the mean\n\nWithout me telling you how to work out the standard error of the mean can you work out what it’s formula might be?\nTo do this, I want you to think of two scenarios where the standard error of the mean will be very small:\n\nOne of these scenarios has something to do with the nature of the samples you’re collecting\nThe other scenario has something to do with the nature of the population you’re sampling from\n\n\n\nmutable summary = null\n\n\n\n\n\n\n\nround2 = (v) =&gt; Math.round(v * 100) / 100\n\n\n\n\n\n\n\nimport { set } from \"@observablehq/synchronized-inputs\"\n\n\n\n\n\n\n\nimport { dist } from \"@ljcolling/wasm-distributions\"\n\n\n\n\n\n\n\nmaketable = (data) =&gt; {\n  let headers = Object.keys(data[0]).map((v) =&gt; html.fragment`&lt;td&gt;${v}&lt;/th&gt;`);\n  let body = data.map((r) =&gt; {\n    let this_row = Object.values(r).map((v) =&gt; html.fragment`&lt;td&gt;${v}&lt;/td&gt;`);\n    return html.fragment`&lt;tr&gt;${this_row}&lt;/tr&gt;`;\n  });\n  return htl.html`&lt;table class=\"table table-striped\"&gt;&lt;thead class=\"thead-dark\"&gt;&lt;tr&gt;${headers}&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;${body}&lt;/tbody&gt;&lt;/table&gt;`;\n};\n\n\n\n\n\n\n\nhtml = htl.html\n\n\n\n\n\n\n\nmeasures = [\n  { name: \"Range\", tag: 0, fun: drawrange },\n  { name: \"Interquartile range\", tag: 1, fun: drawiqr },\n  { name: \"None\", tag: 2, fun: () =&gt; {} }\n]\n\n\n\n\n\n\n\nfunction data_mean_median(data, drawlines, varname) {\n  let height = 400;\n  let xAxisOffest = 0;\n  let radius = 20;\n  var this_summary = {};\n  const pin = (v) =&gt; {\n    return v;\n  };\n  const update = () =&gt; {\n    let data = svg.selectAll(\"circle\").data();\n    let mean = d3.mean(data.map((v) =&gt; v.x));\n    let median = d3.median(data.map((v) =&gt; v.x));\n    drawlines(marker, data);\n    return data;\n  };\n\n  function dragstarted(event, d) {\n    d3.select(this).attr(\"stroke\", \"green\").attr(\"stroke-width\", 5);\n  }\n\n  function dragged(event, d) {\n    d3.select(this)\n      .raise()\n      .attr(\"cx\", d.x = Math.round(event.x))\n      .attr(\"cy\", d.y = Math.round(event.y));\n    data = update();\n\n    var this_summary = {};\n    this_summary[varname] = {\n      mean: d3.mean(data.map((v) =&gt; v.x)),\n      median: d3.median(data.map((v) =&gt; v.x)),\n      data: data.map((v) =&gt; v.x),\n    };\n    summaryupdate(this_summary);\n  }\n\n  function dragended(event, d) {\n    d3.select(this).attr(\"stroke\", null);\n    d3.select(this).attr(\"r\", radius);\n    let data = svg.selectAll(\"circle\").data();\n\n    var this_summary = {};\n    this_summary[varname] = {\n      mean: d3.mean(data.map((v) =&gt; v.x)),\n      median: d3.median(data.map((v) =&gt; v.x)),\n      data: data.map((v) =&gt; v.x),\n    };\n    summaryupdate(this_summary);\n  }\n\n  this_summary[varname] = {\n    mean: d3.mean(data.map((v) =&gt; v.x)),\n    median: d3.median(data.map((v) =&gt; v.x)),\n    data: data.map((v) =&gt; v.x),\n  };\n\n  summaryupdate(this_summary); \n  let svg = d3\n    .create(\"svg\")\n    .attr(\"viewBox\", [0, 0, width, height])\n    .attr(\"stroke-width\", 2);\n  let marker = svg.append(\"g\");\n  let points = svg.append(\"g\");\n\n  points\n    .selectAll(\"circle\")\n    .data(data)\n    .join(\"circle\")\n    .attr(\"cx\", (d) =&gt; d.x)\n    .attr(\"cy\", (d) =&gt; d.y)\n    .attr(\"r\", radius)\n    .on(\"mouseover\", function (d, i) {\n      d3.select(this)\n        .transition()\n        .duration(\"50\")\n        .attr(\"opacity\", \".5\")\n        .attr(\"stroke\", \"blue\");\n    })\n    .on(\"mouseout\", function (d, i) {\n      d3.select(this)\n        .transition()\n        .duration(\"50\")\n        .attr(\"opacity\", \"1\")\n        .attr(\"stroke\", \"black\");\n    })\n    .call(\n      d3\n        .drag()\n        .on(\"start\", dragstarted)\n        .on(\"drag\", dragged)\n        .on(\"end\", dragended),\n    );\n\n  marker.selectAll(\"#labelline\").remove();\n  drawlines(marker, data);\n\n  var scale = d3.scaleLinear().domain([0, width]).range([0, width]);\n  var x_axis = d3.axisBottom().scale(scale);\n\n  const clicked = (event, d) =&gt; {\n    if (event.defaultPrevented) return; \n    let x = Math.round(event.offsetX);\n    let y = Math.round(event.offsetY);\n    data.push({ x: x, y: y });\n    var this_summary = {};\n    this_summary[varname] = {\n      mean: d3.mean(data.map((v) =&gt; v.x)),\n      median: d3.median(data.map((v) =&gt; v.x)),\n      data: data.map((v) =&gt; v.x),\n    };\n\n    summaryupdate(this_summary); \n\n    svg\n      .selectAll(\"circle\")\n      .data(data)\n      .join(\"circle\")\n      .attr(\"cx\", (d) =&gt; d.x)\n      .attr(\"cy\", (d) =&gt; d.y)\n      .attr(\"r\", radius)\n      .on(\"mouseover\", function (d, i) {\n        d3.select(this)\n          .transition()\n          .duration(\"50\")\n          .attr(\"opacity\", \".5\")\n          .attr(\"stroke\", \"blue\");\n      })\n      .on(\"mouseout\", function (d, i) {\n        d3.select(this)\n          .transition()\n          .duration(\"50\")\n          .attr(\"opacity\", \"1\")\n          .attr(\"stroke\", \"black\");\n      })\n      .call(\n        d3\n          .drag()\n          .on(\"start\", dragstarted)\n          .on(\"drag\", dragged)\n          .on(\"end\", dragended),\n      );\n\n    update();\n  };\n\n  svg.on(\"click\", clicked);\n  svg\n    .append(\"g\")\n    .attr(\"transform\", \"translate(\" + xAxisOffest + \", \" + height * 0.9 + \")\")\n    .call(x_axis);\n\n  return svg.node();\n}\n\n\n\n\n\n\n\nfunction summaryupdate(this_summary) {\n    mutable summary = Object.assign({}, mutable summary, this_summary);\n}\n\n\n\n\n\n\n\ndrawlines = {\n  return (marker, data) =&gt; {\n    let height = 400;\n    marker.selectAll(\"#labelline\").remove();\n    // draw the mean line\n    /*\n    svg\n      .append(\"line\")\n      .attr(\"id\", \"labelline\")\n      .attr(\"x1\", d3.mean(data.map((v) =&gt; v.x)) || -10)\n      .attr(\"x2\", d3.mean(data.map((v) =&gt; v.x)) || -10)\n      .attr(\"y1\", 0)\n      .attr(\"y2\", height * 0.9)\n      .attr(\"stroke\", \"red\")\n      .attr(\"stroke-width\", 5);\n*/\n    // draw the deviation lines\n    marker\n      .selectAll(\"line\")\n      .data(data)\n      .join(\"line\")\n      .attr(\"id\", \"labelline\")\n      .attr(\"x1\", (d) =&gt; d.x)\n      .attr(\"x2\", d3.mean(data.map((v) =&gt; v.x)) || -10)\n      .attr(\"y1\", (d) =&gt; d.y)\n      .attr(\"y2\", (d) =&gt; d.y)\n      .attr(\"stroke\", \"red\")\n      .style(\"stroke-dasharray\", \"3, 3\")\n      .attr(\"stroke-width\", 2);\n    marker\n      .append(\"line\")\n      .attr(\"id\", \"labelline\")\n      .attr(\"x1\", d3.mean(data.map((v) =&gt; v.x)) || -10)\n      .attr(\"x2\", d3.mean(data.map((v) =&gt; v.x)) || -10)\n      .attr(\"y1\", 0)\n      .attr(\"y2\", height * 0.9)\n      .attr(\"stroke\", \"red\")\n      .attr(\"stroke-width\", 5);\n  };\n  //end of drawlines\n}\n\n\n\n\n\n\n\ndrawrange = {\n  return (marker, data) =&gt; {\n    let height = 400;\n    marker.selectAll(\"#labelline\").remove();\n\n    const max = d3.max(data.map((v) =&gt; v.x)) || -10;\n    const min = d3.min(data.map((v) =&gt; v.x)) || -10;\n    const width = max - min || 0;\n\n    marker\n      .append(\"line\")\n      .attr(\"id\", \"labelline\")\n      .attr(\"x1\", min)\n      .attr(\"x2\", min)\n      .attr(\"y1\", 0)\n      .attr(\"y2\", height * 0.9)\n      .attr(\"stroke\", \"red\")\n      .attr(\"stroke-width\", 5);\n\n    marker\n      .append(\"line\")\n      .attr(\"id\", \"labelline\")\n      .attr(\"x1\", max)\n      .attr(\"x2\", max)\n      .attr(\"y1\", 0)\n      .attr(\"y2\", height * 0.9)\n      .attr(\"stroke\", \"red\")\n      .attr(\"stroke-width\", 5);\n\n    marker\n      .append(\"rect\")\n      .attr(\"id\", \"labelline\")\n      .attr(\"x\", d3.min(data.map((v) =&gt; v.x)))\n      .attr(\"y\", 0)\n      .attr(\"width\", width)\n      .attr(\"height\", height * 0.9)\n      .attr(\"stroke\", \"red\")\n      .attr(\"fill\", \"red\")\n      .attr(\"stroke-width\", 5)\n      .attr(\"opacity\", 0.5);\n  };\n}\n\n\n\n\n\n\n\ncalc_iqr_limits = (d) =&gt; {\n  const data = d.map((v) =&gt; v.x);\n  const quantiles = [0, 0.25, 0.5, 0.75, 1].map((q) =&gt; d3.quantile(data, q));\n  const widths = quantiles.slice(0, -1).map((v, i) =&gt; {\n    return quantiles[i + 1] - v;\n  });\n\n  const marks = new Map();\n\n  widths.map((v, i) =&gt; {\n    const start = quantiles[i];\n    const width = v;\n    marks.set(\"q\" + (i + 1) + \"_s\", start);\n    marks.set(\"q\" + (i + 1) + \"_w\", width);\n  });\n\n  return marks;\n}\n\n\n\n\n\n\n\ndrawiqr = {\n  return (marker, data) =&gt; {\n    let height = 400;\n    marker.selectAll(\"#labelline\").remove();\n\n    const max = d3.max(data.map((v) =&gt; v.x)) || -10;\n    const min = d3.min(data.map((v) =&gt; v.x)) || -10;\n    const width = max - min || 0;\n\n    const quantiles = calc_iqr_limits(data);\n\n    for (let i = 1; i &lt; 5; i++) {\n      const start = quantiles.get(\"q\" + i + \"_s\");\n      const width = quantiles.get(\"q\" + i + \"_w\");\n\n      marker\n        .append(\"rect\")\n        .attr(\"id\", \"labelline\")\n        .attr(\"x\", start)\n        .attr(\"y\", 0)\n        .attr(\"width\", width)\n        .attr(\"height\", height * 0.9)\n        .attr(\"fill\", i === 2 || i === 3 ? \"red\" : \"blue\")\n        .attr(\"stroke-width\", 0)\n        .attr(\"opacity\", 0.5);\n\n      marker\n        .append(\"line\")\n        .attr(\"id\", \"labelline\")\n        .attr(\"x1\", start)\n        .attr(\"x2\", start)\n        .attr(\"y1\", 0)\n        .attr(\"y2\", height * 0.9)\n        .attr(\"stroke\", i === 1 ? \"blue\" : \"red\")\n        .attr(\"stroke-width\", 2);\n    }\n\n    marker\n      .append(\"line\")\n      .attr(\"id\", \"labelline\")\n      .attr(\"x1\", max || -10)\n      .attr(\"x2\", max || -10)\n      .attr(\"y1\", 0)\n      .attr(\"y2\", height * 0.9)\n      .attr(\"stroke\", \"blue\")\n      .attr(\"stroke-width\", 2);\n  };\n\n}\n\n\n\n\n\n\n\nraw_data = {\n  replay_variance_1\n  replay_variance_2\n  replay_variance_3\n  let sample_size = 50;\n  let population_mean = 100;\n  let sd = 15;\n  return dist.rand_normal(population_mean, sd, sample_size, 100);\n}\n\n\n\n\n\n\n\nraw_data_ave = {\n  replay_variance_1\n  replay_variance_2\n  replay_variance_3\n\n  let sample_size = 50;\n  let population_mean = 100;\n  let sd = 15;\n  return dist.rand_normal(population_mean, sd, sample_size, 10000);\n}"
  },
  {
    "objectID": "lectures/week08/slides/index.html#plan-for-today",
    "href": "lectures/week08/slides/index.html#plan-for-today",
    "title": "Psychology As A Science",
    "section": "Plan for today",
    "text": "Plan for today\nToday we’ll learn about the sampling distribution\nBut before we can do that we need to know what distributions are, where they come from, and how to describe them\n\nThe binomial distribution\nThe normal distribution\n\nProcesses that produce normal distributions\nProcess that don’t produce normal distributions\nDescribing normal distributions\nDescribing departures from the normal distributions\n\nDistributions and samples\n\nThe Central Limit Theorem\n\nThe Standard Error of the Mean"
  },
  {
    "objectID": "lectures/week08/slides/index.html#the-binomial-distributions",
    "href": "lectures/week08/slides/index.html#the-binomial-distributions",
    "title": "Psychology As A Science",
    "section": "The Binomial Distributions",
    "text": "The Binomial Distributions\n\nThe binomial distribution is one of the simplest distribution you’ll come across\nTo see where it comes from, we’ll just build one!\nWe can build one by flipping a coin (multiple times) and counting up the number of heads that we get"
  },
  {
    "objectID": "lectures/week08/slides/index.html#the-binomial-distribution",
    "href": "lectures/week08/slides/index.html#the-binomial-distribution",
    "title": "Psychology As A Science",
    "section": "The binomial distribution",
    "text": "The binomial distribution\n\n\n\nThe binomial distribution is just an idealised representation of the process that generates sequences of heads and tails when we flip a coin\n\nOr any other process that gives rise to binary data\n\nIt’s an idealisation but natural processes do give rise to binomial distribution\nIn the bean machine (Figure 3) balls fall from the top and bounce off pegs as they fall\n\nBalls can bounce one of two directions (left or right; binary outcome)\n\nMost of the balls collect near the middle, and fewer balls are found at the edges\n\n\n\n\n\n\n\n\nFigure 3: Example of the bean machine"
  },
  {
    "objectID": "lectures/week08/slides/index.html#the-normal-distribution",
    "href": "lectures/week08/slides/index.html#the-normal-distribution",
    "title": "Psychology As A Science",
    "section": "The normal distribution",
    "text": "The normal distribution\nFlipping coins might seem a long way off anything you might want to study in psychology, but the shape of the binomial distribution might be familiar to you\n\nThe binomial distribution has a shape that is similar to the normal distribution\n\nBut there are a few key differences:\n\nThe binomial distribution is bounded at 0 and n (number of coins)\n\nThe normal distribution can range from \\(+\\infty\\) to \\(-\\infty\\)\n\nThe binomial distribution is discrete (0, 1, 2, 3 etc, but no 2.5)\n\nThe normal distribution is continuous\n\n\nThe normal distribution is a mathematical abstraction, but we can use it as model of real-life populations that are produced by certain kinds of natural processes"
  },
  {
    "objectID": "lectures/week08/slides/index.html#distributions-and-samples",
    "href": "lectures/week08/slides/index.html#distributions-and-samples",
    "title": "Psychology As A Science",
    "section": "Distributions and samples",
    "text": "Distributions and samples\n\nWe’ve seen that whenever we look at the distribution of values where the values are produced by adding up numbers we got something that looked like a normal distribution\nIn Lecture 6, we saw that the formula for the sample mean was as shown in in Equation 1, below:\n\n\\[\\bar{x}={\\displaystyle\\sum^{N}_{i=1}{\\frac{x_i}{N}}} \\qquad(1)\\]\n\nSo to calculate a sample mean, we just add up a bunch of numbers\nLet’s say I take lots of samples from a population.\n\nAnd for each sample, I calculate the sample mean.\nIf we had to plot these sample means, then what would the distribution look like?"
  },
  {
    "objectID": "lectures/week08/slides/index.html#the-central-limit-theorem",
    "href": "lectures/week08/slides/index.html#the-central-limit-theorem",
    "title": "Psychology As A Science",
    "section": "The Central Limit Theorem",
    "text": "The Central Limit Theorem\nBefore we move on to how to calculate the standard error of the mean I want to assure you something\n\nYou might think that the sampling distribution of the mean in Figure 5 is normally distributed because the population is normally distributed\nBut this is not the case, as your sample size increases, then sampling distribution of the mean will be normally distributed\nAnd this will happen even if the population is not normally distributed"
  },
  {
    "objectID": "lectures/week08/slides/index.html#the-central-limit-theorem-1",
    "href": "lectures/week08/slides/index.html#the-central-limit-theorem-1",
    "title": "Psychology As A Science",
    "section": "The Central Limit Theorem",
    "text": "The Central Limit Theorem\n\n\n\nimport {viewof poptype} from \"@ljcolling/central-limit\"\nimport {viewof n} from \"@ljcolling/central-limit\"\nimport {pop_hist_plot} from \"@ljcolling/central-limit\"\nimport {sampling_distrubution} from \"@ljcolling/central-limit\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof poptype\n\n\n\n\n\n\n\npop_hist_plot\n\n\n\n\n\nFigure 6: Distribution of the population\n\n\n\n\n\nsampling_distrubution\n\n\n\n\n\nFigure 7: Sampling distribution of the mean (50, 000 samples)\n\n\n\n\nviewof n\n\n\n\n\n\n\n\n\n\nIf the sample size is large enough, then the sampling distribution of the mean will approach a normal distribution. This occurs even if the population isn’t normally distributed"
  },
  {
    "objectID": "lectures/week08/slides/index.html#the-standard-error-of-the-mean",
    "href": "lectures/week08/slides/index.html#the-standard-error-of-the-mean",
    "title": "Psychology As A Science",
    "section": "The standard error of the mean",
    "text": "The standard error of the mean\n\nIn Lecture 7 we started talking about the spread of sample means around the population\n\n\nFigure 8: (a) 10 samples with a standard deviation of 7.49 (b) 10 samples with a standard deviation of 11.3\nI showed you Figure 8 (above) where the average deviation of sample means from the population mean was either small (A) or large (B)"
  },
  {
    "objectID": "lectures/week08/slides/index.html#the-standard-error-of-the-mean-1",
    "href": "lectures/week08/slides/index.html#the-standard-error-of-the-mean-1",
    "title": "Psychology As A Science",
    "section": "The standard error of the mean",
    "text": "The standard error of the mean\nI asked you to image two scenarios: One that was a feature of the population and one that was a feature of the samples where the average deviation of sample means from the population mean would be small (or zero).\nIf you managed, then great! But if not, then here they are:\n\nIf the average (squared) deviation in the population is 0 then the average deviation of sample means from the population mean would be 0\n\nBecause all members of the population would be the same, so all samples would be the same, so all sample means would be the same\nConversely, if the average (squared) deviations in the population was larger, then the average deviations of sample means from the population mean would be larger"
  },
  {
    "objectID": "lectures/week08/slides/index.html#the-standard-error-of-the-mean-2",
    "href": "lectures/week08/slides/index.html#the-standard-error-of-the-mean-2",
    "title": "Psychology As A Science",
    "section": "The standard error of the mean",
    "text": "The standard error of the mean\n\nIf the sample size was large (so large to include the entire population) then the average deviation of sample means from the population mean would be 0\n\nBecause every sample would be identical to the population, so every sample mean would be identical to the population mean\nConversely, if the sample size was smaller, then the average deviations of sample means from the population mean would be larger\n\n\nLet’s put these two ideas together to try come up with a formula for the average (squared) deviations of the sample means from the population mean\nOur formula will include:\n\n\\(n\\): the sample size\n\\(\\sigma^2\\): the average (squared) deviations in the population (aka the variance of the population)\nAnd we’ll call our result \\(\\sigma_{\\bar{x}}^2\\)"
  },
  {
    "objectID": "lectures/week08/slides/index.html#the-standard-error-of-mean",
    "href": "lectures/week08/slides/index.html#the-standard-error-of-mean",
    "title": "Psychology As A Science",
    "section": "The standard error of mean",
    "text": "The standard error of mean\nThe only way to combine \\(n\\) and \\(\\sigma^2\\) so that:\n\nwhen \\(n\\) is very big \\(\\sigma_{\\bar{x}}^2\\) will be small (and vice versa) and\nwhen \\(\\sigma^2\\) is very small \\(\\sigma_{\\bar{x}}^2\\) will be small (and vice versa)\n\nis formula is Equation 2, below:\n\\[\\sigma_{\\bar{x}}^2=\\frac{\\sigma^2}{n} \\qquad(2)\\]\nBut remember, we don’t actually know the true \\(\\sigma^2\\) (the variance of the population), we only know \\(s^2\\) (the sample variance, which is out estimate of the variance in the population). So we’ll make a slight change to the formula as in Equation 3\n\\[s_{\\bar{x}}^2=\\frac{s^2}{n} \\qquad(3)\\]"
  },
  {
    "objectID": "lectures/week08/slides/index.html#the-standard-error-of-mean-1",
    "href": "lectures/week08/slides/index.html#the-standard-error-of-mean-1",
    "title": "Psychology As A Science",
    "section": "The standard error of mean",
    "text": "The standard error of mean\n\nThere’s one final step to get to the formula for the standard error of the mean.\nThe formula in Equation 3 is framed in terms of the average (squared) deviations) of sample means from the population mean—that is, in terms of variance.\nBut the standard error of the mean is the standard deviation of the sampling distribution\nThe standard deviation is just the square root of the variance, so we just need to take the square root of both sides of Equation 3, to get the equation in Equation 4, below:\n\n\\[s_{\\bar{x}}=\\frac{s}{\\sqrt{n}} \\qquad(4)\\]\nMore commonly, however, you’ll see \\(s_{\\bar{x}}\\) just written as \\(\\mathrm{SEM}\\) for Standard Error of the Mean\nAnd is the formula for the standard error of the mean and where it comes from"
  },
  {
    "objectID": "lectures/week08/slides/index.html#the-standard-error-of-mean-2",
    "href": "lectures/week08/slides/index.html#the-standard-error-of-mean-2",
    "title": "Psychology As A Science",
    "section": "The standard error of mean",
    "text": "The standard error of mean\nThis was, admittedly, a fairly long winded way to get to what is essentially a very simple formula\n\nHowever, as I have alluded to several times, the standard error of the mean is a fairly misunderstood concept\nI hope that getting there the long way has helped you to build a better intuition of what the standard error of the mean actually is\n\nI dislike talking about misconceptions because I think it can sometimes create them\nBut it worth talking about one prominent one\nMisconception\nThe SEM tells you how far away the sample mean is (likely) to be from the actual population mean\nBut it doesn’t tell you anything about the sample mean… at least not your sample mean that you have calculated for your particular sample"
  },
  {
    "objectID": "lectures/week08/slides/index.html#the-standard-error-of-mean-3",
    "href": "lectures/week08/slides/index.html#the-standard-error-of-mean-3",
    "title": "Psychology As A Science",
    "section": "The standard error of mean",
    "text": "The standard error of mean\nThe standard error of the mean is just what we’re defined it as:\nThe standard deviation of the sampling distribution\nSo what does this tell you?\n\nIt tells you how far on averages sample means (not your sample mean) will be from the population mean\nYour sample mean might be close to the population mean, it might be far away from the population mean. But the SEM doesn’t quantity this\n\nYour sample mean is either close or it is far from population mean\n\nThe SEM tells you something about the consequences of a sampling process\nNot something about your sample\n\nSo why is it even useful? More on that next week!"
  },
  {
    "objectID": "lectures/week09/slides/index.html#outline-for-today",
    "href": "lectures/week09/slides/index.html#outline-for-today",
    "title": "Psychology As A Science",
    "section": "Outline for today",
    "text": "Outline for today\n\nThe standard normal distribution\nTransformations\n\nCentering\nScaling\nThe z-transform\n\nMaking comparisons\n\nComparing groups\nComparing across groups\nMaking comparisons with the sampling distribution"
  },
  {
    "objectID": "lectures/week09/slides/index.html#the-shape-of-things",
    "href": "lectures/week09/slides/index.html#the-shape-of-things",
    "title": "Psychology As A Science",
    "section": "The shape of things",
    "text": "The shape of things\n\nIf we measured the height of 1000 women and plotted the values then we might get something like Figure 1.\nMost heights are in the 155–175 centimetre range.\nThe distribution is roughly symmetrical around its mean (165 cm) and it has a shape characteristic of a normal distribution.\n\n\nFigure 1: Distribution of heights in a sample of 1000 women. Not real data."
  },
  {
    "objectID": "lectures/week09/slides/index.html#the-shape-of-things-1",
    "href": "lectures/week09/slides/index.html#the-shape-of-things-1",
    "title": "Psychology As A Science",
    "section": "The shape of things",
    "text": "The shape of things\n\n\n\nOf course the plot in Figure 1 doesn’t look exactly like a normal distribution\nBut if we measured more and more people (e.g., 100, 000 people) then we might get something like Figure 2\nFigure 2 also shows the corresponding normal distribution with a mean of 165 and a standard deviation of 10\nAlthough the normal distribution is an idealisation, or an abstraction, we can use it to do some very useful things\n\n\n\n\n\n\n\nFigure 2: Distribution of heights in a sample of 100,000 women (Not real data) and the corresponding normal distribution"
  },
  {
    "objectID": "lectures/week09/slides/index.html#the-standard-normal-distribution",
    "href": "lectures/week09/slides/index.html#the-standard-normal-distribution",
    "title": "Psychology As A Science",
    "section": "The standard normal distribution",
    "text": "The standard normal distribution\n\nIn lecture 8, I said that two parameters, \\(\\mu\\) and \\(\\sigma\\) changed where the normal distribution was centred and how spread out it was\nWhen \\(\\mu = 0\\) and \\(\\sigma = 1\\), then this distribution is called the standard normal distribution\nI said that changing these values didn’t change the relative position of points on the plot. The overall shape remains the same\nAll normal distributions have the same overall shape as the standard normal distribution even if they’re centered in a different place and are more or less spread out\n\nTo see what I mean by this, we’ll take out heights of 1000 people, but instead of displaying them in centimetres we’ll display them in metres"
  },
  {
    "objectID": "lectures/week09/slides/index.html#transformations",
    "href": "lectures/week09/slides/index.html#transformations",
    "title": "Psychology As A Science",
    "section": "Transformations",
    "text": "Transformations\n\nIn Figure 3 and Figure 4 we saw that we could transform a variable so that it had a new location (mean) or scale (standard deviation) without changing the shape\nThese two kinds of transformations are known as centering and **scaling*\n\nCentering\n\nTo centre a set of measurements, you subtract a fixed value from each observation in the dataset.\nThis has the effect of shifting the distribution of the variable along the x-axis\nYou can technically centre a variable by subtracting any value from it but the most frequently used method is mean-centring\n\nThis is shown in Equation 1, below:\n\\[x_i - \\bar{x} \\qquad(1)\\]"
  },
  {
    "objectID": "lectures/week09/slides/index.html#comparing-groups",
    "href": "lectures/week09/slides/index.html#comparing-groups",
    "title": "Psychology As A Science",
    "section": "Comparing groups",
    "text": "Comparing groups\n\n\nIn the context of quantitative research we’re often looking at the average difference in a variable between groups\nIn the Figure 5 we can see measurements from a reaction time task.\n\nAmateurs sportspeople have a mean reaction time of 500 ms and professionals have a mean reaction time of 460 ms.\nThere is overlap between the two groups, but there is a difference between the averages\nTo quantify the difference, just subtract the mean of one group from the mean of the other1\n\n\n\n\n\n\n\nFigure 5: Distribution of reaction times in a sample of amateur (green) and 500 professional (blue) sportspeople. Group means are indicated with the vertical lines.\n\n\n\n\n\nThe mean difference is just 500ms - 460ms = 40ms.\n\n\n\nIn later years you’ll learn how to quantify differences like this in standardised units—that is, units of standard deviation"
  },
  {
    "objectID": "lectures/week09/slides/index.html#comparing-across-groups",
    "href": "lectures/week09/slides/index.html#comparing-across-groups",
    "title": "Psychology As A Science",
    "section": "Comparing across groups",
    "text": "Comparing across groups\n\nIn the previous example the comparisons were easy because the measurements were on the same scale (milliseconds)\nBut let’s say that you want to compare two children on a puzzle completion task\n\nOne child is 8 years old, and the other is 14 years old\nThey do slightly different versions of the task and the tasks are score differently\n\nBecause we have two different tests that might have a different number of items etc we can’t just compare the raw numbers to see which is bigger\n\nLet’s look at an example…"
  },
  {
    "objectID": "lectures/week09/slides/index.html#making-comparisons-with-the-sampling-distribution",
    "href": "lectures/week09/slides/index.html#making-comparisons-with-the-sampling-distribution",
    "title": "Psychology As A Science",
    "section": "Making comparisons with the sampling distribution",
    "text": "Making comparisons with the sampling distribution\nThe final comparison we’ll talk about is comparisons against the sampling distribution\n\nFrom last week we learned that the sampling distribution of the mean will be\n\nCentred at the population mean\nHave a standard deviation equal to the standard error of the mean\n\nBut remember, we don’t know the value of the population mean, so we won’t actually know what the sampling distribution looks like\n\nAlthough we don’t know the value of the population mean we can generate a hypothesis about what we think the population mean might be…\nWe can then generate a hypothetical sampling distribution based on our hypothesised value of the population mean"
  },
  {
    "objectID": "lectures/week10/slides/index.html#plan-for-today",
    "href": "lectures/week10/slides/index.html#plan-for-today",
    "title": "Psychology As A Science",
    "section": "Plan for today",
    "text": "Plan for today\nWhy use tables and plots?\nTables\n\nWhat makes a good table?\nFrequency tables\nSummary tables\n\nPlotting basics\n\nWhat makes a good plot?\nStructure of a plot\nFrequency plots\nSummary plots\n\n\nAll plots and tables in the slides made in R"
  },
  {
    "objectID": "lectures/week10/slides/index.html#the-data",
    "href": "lectures/week10/slides/index.html#the-data",
    "title": "Psychology As A Science",
    "section": "The Data",
    "text": "The Data\n\n4,468 films pulled from IMDB.com\nInclusion criteria\n\nRated at least 5,000 times\nReleased in 2010 or later\n\nData available here\n\n Of the 4,468 titles, 52 (1.16%) were produced, or co-produced by at least one African country, 3,009 (67.35%) by at least one country in the Americas, 1,058 (23.68%) by a country in Asia, 1,604 (35.9%) in Europe, and 153 (3.42%) in Oceania. The sum of these numbers is necessarily higher than the total number of titles in the data set, as one title can have multiple production attributions and so can count towards several “Continent” categories.\nThe number of IMDB ratings ranged from 5,003 to 2.03×106, with a mean of 6.31×104 and SD of 1.23×105. The average user rating for a given title spanned the range 1–9.2; M=6.37, SD=1.02.\nInformation on estimated budget was only available for 2,170 of the titles in the data set: 18–3.56×108, M=3.96×107, SD=5.09×107."
  },
  {
    "objectID": "lectures/week10/slides/index.html#why-and-when-to-use-tables-and-plots",
    "href": "lectures/week10/slides/index.html#why-and-when-to-use-tables-and-plots",
    "title": "Psychology As A Science",
    "section": "Why and when to use tables and plots?",
    "text": "Why and when to use tables and plots?\nWhy\n\nPlots and tables allow us to convey a lot of information using relatively small amounts of space\nThey structure the information we’re communicating so that it’s easier to understand than a wall of text\nGood tables and plots are simply #aesthetic\n\nWhen\n\nTables and plot are not just for reports\nThey are a good way of exploring data before analysis in order for us to get to know them\nNot all plots and table we create should be put in reports/papers\nIf we are including them in reports/papers, they should be used to convey important information that would be cumbersome to convey in body text"
  },
  {
    "objectID": "lectures/week10/slides/index.html#tables",
    "href": "lectures/week10/slides/index.html#tables",
    "title": "Psychology As A Science",
    "section": "Tables",
    "text": "Tables\n\nTidy way of presenting a lot of numbers\nA good table should be easy to read, well-organised, and clear\nGood for exploring and summarising data (this lecture), and presenting results (future modules)\n\nStructural elements\n\nNumber: all tables should be numbered and the number should be referenced in paper/report\nTitle: should be descriptive\nHeader: clearly indicates what the data in each column mean\nBody: logically organised into rows and columns\nNote: optional, provides additional information necessary to correctly interpret data in the table"
  },
  {
    "objectID": "lectures/week10/slides/index.html#plots",
    "href": "lectures/week10/slides/index.html#plots",
    "title": "Psychology As A Science",
    "section": "Plots",
    "text": "Plots\n\nSometimes, a picture is worth a thousand words\nGreat for communicating information about data that takes a lot of space to explain in writing\nGood graphics should be both clear and packed full of information\n\nStructural elements\n\nNumber: just like tables, all plots should be numbered and the number should be referenced\nTitle: should be descriptive\nAxes: clearly labelled, with sensible ticks along them, and units of measurement\nGraphics: clear, well designed, good size\nLegend: if graphical elements are used to distinguish levels of variables, legend must be provided\nNote: optional, provides additional information necessary to correctly interpret the plot"
  },
  {
    "objectID": "lectures/week11/slides/index.html#probability",
    "href": "lectures/week11/slides/index.html#probability",
    "title": "Psychology As A Science",
    "section": "Probability",
    "text": "Probability\nWhat do we mean by “probability”?\nIt might seem like there’s an easy answer to this question, but there’s at least three senses of probability.\nThese different senses are often employed in different contexts, because some make more sense in some contexts relative to others\nThe three I’ll cover are:\n\nThe classical view of probability\nThe frequency view of probability\nThe subjective view of probability"
  },
  {
    "objectID": "lectures/week11/slides/index.html#calculating-with-probability",
    "href": "lectures/week11/slides/index.html#calculating-with-probability",
    "title": "Psychology As A Science",
    "section": "Calculating with probability",
    "text": "Calculating with probability\nThe different views of probability have got to do with what the numbers mean, but once we have the numbers there’s no real disagreements about how we do calculations with those numbers 1\nSome properties of probabilities will help us to do calculations\nWhen we attach numbers to probabilities those numbers must range from 0 to 1\n\nIf an event has probability 0 then it is impossible\nIf an event has probability 1 then it is guaranteed\n\nThese two simple rules can help us to check our calculations with probabilities. If we get a value more than 1 or a value less than 0, then something has gone wrong!\nProbabilities don’t always have to have numbers attached. There is a sense in which something can be more probable than something else without numbers being attached."
  },
  {
    "objectID": "lectures/week11/slides/index.html#independence-and-non-independence",
    "href": "lectures/week11/slides/index.html#independence-and-non-independence",
    "title": "Psychology As A Science",
    "section": "Independence and non-independence",
    "text": "Independence and non-independence\nIn the previous example, the two choices were independent\n\nThis means that knowing whether you got Heads/Tails on the first flip didn’t impact how you calculated the probability of getting Heads/Tails on the second flip\nWe can calculate the probability of each event without considering anything about the other event\n\nBut sometimes this isn’t the case… sometimes knowing what happened on the first event changes how to calculate the probability of for the second event\nLet us look at a simple example…"
  },
  {
    "objectID": "lectures/week11/slides/index.html#working-with-conditional-probabilities",
    "href": "lectures/week11/slides/index.html#working-with-conditional-probabilities",
    "title": "Psychology As A Science",
    "section": "Working with conditional probabilities",
    "text": "Working with conditional probabilities\n\nWe often encounter conditional probabilities in every day life where we use some bit of information to help us work out the probability of something.\nHowever, reasoning about conditional probabilities can be difficult and as a result people make a lot of mistakes when dealing with them.\n\nThe most common mistake that you’ll encounter is the confusion being P(A|B) and P(B|A).\nOr as in the dice example:\n\nP(Roll 20 | D20), which is 1/20\nP(D20 | Roll 20), which is 1"
  },
  {
    "objectID": "lectures/week11/slides/index.html#working-with-conditional-probabilities-1",
    "href": "lectures/week11/slides/index.html#working-with-conditional-probabilities-1",
    "title": "Psychology As A Science",
    "section": "Working with conditional probabilities",
    "text": "Working with conditional probabilities\nThe other common mistake is confusing the conditional probabilities for the unconditional probabilities\nThat is, confusing, for example, P(A|B) and P(A)\nOr in the dice example:\n\nP(D20 | Roll 20), which is 1\nP(D20) which is 0.5\n\nThere is a mathematically formula that relates all these quantities together.\nThis is known as Bayes theorem\n\nBayes theorem allows us to update our probability calculations when we find out new information\n\nFor example, we can update our calculation for rolling a 20 when we find out that we selected a D-20"
  },
  {
    "objectID": "lectures/week11/slides/index.html#final-thoughts",
    "href": "lectures/week11/slides/index.html#final-thoughts",
    "title": "Psychology As A Science",
    "section": "Final thoughts",
    "text": "Final thoughts\nI hope this serves as a sobering message for just how important research methods (including probability theory) is in your training\n\nYou might one day be in the position to make policies for governments so I hope you don’t fall victim to faulty reasoning when you do!\nAnd that you know how to access and interpret research correctly"
  },
  {
    "objectID": "lectures/week11/slides/index.html#the-exam",
    "href": "lectures/week11/slides/index.html#the-exam",
    "title": "Psychology As A Science",
    "section": "The exam",
    "text": "The exam\nThe final assessment of this course is the final exam\nThe final exam is worth 50% of your grade and it covers the material from the 12 weekly lectures only\n\nDoesn’t include material on R and RStudio\nDoesn’t include material from the ethics lecture\nThe exam will be online (This is specific to PAAS. Your other exams are probably in person)\n\nFormat\n\nMostly multi-choice, with a few questions where you have to enter in numbers\nSome of the numeric questions will just involve finding the correct number in a table, but some will involve calculating a number\nA sample exam will be made available on Canvas in the next couple of days\n\n\n\nfunction circle(x, y, fill, stroke) {\n  return `&lt;circle cx=\"${x}\" cy=\"${y}\" r=\"20\" stroke=\"none\" fill=\"${fill}\" stroke-width=\"0\"&gt;&lt;/circle&gt;\n&lt;circle cx=\"${x}\" cy=\"${y}\" r=\"10\" stroke=\"${fill}\" fill=\"${stroke}\" stroke-width=\"0\"&gt;&lt;/circle&gt;`;\n}\n\n\n\n\n\n\n\nfull_grid = _.flatten(\n  _.range(10).map((y) =&gt;\n    _.range(10)\n      .map((x) =&gt; 44 * (x + 1))\n      .map((x) =&gt; [x, 44 * (y + 1)])\n  )\n)\n\n\n\n\n\n\n\nfunction round3(x) {\n  return Math.round(x * 1000) / 1000;\n}\n\n\n\n\n\n\n\nfunction getvalue(x) {\n  if (x === \"red\") {\n    return red;\n  }\n\n  if (x === \"blue\") {\n    return blue;\n  }\n\n  if (x === \"green\") {\n    return green;\n  }\n}\n\n\n\n\n\n\n\nviewof blue2 = Inputs.input(10)\n\n\n\n\n\n\n\nviewof red2 = Inputs.input(10)\n\n\n\n\n\n\n\nviewof blue2_with = Inputs.input(5)\n\n\n\n\n\n\n\nviewof red2_with = Inputs.input(5)\n\n\n\n\n\n\n\nfunction frac(decimal, den) {\n  const n = Math.round(decimal * den);\n\n  return md`${tex`\\frac{${n}}{${den}}`}`;\n}\n\n\n\n\n\n\n\nimport { texmd } from \"@kelleyvanevert/katex-within-markdown\"\n\n\n\n\n\n\n\nparsedSpec = {\n  return vega.parse(spec3);\n}\n\n\n\n\n\n\n\nspec3 = {\n  return {\n    $schema: \"https://vega.github.io/schema/vega/v5.0.json\",\n    padding: 0,\n    width: 500,\n    height: 100,\n    layout: {\n      padding: 0,\n      columns: 1\n    },\n    marks: [\n      {\n        type: \"group\",\n        encode: {\n          update: {\n            width: {\n              value: 1000\n            },\n            height: {\n              value: 130\n            }\n          }\n        },\n        data: [\n          {\n            name: \"tree\",\n            values: coin_data,\n            transform: [\n              {\n                type: \"stratify\",\n                key: \"id\",\n                parentKey: \"parent\"\n              },\n              {\n                type: \"tree\",\n                method: \"tidy\",\n                size: [500, 200],\n                as: [\"x\", \"y\", \"depth\", \"children\"]\n              }\n            ]\n          },\n          {\n            name: \"links\",\n            source: \"tree\",\n            transform: [\n              {\n                type: \"treelinks\",\n                key: \"id\"\n              },\n              {\n                type: \"linkpath\",\n                orient: \"horizontal\",\n                shape: \"line\"\n              }\n            ]\n          }\n        ],\n        scales: [\n          {\n            name: \"color\",\n            domain: [0, 1, 2, 3, 4, 5],\n            type: \"sequential\",\n            range: \"ramp\"\n          }\n        ],\n        marks: [\n          {\n            type: \"path\",\n            from: {\n              data: \"links\"\n            },\n            encode: {\n              update: {\n                path: {\n                  field: \"path\"\n                },\n                stroke: {\n                  value: \"black\"\n                }\n              }\n            }\n          },\n          {\n            type: \"symbol\",\n            from: {\n              data: \"tree\"\n            },\n            encode: {\n              enter: {\n                size: {\n                  value: 50\n                },\n                stroke: {\n                  value: \"black\"\n                }\n              },\n              update: {\n                x: {\n                  field: \"x\"\n                },\n                y: {\n                  field: \"y\"\n                },\n                fill: {\n                  field: \"color\"\n                }\n              }\n            }\n          },\n          {\n            type: \"text\",\n            from: {\n              data: \"tree\"\n            },\n            encode: {\n              enter: {\n                text: {\n                  field: \"name\"\n                },\n                fontSize: {\n                  value: 0\n                },\n                baseline: {\n                  value: \"bottom\"\n                }\n              },\n              update: {\n                x: {\n                  field: \"x\"\n                },\n                y: {\n                  field: \"y\"\n                }\n              }\n            }\n          }\n        ]\n      }\n    ]\n  };\n}\n\n\n\n\n\n\n\nvega = require(\"https://cdn.jsdelivr.net/npm/vega@5/build/vega.js\")\n\n\n\n\n\n\n\ncoin_data = [\n  { name: \"START\", id: 1, parent: \"\", color: \"red\" },\n  ...d3.range(2, 2 ** coins).map((i) =&gt; {\n    return {\n      name: i % 2 ? \"T\" : \"H\",\n      id: i,\n      parent: Math.floor(i / 2)\n    };\n  })\n].map((x) =&gt; {\n  let colors = { H: \"black\", T: \"white\", START: \"red\" };\n  x.color = colors[x.name];\n  return x;\n})\n\n\n\n\n\n\n\nfunctions = {\n  return {\n    circle: (x, y, fill, stroke) =&gt; {\n      return `&lt;circle cx=\"${x}\" cy=\"${y}\" r=\"14\" stroke=\"none\" fill=\"${fill}\" stroke-width=\"0\"&gt;&lt;/circle&gt;\n&lt;circle cx=\"${x}\" cy=\"${y}\" r=\"5\" stroke=\"${fill}\" fill=\"${stroke}\" stroke-width=\"0\"&gt;&lt;/circle&gt;`;\n    },\n    full_grid: _.flatten(\n      _.range(10).map((y) =&gt;\n        _.range(10)\n          .map((x) =&gt; 30 * (x + 1))\n          .map((x) =&gt; [x, 30 * (y + 1)])\n      )\n    ),\n    round3: (x) =&gt; Math.round(x * 1000) / 1000,\n    frac: (decimal, den) =&gt; {\n      const n = Math.round(decimal * den);\n      return md`${tex`\\frac{${n}}{${den}}`}`;\n    }\n  };\n}\n\n\n\n\n\n\n\ndata = {\n  let proportions = incidence === \"rare\" ? [5, 95] : [80, 20];\n  let percentages = incidence === \"rare\" ? [4 / 5, 1 / 20] : [8 / 10, 6 / 90];\n  let g = [\n    Math.round(proportions[0] * (1 - percentages[0])),\n    Math.round(proportions[0] * percentages[0]),\n    Math.round(proportions[1] * percentages[1]),\n    Math.round(proportions[1] * (1 - percentages[1]))\n  ];\n\n  let all = [\n    [\"red\", \"red\"],\n    [\"red\", \"black\"],\n    [\"green\", \"black\"],\n    [\"green\", \"green\"]\n  ];\n\n  let pos = [\n    [\"white\", \"white\"],\n    [\"red\", \"black\"],\n    [\"green\", \"black\"],\n    [\"white\", \"white\"]\n  ];\n\n  let sick = [\n    [\"red\", \"red\"],\n    [\"red\", \"black\"],\n    [\"white\", \"white\"],\n    [\"white\", \"white\"]\n  ]\n\n  let health = [\n    [\"white\", \"white\"],\n    [\"white\", \"white\"],\n    [\"green\", \"black\"],\n    [\"green\", \"green\"]\n  ];\n\n\n  let colors = {\"all\" : all, \"sick\": sick, \"pos\": pos, \"health\": health}\n\n  let conds = {\n    red_dot: g[1] / (g[1] + g[2]),\n    dot_red: g[1] / (g[1] + g[0]),\n    red: (g[0] + g[1]) / _.sum(g),\n    dot: (g[1] + g[2]) / _.sum(g),\n    green: 1 - (g[0] + g[1]) / _.sum(g),\n    dot_green: g[2] / (g[3] + g[2])\n  };\n\n  return {\n    proportions: proportions,\n    percentages: percentages,\n    g: g,\n    conds: conds,\n    colours: colors[positive_tests]\n  };\n}"
  },
  {
    "objectID": "lectures/week01/slides/landing.html",
    "href": "lectures/week01/slides/landing.html",
    "title": "Introduction to Psychology as a Science",
    "section": "",
    "text": "Open slides a new window"
  },
  {
    "objectID": "lectures/week02/slides/landing.html",
    "href": "lectures/week02/slides/landing.html",
    "title": "What is this thing called \"Science\"?",
    "section": "",
    "text": "Open slides a new window"
  },
  {
    "objectID": "lectures/week03/slides/landing.html",
    "href": "lectures/week03/slides/landing.html",
    "title": "Approaches to Research",
    "section": "",
    "text": "Open slides a new window"
  },
  {
    "objectID": "lectures/week04/slides/landing.html",
    "href": "lectures/week04/slides/landing.html",
    "title": "Introduction to study design",
    "section": "",
    "text": "Open slides a new window"
  },
  {
    "objectID": "lectures/week05/slides/landing.html",
    "href": "lectures/week05/slides/landing.html",
    "title": "Open Science",
    "section": "",
    "text": "Open slides a new window"
  },
  {
    "objectID": "lectures/week06/slides/landing.html",
    "href": "lectures/week06/slides/landing.html",
    "title": "Describing measurements I",
    "section": "",
    "text": "Open slides a new window"
  },
  {
    "objectID": "lectures/week07/slides/landing.html",
    "href": "lectures/week07/slides/landing.html",
    "title": "Describing measurements II",
    "section": "",
    "text": "Open slides a new window"
  },
  {
    "objectID": "lectures/week08/slides/landing.html",
    "href": "lectures/week08/slides/landing.html",
    "title": "Distributions",
    "section": "",
    "text": "Open slides a new window"
  },
  {
    "objectID": "lectures/week09/slides/landing.html",
    "href": "lectures/week09/slides/landing.html",
    "title": "Transformation and comparisons",
    "section": "",
    "text": "Open slides a new window"
  },
  {
    "objectID": "lectures/week10/slides/landing.html",
    "href": "lectures/week10/slides/landing.html",
    "title": "Visual summaries of data",
    "section": "",
    "text": "Open slides a new window"
  },
  {
    "objectID": "lectures/week11/slides/landing.html",
    "href": "lectures/week11/slides/landing.html",
    "title": "Introduction to probability",
    "section": "",
    "text": "Open slides a new window"
  }
]