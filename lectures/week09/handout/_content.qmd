In last week's lecture we started learning a little bit about distributions.
We learned about the normal distribution, and where it comes from. And we
also learned a little bit about the sampling distribution, and why knowing
the sampling distribution might be useful.

This week we'll talk a little bit more about distributions and why the normal
distribution is particularly useful.

## The shape of things

```{r}
#| echo: false
#| message: false
require(ggplot2)
require(cowplot)
set.seed(124)
df <- tibble::tibble(x = rnorm(n = 1000, mean = 0, sd = 1) |>
scale() |> as.numeric())
df_og <- df
df$x <- df$x * 10
df$x <- df$x + 165
df_m <- df$x |> mean()
df_s <- df$x |> sd()
range <- glue::glue("{round(df_m - df_s  * 1)}--{round(df_m + df_s * 1)}")
p <- ggplot(df, aes(x = x)) +
  geom_histogram(binwidth = 5, boundary = 130) +
  labs(x = "height in cm") +
  theme_cowplot() +
  NULL
```

If we were to measure the height of `r df$x |> length()` women and
plot the values then we might get something like the plot in
@fig-height-histogram. As you can see, the vast majority of the measured
heights are in the `r range` centimetre range. Only a small number of people
fall outside of this range. You can also see that the distribution is roughly
symmetrical around its mean (`r round(df_m)` cm) and it has a shape
characteristic of a normal distribution.


```{r}
#| echo: false
#| label: fig-height-histogram
#| fig.cap: |
#|    Distribution of heights in a sample of 1000 women. Not real
#|    data.
p
```

Of course it doesn't look exactly like a normal distribution, because, as we 
saw in Lecture 8, a normal distribution is smooth line. Our plot is a histogram
where we've just counted up the number of people that fall into each 5 centimetre 
bin. However, we could image measuring the heights of more and more people and 
making the bins narrower and narrower. In @fig-height-histogram2 we can 
see what the histogram might look like if we were to measure 100,000 women.

```{r}
#| echo: false
#| label: fig-height-histogram2
#| fig.cap: |
#|    Distribution of heights in a sample of 100,000 women (Not real
#|    data) and the corresponding normal distribution
df <- tibble::tibble(x = rnorm(n = 100000, mean = 0, sd = 1) |>
scale() |> as.numeric())
df$x <- df$x * 10
df$x <- df$x + 165
df_m <- df$x |> mean()
df_s <- df$x |> sd()
ideal <- \(x) (dnorm(x, df_m, df_s) / dnorm(df_m, df_m, df_s)) * ((195 / 5) * 100)
ggplot(df, aes(x)) +
  geom_histogram(binwidth = 1, boundary = 130) +
  geom_density(col = "grey20", size = 2, aes(y = ..count.. )) +
  geom_function(fun = ideal, col = "blue", size = 2) +
  labs(y = "count", x = "height in cm") +
  theme_cowplot()
```

In @fig-height-histogram2 you can also see the what the corresponding normal
distribution looks like. This idealised representation is a normal distribution
with a mean of `r df_m` and a standard deviation of `r df_s`. Although the
**normal distribution** is an idealisation, or an abstraction, we can use it to
do some very useful things.

## The standard normal distribution

When we were first introduced to the normal distribution last week we saw that
there were two **parameters** that we could change ($\mu$ and $\sigma$) that
changed where the normal distribution was centered and how spread out it was.
When $\mu=0$ and $\sigma=1$, then the normal distribution is called the
**standard normal distribution**. You can explore the normal distribution again
in @exm-normal.

I said in Lecture 8 that when you adjust the $\mu$ and $\sigma$ values then the
absolute positions of points on the plot change, but the **relative position**
doesn't change. 

To understand what I mean by this, we'll use an example. Let's take the
heights of people that we plotted in @fig-height-histogram.  In this example we 
measures height in centimeters. It should be pretty obvious that your height
doesn't change depending on the units you measure it in. You're the same height
whether you get measured in centimetres, metres, millimetres, feet, or inches.

If we measured height of the sample of women in metres instead of
centimetres, the shape of the plot should remain the same. You can see
this in @fig-height-histogram-new. 

```{r}
#| echo: false
#| label: fig-height-histogram-new
#| fig-cap: |
#|    Distribution of heights in a sample of 1000 women. Not real
#|    data.
#| fig-subcap:
#|    - "Measured in centimetres"
#|    - "Measured in metres"
#| layout-ncol: 2
# require(ggplot2)
# require(plotshow)
require(cowplot)
set.seed(124)
df <- tibble::tibble(x = rnorm(n = 1000, mean = 0, sd = 1) |>
scale() |> as.numeric())
df$x <- df$x * 10
df$x <- df$x + 165
df_m <- df$x |> mean()
df_s1 <- df$x |> sd()
range <- glue::glue("{round(df_m - df_s  * 1)}--{round(df_m + df_s * 1)}")
ggplot(df, aes(x = x)) +
  geom_histogram(binwidth = 5, boundary = 130) +
  labs(x = "height in cm") +
  theme_cowplot() +
  NULL


set.seed(124)
df <- tibble::tibble(x = rnorm(n = 1000, mean = 0, sd = 1) |>
scale() |> as.numeric())
df$x <- df$x * (10 / 100)
df$x <- df$x + (165 / 100)
df_m <- df$x |> mean()
df_s2 <- df$x |> sd()
range <- glue::glue("{round(df_m - df_s  * 1)}--{round(df_m + df_s * 1)}")
ggplot(df, aes(x = x)) +
  geom_histogram(binwidth = 5 / 100, boundary = 130 / 100) +
  labs(x = "height in m") +
  theme_cowplot() +
  NULL
```


The distribution in @fig-height-histogram-new-1 has a standard deviation
of `r df_s1` while the distribution in @fig-height-histogram-new-2 has a
standard deviation of `r df_s2`. But as you can see, they're they same
distributions---they're just displayed on **different scales**
(centimetres versus metres).


:::{.callout-note}

Changing the **scale** changes the **standard deviation**. This is why
the **standard deviation** is sometimes referred to as the **scale
parameter** for the distribution.

:::


We've seen how we can change the **scale** of the distribution, by
measuring it in metres instead of centimetres. But we can also change
the where the distribution is centred. We can see an example of this in
@fig-height-histogram-new-location.



```{r}
#| echo: false
#| label: fig-height-histogram-new-location
#| fig-cap: |
#|    Distribution of heights in a sample of 1000 women. Not real
#|    data.
#| fig-subcap:
#|    - "Measured in centimetres"
#|    - "Measured in difference from the average height"
#| layout-ncol: 2
# require(ggplot2)
# require(plotshow)
require(cowplot)
set.seed(124)
df <- tibble::tibble(x = rnorm(n = 1000, mean = 0, sd = 1) |>
scale() |> as.numeric())
df$x <- df$x * 10
df$x <- df$x + 165 
df_m <- df$x |> mean()
df_s1 <- df$x |> sd()
range <- glue::glue("{round(df_m - df_s  * 1)}--{round(df_m + df_s * 1)}")
ggplot(df, aes(x = x)) +
  geom_histogram(binwidth = 5, boundary = 130) +
  labs(x = "height in cm") +
  theme_cowplot() +
  NULL


set.seed(124)
df <- tibble::tibble(x = rnorm(n = 1000, mean = 0, sd = 1) |>
scale() |> as.numeric())
df$x <- df$x * (10 )
df$x <- df$x + (165 - 165)
df_m <- df$x |> mean()
df_s2 <- df$x |> sd()
range <- glue::glue("{round(df_m - df_s  * 1)}--{round(df_m + df_s * 1)}")
ggplot(df, aes(x = x)) +
  geom_histogram(binwidth = 5,  boundary = 130 - 165) +
  labs(x = "difference from average height in cm") +
  theme_cowplot() +
  NULL
```

In @fig-height-histogram-new-location-1 we can see the same distribution
as before. But in @fig-height-histogram-new-location-2 we can see a
distribution that is now centred at 0. In this distribution we've just
changed where the centred is **located**, but the distribution is still
the same.

:::{.callout-note}

Changing the **mean** changes where the centre of the distribution is
**located**. This is why the mean is sometimes referred to as the
**location parameter** for the distribution.

:::


The fact that the relative positions of points don't change is a useful
property. In the standard normal distribution, ~68% of the distribution
falls between -1 and +1. Or, put into relative terms, ±1 $\sigma$ from $\mu$.
And ~68% of the distribution will always fall between ±1 $\sigma$ from $\mu$
no matter what value you set for $\sigma$ and $\mu$. You can explore this in 
@exm-normal.



:::{.callout-tip icon="false" appearance="simple"}

:::{#exm-normal}
#### The normal distribution
:::


```{ojs}
normal_plot_output = Plot.plot({
  x: {
    grid: true,
    domain: [-4, 4]
  },
  y: {
    grid: true,
    domain: [0, 0.8]
  },
  marks: [
   Plot.line(
     normal_plot(
       mean_value - sd_value * s > -4 ? mean_value - sd_value * s : -4,
       mean_value + sd_value * s < 4 ? mean_value + sd_value * s : 4,
       mean_value,
       sd_value
     ),
     {
       x: "x",
       y: "y",
       strokeWidth: 1,
       fill: "blue",
       opacity: show ? 0.5 : 0
     }
   ),
   Plot.line(fill_limits(s), {
     x: "x",
     y: "y",
     fill: "blue",
     strokeWidth: 1,
     opacity: show ? 0.5 : 0
   }),
    Plot.line(normal_plot(-4, 4, mean_value, sd_value), {
      x: "x",
      y: "y",
      strokeWidth: 4
    }),
    Plot.ruleY([0])
  ]
})
```

```{ojs}
normal_sliders = htl.html`${mean_value_slider}${sd_value_slider}`
```


```{ojs}
{
  if(mean_value === 0 & sd_value === 1) {
    return texmd`This plot shows the **standard normal** distribution.`
  } else {
    return texmd`This plot shows a normal distribution with a mean ($\mu$) of ${mean_value} and a standard deviation ($\sigma$) of ${sd_value}.`

  }

}
```

You can make adjustments to $\mu$ and $\sigma$ to change the centre and scale
of the normal distribution.


```{ojs}
viewof show = Inputs.toggle({ label: "Show coverage", value: false })
```

```{ojs}
viewof s = Inputs.range([0, 3], { step: 0.5, value: 1 })
```

\
```{ojs}
texmd`Approximately ~68% of the distribution will fall between ±1 $\sigma$ of $\mu$.

Toggle **Show coverage** to highlight the region corresponding to ±$ ${s} $\sigma$ 
from $\mu$. This covers ~${coverage(s)}% of the distribution.

Make adjustments to $\mu$ and $\sigma$. See how the highlighted region covers
the region from ${mean_value - s * sd_value} to ${mean_value + s * sd_value}, or
(${mean_value} - [${s} × ${sd_value}]) to (${mean_value} + [${s} × ${sd_value}]).
`
```

```{ojs}
function coverage(s) {
let cov = jstat.normal.cdf(s, 0, 1) * 2 - 1
return Math.round(cov * 100)
}
```

::::


## Transformations

In @fig-height-histogram-new and @fig-height-histogram-new-location we saw how
we could **transform** a variable so that the shape of the distribution stayed
the same, but the **mean** and the **standard deviation** changed. These two
kinds of **transformations** are known as **centring** and **scaling**.


### Centering 

Centring is performed by **subtracting** a fixed value from each observation in
our dataset. This has the effect of shifting the distribution of our variable
_along the x-axis_. You can technically centre a variable but subtracting _any
value_ from it but the most frequently used method is **mean-centring**.


This is shown in @eq-center, below:

$$x_i - \bar{x}$${#eq-center}


:::{.callout-tip}

Applying this transformation results in shifting the variable so that
it's mean is at the zero point and the individual values of the
mean-centred variable tell us how far that observation is from the mean
of the entire variable.

:::


It's crucially important to understand that mean-centring **does not
alter the shape of the variable, nor does it change the scale at which
the variable is measured**. It only **changes the interpretation** of
the values from the raw scores to differences from the mean.



### Scaling

Scaling is performed by **dividing** each observation by some fixed
value. This has the effect of stretching or compressing the variable
*along the x-axis*.

Just like centring, you can technically scaled a variable by dividing it
by *any* value. For example, we created @fig-height-histogram-new-2 by
taking the values in @fig-height-histogram-new-1 and dividing them by
100 to transform the height in centimetres to a height in metres.
However, the most frequent method of scaling is by dividing values by
the standard deviation of the dataset. This is shown in @eq-scale,
below:

$$\frac{x_i}{s}$${#eq-scale}

Just like with centring, the fundamental shape of the variable's
distribution did not change as a result of scaling. After 
scaling the data by the standard deviation the values would
now be measured in units of sd. 

:::{.callout-tip}

Unlike centring, however, scaling does change the **scale**, or units, on which
the variable is measured. After all, that's why it's called scaling.

:::

### The *z*-transform

The combination of first mean-centering a variable and then scaling the
variable by its standard deviation is known as the **z**-transform. The
formula for this is shown in @eq-z, below:

$$z(x) = \frac{x_i - \bar{x}}{s}$${#eq-z}


```{r}
set.seed(555)
d <- rnorm(10, 5, 2) |> round()


```

We can see an example of how to z-transform some data in @tbl-z. The 10
values in @tbl-z have a mean of `r mean(d)` and a standard deviation
of `r sd(d) |> round(2)`. In the column labelled **centred**, the `r mean(d)`
has been subtracted from the raw values. If we were to work 
out the mean of this column the value would be 0. The
column labelled **scaled** contains the values in **centred**
but divided by `r sd(d) |> round(2)`. If you were to work out
the mean of this column is would still be 0. And if you were to work
out the standard deviation of this column it would now be 1.^[I 
won't include the `R` code for how you might z score some data in 
a `tibble`. Instead, I'll leave that are an exercise for you to figure out]

```{r}
#| label: tbl-z
#| tbl-cap: z transformed data
tibble::tibble(r = d) |>
  dplyr::mutate(centred = d - mean(d)) |>
  dplyr::mutate(scaled = centred / sd(d)) |>
  magrittr::set_colnames(
c("Raw values",
"Centred",
"Scaled")) |>
knitr::kable(digits = 2)
```

When we've _z_-transformed data^[Also called
**standardising**, because the *mean* is 0 and the *standard deviation* is 1, just
like the **standard normal distribution**] we can now interpret the data in terms 
of **distance from the mean in units of standard deviation**.  

Being able to do this makes it easier to make comparisons. And making comparisons
is where we turn our attention to next.

## Making comparisons 

### Comparing groups

```{r}
sport_data <- tibble::tribble(
  ~group, ~mean, ~sd,
  "amateur", 500, 30,
  "sport", 460, 15
)


ama_mean <- sport_data |>
  dplyr::filter(group == "amateur") |>
  dplyr::pull(mean)
spo_mean <- sport_data |>
  dplyr::filter(group == "sport") |>
  dplyr::pull(mean)

N <- 500

set.seed(123)
sport_tib <- purrr::pmap_df(sport_data, function(group, mean, sd) {
  d <- rnorm(N, mean, sd) |> scale() |> as.numeric()
  tibble::tibble(group = group, x = mean + (d * sd))
})
```

When we talk about comparing groups on some variable in the context of
quantitative research we are most often talking about looking at the
**average difference** in the variable between the groups. In other
words, we are asking, how different the groups are *on average*.

Let's make it concrete with an example. Suppose we're interested in
amateur and professional sportspeople on a simple target detection task
where participants have to press a button as quickly as possible
when a particular stimulus appears on computer screen. We get 500
amateur and 500 professional sportspeople to participant in this
experiment. The results of this experiment show that amateurs have a 
mean reaction time of `r ama_mean` ms and professionals have a mean 
reaction time of `r spo_mean` ms. @fig-sport-histogram shows the 
histogram of these data. 

```{r}
#| echo: false
#| warning: false
#| label: fig-sport-histogram
#| fig.align: center
#| fig.cap: |
#|    Distribution of reaction times in a sample of amateur (green) and
#|    500 professional (blue) sportspeople. Group means are indicated
#|    with the vertical lines.
require(ggplot2)

ggplot2::ggplot(sport_tib, aes(x = x, 
  fill = group,
  group = group,
  alpha = group,
  color = group,
  size = group)) +
  geom_histogram(bins = 50, boundary = 100, position = "identity") +
  scale_alpha_manual(values = c(1, 0.7), guide = "none") +
  scale_fill_manual(values = c("seagreen", "blue"), guide = "none") +
  scale_color_manual(values = c("white", "white"), guide = "none") +
  scale_size_manual(values = c(.2, .2), guide = "none") +
  geom_vline(xintercept = ama_mean, color = "seagreen", size = 1.5) +
  geom_vline(xintercept = spo_mean, color = "blue", size = 1.5) +
  ggplot2::annotate(geom = "text", x = ama_mean, y = 60,
  label = expression(bar('X')[amateur]), hjust = -0.3, color = "seagreen") +
  ggplot2::annotate(geom = "text", x = spo_mean, y = 60,
  label = expression(bar('X')[pro]), hjust = -0.3, color = "blue") +
  theme_cowplot() +
  labs(x = "reaction time (ms)", y = "count") +
  NULL

```

In this example we can see that there is a lot of overlap between the
two groups. However, we can also clearly see that there is a difference
in the *average* reaction times between amateur and professional
sportspeople. To quantify this difference, all we would need to do is to
**subtract the mean of one group from the mean of the other group**.

So for our example in @fig-sport-histogram, the mean of the amateurs is 
`r ama_mean` ms and the mean of the professionals is `r spo_mean`, so the mean
**difference** is just `r ama_mean`ms - `r spo_mean`ms = `r ama_mean - spo_mean`ms. 
Because this difference is positive, the amateurs have a
reaction time that is `r ama_mean - spo_mean`ms higher (slower) than the
professionals.


The sign indicates the direction of the difference. If the number is
positive, that means that the first group's mean is larger than that of the
second group. If the number is negative, the opposite is true. Of course, it
is completely arbitrary which group is *first* and which is *second*.

### Comparison across groups

In the example above, the comparison was easy to make because the two 
measurements were measured on the same scale. But sometimes we want 
to compare measurements that are measured on different scales.

Suppose you're interested in comparing the performance of two children on a
puzzle completion tasks. One child is 8 years old and the other is 14 years
old. Because 8-year-olds and 14-years-olds are at different developmental stages
there are two versions of the task that are scored in slightly different ways.
Because we now have two tests that might have a different number of items, and
that might be scored in different ways, we can't just compare the two numbers
to see which is bigger. So what do we do instead? We'll explore this with
and example.

```{r}
#| echo: false
stats <- tibble::tribble(
  ~age, ~mean, ~sd,
  "8", 80, 2,
  "14", 120, 8,
  )

a_z <- 3
b_z <- .5
a_child <- stats$mean[1] + (stats$sd[1] * a_z)
b_child <- stats$mean[2] + (stats$sd[2] * b_z)


```

Ahorangi is 8 years old, and she got a score of `r a_child`. Benjamin is
14 years old, and he got a score of `r b_child`. We can easily tell that
Benjamin got a higher score than Ahorangi. But the scores are not
directly comparable, because they're measured on different scales. So
how can we compare them? What we need to do instead is look at how each
of them performed **relative** to their age groups. Is
Ahorangi better performing relative to 8-year-olds than Benjamin is
relative to 14-year-olds?

To answer this question we can use the *z*-scores we learned about earlier. By
standardising the time variable across each group, we get variables that are on
the same scale. Do do this, we'll need to know the **mean** and **standard
deviation** for each of the age groups. We can see these details in
@tbl-age-means.


```{r}
#| echo: false
#| label: tbl-age-means
#| tbl-cap: |
#|      Means and Standard deviations for the 8-year-old and
#|      14-year-old age groups
stats |>
  dplyr::mutate(age = dplyr::case_when(
    age == 8 ~ "8-year-olds",
    age == "14" ~ "14-year-olds")) |>
  knitr::kable(col.names = c("Age group", "Mean", "Standard deviation"))
```


Let's use this formula to calculate Ahorangi and Benjamin's *z*-score. First,
for Ahorangi:

$$`r a_z` = \frac{`r a_child` - `r stats$mean[1]`}{`r stats$sd[1]`}$$

And next for Benjamin:


$$`r b_z` = \frac{`r b_child` - `r stats$mean[2]`}{`r stats$sd[2]`}$$

So we now know that Ahorangi's *z*-score is `r a_z`, and that Benjamin's
*z*-score is `r b_z`. This means that Ahorangi's score is `r a_z` standard
deviations *higher* than the average 8-year-old. Benjamin's *z*-score is `rb_z` 
*higher* than the average 14-year-old. That means, that Ahorangi, despite
having a lower score, actually scored very high for an 8-year-old. Benjamin, on
the other hand, only scored a little higher than the average 14-year-old.

## Making comparisons with the sampling distribution

The final kind of comparison we'll talk about is the comparison between 
our sample and the sampling distribution. Last week learned about the sampling
distribution. And we learned that the sampling distribution of the mean will
be centred at the population mean and have a standard deviation that is
equal to the standard error of the mean.^[This is a slight simplification. 
The standard error of the mean is an _estimate_ of the standard deviation 
of the sampling distribution. This estimate is biased, so in reality
you would make a slight modification to the normal distribution to 
correct for this bias. You'll learn about this next year when you cover
the _t_-test.]

As I've already **emphasised**, you don't know the value of the **population
mean**, which means that we won't know what the sampling distribution of 
the mean looks like for our particular population. So what use then is 
the sampling distribution?

Although we won't know the population mean we can generate a hypothesis about
what we think the population mean might be. We can then use this hypothesis 
about the population mean value, together with the standard error, to generate 
a sampling distribution. The sampling distribution describes what would happen
if we were to repeatedly take samples from the population and work out the mean.

```{r}
#| echo: false
#| message: false
require(tidyverse)
# require(plotshow)
set.seed(123)
n <- 50
face_celeb <- rnorm(n, 500, 50)
face_family <- rnorm(n, 520, 45)
subject_id <- seq(1, n)
df <- tibble::tibble(face_celeb, face_family, subject_id)

df |>
  tidyr::pivot_longer(1:2,
    names_to = "condition",
          values_to = "rt")|>
  dplyr::group_by(subject_id) |>
  dplyr::arrange(condition) |>
  dplyr::summarise(diff = diff(rt)) |>
  dplyr::ungroup() -> df

sum_stats <- df  |>
  dplyr::summarise(
    mean_rt = mean(diff),
    sd_rt = sd(diff),
    n = n(),
    se = sd_rt / sqrt(n)
)




```

I'll make this concrete by way of an example. Let's say that interested in
whether people are quicker at recognising the faces of family members versus
the faces of celebrities. You get your participants to perform a task whether
they are shown faces and they have to press a button whenever the recognise
them. You find that the mean difference between these two conditions is 
`r sum_stats$mean_rt |> round(2)`ms. But this is just the mean difference is your sample. The
mean difference in the population might be some other value that is smaller or
larger.

Now we don't know the population mean difference, but we could hypothesise a
value. For example, we might hypothesise that the population mean difference is
100 ms, 50 ms, 0 ms, or some other value. Once we have hypothesised a value, we can generate a
sampling distribution. Our sampling distribution will be centered at our
hypothesised value, and it'll have a standard deviation equal to the standard
error of the mean. For our experiment, we had `r sum_stats$n` participants,
so our sample size is `r sum_stats$n`. Our sample also had a standard
deviation of `r sum_stats$sd_rt |> round(2)`. Together we can work out a standard error
of the mean that is equal to $\frac{`r sum_stats$sd_rt |> round(2)`}{\sqrt{50}}$ or
`r round(sum_stats$sd_rt / sqrt(sum_stats$n), 2)`.

Now that we have a hypothesis about the population mean, and we have an
estimate of the standard error of the mean, we can create a sampling
distribution that tells us what would happen if we were to run the experiment
many times. In @fig-sampling we can see what the sampling distribution would
look like if the population mean were 0.

```{r}
#| label: fig-sampling
#| fig-cap: !expr glue::glue("The sampling distribution with a mean of 0 and a SEM of {sum_stats$se |> round(2)}")
require(tidyverse)

args <- sum_stats |>
  magrittr::set_colnames(
    c("mean", "sd", "n","se")
) |>
  dplyr::mutate(sd = sd / sqrt(n)) |>
  dplyr::mutate(mean = 0) |>
  dplyr::select(-n, -se) |> as.list()



ggplot() +
  geom_function(fun = dnorm, args = args) +
  xlim(args$mean - 4 * args$sd, args$mean + 4 * args$sd) +
  cowplot::theme_cowplot() +
  labs(y = NULL, x = "sample mean")

```

We can now compare our particular sample mean of `r sum_stats$mean_rt |> round(2)`ms 
to this sampling distribution. Because the sampling distribution is a normal distribution
we know that ~68% of the time the sample means will fall between ±1 SEM of the population
mean. Or, between `r round(sum_stats$se, 2) * -1`ms and `r round(sum_stats$se, 2)`ms.
And ~95% of the time sample means will fall between `r round(sum_stats$se, 2) * -2`ms
and `r round(sum_stats$se, 2) * 2`ms. 

If we look at our particular mean we see that it falls 
`r round(sum_stats$mean_rt / sum_stats$se, 2)` SEM from our hypothesised
population mean. What can we make of this? From this we can conclude that if
the population mean were in fact 0, then we have observed something rare. That
is, if the population mean were in fact 0, it would be rare for a sample mean
(calculated from a sample drawn from this population) to be that far away from
the population mean. Observing something rare does not in itself tell us that
our hypothesis about the population mean is wrong. After all rare events, like
people winning the lottery, happen every day. On a more mundane level, we
might be confident that men are (on average) taller than women. But as 
anybody who has attended a professional netball will tell you, it can
certainly happen that you come across a group of people were all the women 
are taller than the men, although it would be rare and surprising. 

The same goes for our sample. Observing something rare (according to our
hypothesised population mean) might just mean we've observed some rare. But if
we were to run our experiments again and again, and we continued to observe
rare events then we would probably have a good reason to update our hypothesis. 

This process, where we compared our sample value to the sampling distributed
constructed from a hypothesised value is known as *null hypothesis significance
testing*, will be a major topic that you'll cover next year. 



## Check your understanding

Use this quiz to make sure that you've understood the key concepts.

```{r}
#| eval: false
#| echo: false
#| message: false

require(niceQuiz)

sem <- function(x) {
  s <- sd(x)
  n <- length(x)
  s / sqrt(n)
}

quiz(
  caption = "",
  question(
    "The mean of a set of numbers is 10, the variance is 25, and the sample size is 16. <br />What is the standard error of the mean?",
    numericanswer(sqrt(25)/sqrt(16)),
    type = "fillin")
    )
```

```{r}
#| eval: false
quiz(
caption = "",
question(
  "A distribution with fatter tails than a normal distribution is called?",
  answer("Mesokurtic"),
  answer("Platykurtic"),
  answer("Leptokurtic", correct = TRUE)),

question(
  "A distribution with skewness of -1.5 is:",
  answer("Right-skewed"),
  answer("Left-skewed", correct = TRUE)),

question("The sampling distribution of the mean with have what kind of distributions?",
answer("A normal distribution", correct = TRUE),
answer("The same distribution as the population"),
answer("A distribution that is more skewed than the population distribution")
)





)

```






```{ojs}
vega = require("https://cdn.jsdelivr.net/npm/vega@5/build/vega.js")
```

```{ojs}
jstat = require("https://bundle.run/jstat@1.9.4")
```

```{ojs}
import { dist } from "@ljcolling/wasm-distributions"
```

```{ojs}
sd_value_slider = Inputs.range([0.5, 2], {
  value: 1,
  step: 0.25,
  label: htl.html`standard deviation <br />&#x3C3;`
})
```

```{ojs}
import { texmd } from "@kelleyvanevert/katex-within-markdown"
```

```{ojs}
mean_value_slider = Inputs.range([-3, 3], {
  value: 0,
  step: 0.25,
  label: htl.html`mean<br />  &#x3BC`
})
```

```{ojs}
sd_value = Generators.input(sd_value_slider)
```

```{ojs}
mean_value = Generators.input(mean_value_slider)
```

```{ojs}
// jStat.normal.pdf( x, mean, std )
normal_plot = (min, max, mean, sd) => {
  // jStat.normal.pdf(x, mean, sd)

  return d3.ticks(min, max, 501).map((v) => {
    return {
      x: v,
      y: dnorm(v, mean, sd)
    };
  });
}
```

```{ojs}
skew_normal_plot = (min, max, alpha) => {
  // jStat.normal.pdf(x, mean, sd)

  return d3.ticks(min, max, 201).map((v) => {
    return {
      x: v,
      y: dsn(v, alpha)
    };
  });
}
```

```{ojs}
dsn = (x, alpha) => {
  // set the defaults

  const xi = 0;
  const omega = 1;
  const tau = 0;

  let z = (x - xi) / omega;

  let logN = -Math.log(Math.sqrt(2 * Math.PI)) - 0 - Math.pow(z, 2) / 2;

  let logS = Math.log(
    jStat.normal.cdf(tau * Math.sqrt(1 + Math.pow(alpha, 2)) + alpha * z, 0, 1)
  );

  let logPDF = logN + logS - Math.log(jStat.normal.cdf(tau, 0, 1));

  return Math.exp(logPDF);
}
```

```{ojs}
// import jStat library
jStat = require("jStat")
```

```{ojs}
kurtosis = {
  return {
    uniform: -(6 / 5),
    raised_cosine: (6 * (90 - Math.PI ** 4)) / (5 * (Math.PI ** 2 - 6) ** 2),
    standard_normal: 0,
    t_dist30: 6 / (30 - 4),
    t_dist20: 6 / (20 - 4),
    t_dist10: 6 / (10 - 4),
    t_dist7: 6 / (7 - 5),
    t_dist5: 6 / (5 - 4)
  };
}
```

```{ojs}
kurtosis_values = Object.values(kurtosis).map((v) => Math.round(v * 100) / 100)
```

```{ojs}
dists = {
  return {
    raised_cosine: d3.ticks(-3, 3, 500).map((v) => {
      return {
        x: v,
        y: dist.raised_cosine(v, 0, 2.5)
      };
    }),
    standard_normal: d3.ticks(-3, 3, 500).map((v) => {
      return {
        x: v,
        y: dnorm(v, 0, 1)
      };
    }),
    t_dist30: d3.ticks(-3, 3, 500).map((v) => {
      return {
        x: v,
        y: dist.dt(v, 30)
      };
    }),
    t_dist20: d3.ticks(-3, 3, 500).map((v) => {
      return {
        x: v,
        y: dist.dt(v, 20)
      };
    }),
    t_dist10: d3.ticks(-3, 3, 500).map((v) => {
      return {
        x: v,
        y: dist.dt(v, 10)
      };
    }),
    t_dist7: d3.ticks(-3, 3, 500).map((v) => {
      return {
        x: v,
        y: dist.dt(v, 7)
      };
    }),
    t_dist5: d3.ticks(-3, 3, 500).map((v) => {
      return {
        x: v,
        y: dist.dt(v, 5)
      };
    }),

    uniform: d3.ticks(-2.1, 2.1, 500).map((v) => {
      return {
        x: v,
        y: dist.dunif(v, -2, 2)
      };
    })
  };
}
```

```{ojs}
fill_limits = (mult) => {
  let s = sd_value * mult;
  return [
    { x: mean_value - s > -4 ? mean_value - s : -4, y: 0 },
    normal_plot(
      mean_value - s > -4 ? mean_value - s : -4,
      mean_value - s > -4 ? mean_value - s : -4,
      mean_value,
      sd_value
    )[0],
    normal_plot(
      mean_value + s < 4 ? mean_value + s : 4,
      mean_value + s < 4 ? mean_value + s : 4,
      mean_value,
      sd_value
    )[0],
    { x: mean_value + s < 4 ? mean_value + s : 4, y: 0 }
  ];
}
```

```{ojs}
function dnorm(x, mean, sd) {
  return jstat.normal.pdf(x, mean, sd);
}
```


