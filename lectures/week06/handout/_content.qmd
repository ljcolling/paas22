```{r}
#| echo: false
#| include: false

require(ggplot2) 
require(cowplot)
require(tidyr)
require(dplyr)
require(plotly)
salary <- readr::read_csv(here::here("data","world_salary.csv"))
USD <- scales::dollar_format(suffix = " USD")
ggplot2::theme_set(cowplot::theme_cowplot())
```

In [Lecture 4](/lectures/week04/handout) we started talking about how
quantitative methods deal with **measurement**---that is, putting numbers to
things. In today's lecture we're going to start talking about what we actually
do with these numbers, and how to make a little more sense of measurements in
general. This lecture will be the first in a set of lectures where we'll talk
about **samples** and **populations**, how to **describe** them, and how to
understand the relationship between them. A lot of these ideas are
interconnected, so we'll be touching on some ideas multiple times. But
hopefully each time we'll be able to gain a richer understanding.

The first thing we'll want to do when we've collected a set of measurements is
to describe them in some way. One way to do this is to work out what the
**typical value** is. And it's this kind of description that we'll turn our
attention to first. We'll start off by looking at three different ways we can
describe the **typical value** is a set of numbers. 

## Measures of central tendency

What we mean by the **typical value** is not always clear. For example, in
@fig-income we can see the average annual salary (in US dollars) for a set of
`r dim(salary)[1]` countries. Each bar of the plot represents the number of
countries in the given salary bracket ($0--$10k, $10-$20k, ...).

```{r}
#| echo: false
#| warning: false
#| message: false
#| fig.align: center
#| fig.height: 3
#| label: fig-income
#| fig.cap: "National average annual salary [source: https://www.worlddata.info/average-income.php]"
(salary |> ggplot(aes(x = yearly)) +
  geom_histogram(binwidth = 1e4, boundary = 0, fill = "white", color = "black") +
  labs(x = "Annual salary in USD", y = "Count")) |>
  plotly::ggplotly(tooltip = "count") |>
  plotly::config(displayModeBar = F)
```


From the plot we can see that there are a lot of countries where the average
annual salary is less than $30,000 USD. There are also a handful of countries
where the average annual salary is more than $100,000 USD. What would you
consider the _most typical_ annual salary? The bracket with the most countries
in it? If so, that would mean the most typical salary on the planet is between
$0 and $10,000 USD per year. Or maybe we should pick the value where half the
countries have a lower average salary and half the countries have a higher one?
Choosing this option leads to an estimate of the most typical salary of
`r salary$yearly |> median() |> USD()` per year.

As you can see, depending on how we define _the most typical value_, we get
different answers. We'll cover the three main ways of defining the _typical
value_ or _average_. Together, these ways of describing the _typical_ or
_average_ value are known as **measures of central tendency**.

### Mode

The **mode** is a term that refers to the **most frequent value** in a set of
measurements. This is the kind of _average_ we discussed above when we said the
most typical salary on the planet is between $0 and $10,000 USD a year. The
easiest way to spot the **mode** is just to draw a plot like the one we did in
@fig-income and then just look for the tallest bar.

A set of numbers can have one or more modes. If it only has one mode, then it is
said to be **unimodal**. **Bimodal** means it has two modes. If it has three or
more modes, then it is usually called **multimodal**. Some examples of
this are shown in @fig-diff-modes.

```{r}
#| echo: false
#| label: fig-diff-modes
#| fig-cap: Datasets with different numbers of modes.
#| fig-subcap:
#|    - "A unimodal dataset"
#|    - "A bimodal dataset"
#|    - "A multimodal dataset"
#| layout-ncol: 3
require(ggplot2)
require(dplyr)

# Let's draw some plots

# Unimodal
set.seed(123)
tibble(x = rchisq(n = 10000, df = 5, ncp = 0)) |>
  ggplot(aes(x = x)) +
  geom_histogram(bins = 50, boundary = 0, col = "grey20", fill = "grey80") +
  theme_cowplot() +
  labs(x = "value", y = "count")

# Bimodal
set.seed(123)
d <- rnorm(n = 5000, mean = 2, 1)
tibble(x = c(
  d,
  d + 3
)) |>
  ggplot(aes(x = x)) +
  geom_histogram(bins = 50, boundary = 0, col = "grey20", fill = "grey80") +
theme_cowplot() +
labs(x = "value", y = "count")

# Multimodal
d <- rep(seq(0, 5, length.out = 100), times = 100)
tibble(x = d) |>
  ggplot(aes(x = x)) +
geom_histogram(bins = 11, boundary = 0, col = "grey20", fill = "grey80") +
theme_cowplot() +
labs(x = "value", y = "count")
```


:::{.callout-important}

The mode is the only definition of _typical value_ that works for data that is
measured at the **nominal**/**categorical** level.

When it comes to truly **continuous** variables, such as height, the mode is
often not very informative. Why? Because no two people are **exactly** the same
height (right down to the exact same number of nanometres), so each value in the
dataset may be unique. For this reason, the mode is rarely used for continuous
variables measured at the **interval** or **ratio** levels.

Average salary is continuous variable, but we turned it into a discrete variable
placing chunking the measurements into discrete bins. 

:::

### Median

```{r}
#| echo: false
dice_res <- c(3, 4, 6, 1, 1) 

```

The **median** is the second kind of average we talked about
[above](#measures-of-central-tendency); the
middle value where half the measurements are above that value and half the
measurements are below. To find the median, we first need to sort our data.
Let's say we roll a 6-sided dice 5 times and get the following: 
`r paste0(dice_res[1:4], collapse = ", ")` and `r dice_res[5]`.

To calculate the median, let's do the two steps:

1. Sort the data from smallest or largest: `r sort(dice_res)`

2. Find the mid-point: We have five observations so the third one in the
sorted sequence is the mid-point.

So out of five rolls the median is `r median(dice_res)` (and the mode is 1). If
we had an even number of observations then the median would be the half-way
point between the two mid-point values. For example, if we instead rolled the
dice 6 times and got the results 1, 1, 3, 4, 4, and 6 then the median would be
the midpoint between 3 and 4, or `r median(c(1, 1, 3, 4, 4, 6))`.

@fig-sorted below shows the annual salary in USD per each of the countries in the
data set, sorted from lowest to highest. Notice, that this time, weâ€™re not
grouping countries in salary brackets and looking at how many there are in each
one as was the case in @fig-income. Here, each bar represents a country.

Because we have an even numbers of countries in our dataset (78), there are two
mid-points. These are highlighted in orange in the plot below. To get the median
annual national salary, we need to find the value half-way between the average
salary in Romania and Venezuela, which in this data set turns out to be 
`r median(salary$yearly) |> USD()`.


```{r}
#| echo: false
#| label: fig-sorted
#| fig.cap: |
#|  Average national salary per country sorted from lowest to highest (Hover 
#|  over the bars to see the name of the country and the value).
(salary |>
  dplyr::arrange(yearly) |>
  dplyr::mutate(index = 1:n()) |>
  dplyr::mutate(
    color =
      case_when(
        index == (1:78 |> median() |> floor()) ~ "red",
        index == (1:78 |> median() |> ceiling()) ~ "red",
        TRUE ~ " grey"
      )
  ) |>
  dplyr::mutate(text = glue::glue("{country} (${round(yearly/1000,2)}k)")) |>
  ggplot(aes(x = index, y = yearly, text = text, fill = color)) +
  ggplot2::scale_fill_manual(
    name = NULL,
    values = c("#454A60", "#ff4500"), labels = NULL, guide = "none"
  ) +
  labs(y = "Annual salary in USD", x = "Country") +
  scale_x_continuous(labels = NULL, breaks = NULL) +
  scale_y_continuous(labels = function(x) paste0(x / 1000, "k")) +
  ggplot2::theme(
    axis.text.x = element_blank(), axis.ticks.x = element_blank(),
    legend.position = "none", legend.title = element_blank(),
    panel.grid.major.y = element_line(color = "grey60"),
    axis.line.x = element_blank(), axis.line.y = element_blank()
  ) +
  geom_col() +
  NULL
) |>
  plotly::ggplotly(tooltip = "text") |>
  plotly::config(displayModeBar = F)
```


:::{.callout-important}

To be able to calculate a meaningful median, the variable must be measured on
**at least the ordinal level**.

:::

### Mean

The arithmetic mean is what most people mean when they talk about the
*average*. You can work the mean of a set of numbers by adding up all
the values and then dividing this by the number of values in the data
set. Mathematically, you can represent this with the formula shown
in @eq-mean, below:

```{r}
#| echo: false
rgb2 <- \(red, green, blue) rgb(red, green, blue, maxColorValue = 255)
hex <- function(x) {
  stringr::str_split(x, ",")[[1]] |>
  as.numeric() |>
  setNames(c(
    "red",
    "green", "blue"
  )) |>
  as.list() |>
  do.call("rgb2", args = _)
}
mean_c <- "255, 0, 0"
sum_c <- "0, 255, 0"
all_c <- "0, 0, 255"
div_c <- "255, 0, 255"
```


$$
\definecolor{sum}{RGB}{`r sum_c`}
\definecolor{n}{RGB}{`r all_c`}
\definecolor{i}{RGB}{`r all_c`}
\definecolor{x}{RGB}{`r all_c`}
\definecolor{div}{RGB}{`r div_c`}
\definecolor{mean}{RGB}{`r mean_c`}
{\color{mean}\bar{x}}={\color{div}\frac{1}{N}}{{\color{sum}\sum^{\color{n}N}_{\color{i}i=1}}{\color{x}x_i}}
$$ {#eq-mean}

Take <font color="`r hex(mean_c)`">mean of the data</font> is equal to
<font color="`r hex(sum_c)`">the sum</font> of
<font color="`r hex(all_c)`">all the values</font>
<font color="`r hex(div_c)`">divided by the number of values</font>

<!-- TODO: Break down the sum notation a bit more -->

### Mean vs Median

Both of these measures have their advantages and disadvantages. The mean is
easier to worth with from a mathematical point of view. And for this reason,
most of the statistical methods we'll be learning about are based on the mean.

Compared to other measures of central tendency, *means taken from different
samples of the same population* tend to be **more similar to each other**. If,
on the other hand, we calculated medians for different samples from the same
population, there would be more variability in the values we'd obtain. Turning
back to our dice example: If we took the median and the mean of 5 dice rolls,
and we did this over and over, the mean values would be more bunched around 3.5
with very few values less than 2. The median values would still be be centred
around 3.5, but we'd get more values less than 2, so they would be more spread
out.


However, there are also some downsides to the **mean** relative to the
**median**. The primary one is that the _mean_ is **sensitive to extreme
values** in a way that the _median_ is not. This means that, even if we have a
really big sample size, adding a **single value** that is extremely big or
extremely small can shift the _mean_ dramatically. This is not the case for
the _median_.




```{r}
#| echo: false

d1 <- c(5, 3, 1, 7, 10, 4, 5)
n_v <- 1000
d2 <- c(d1, n_v)
```


For example, let's say we have a list of numbers: `r d1`. This list of numbers
has a median of `r median(d1)` and a mean of `r mean(d1)`. If we add an
additional value of `r n_v` to this list, then our new median would be unchanged
at `r median(d2)`, but our new mean would be `r mean(d2)`. You can explore the
influe of data points on the mean and the median in @exm-mean-median.



::::{.callout-tip icon="false" appearance="simple"}

:::{#exm-mean-median}
#### Explore the mean and median
:::

Click on the plot below to add data to the data set. After you've place a data
point, you can click and drag it to move it. You can use the check boxes
to diplay the mean (red) and median (blue).

To see how the <font color="red">**mean**</font> and the <font
color="blue">**median**</font> are influenced by extreme value try adding 
several points to the left side of the plot before adding a single point to
the far right. Drag this point around moving it closer or further away 
from the left side points. Notice how the <font color="blue">median</font>
is unchanged but the <font color="red">mean</font> does change.

Tick the *show data table* box to show the raw data. From this you should be
able to work out the mean and median yourself.


```{ojs}
//| echo: false
import {central_tendency_display} from "@ljcolling/measures-of-central-tendency"
import {viewof ave_select} from "@ljcolling/measures-of-central-tendency"
import {viewof clear} from "@ljcolling/measures-of-central-tendency"
import {viewof show_data_table} from "@ljcolling/measures-of-central-tendency"
import { data_table} from "@ljcolling/measures-of-central-tendency"
import {viewof summaryText} from "@ljcolling/measures-of-central-tendency"
central_tendency_display
```

\
```{ojs}
//| echo: false
//| panel: input
viewof ave_select
viewof clear
viewof show_data_table
```

```{ojs}
//| echo: false
data_table
```

```{ojs}
//| echo: false
viewof summaryText
```

::::



<!-- FIXME: Add visualisation of mean and median -->

## Sample means and population means

So far we've just been talking about describing the typical value in a set of
measurements that we have---our **sample**. But one of the key things that we
want to do with *statistics* is to make **inferences** about **populations**
from *the information* that we get from **samples**. That is, we often want to
make a judgement, or draw a conclusion, about an aspect of the population when
all we have access to is a sample.

We'll get to more formal definitions of _populations_ and _samples_ shortly, but
first, let's make things more concrete by introducing an example.

Let's say you're interested in the **average height** of **people in the UK**.
The "easy" way to find an answer to this question is to measure **all the people
in the UK** and then work out the **average height**. Doing this will give you
the exact answer to your question. But if you can't measure everyone in the UK,
then what do you do?

One option is to select a smaller group, or subset, of people from the UK. You
can then measure the height of people in this group, and then try to use this
information to figure out plausible values for the average height of people in
the UK.

In this example, the group (or groups) you're making claims about is the
population. You want to claims about **the average height** of **people in the
UK**. And the **sample** is a subset of this population---the smaller group of
people that you were eventually able to measure.

It's important to note that there isn't a **single** population. What counts as
the population will depend on the claim you're making. For example, let's say
I'm interested in testing the claim, "Do **people in East Sussex** show an
interference effect on the Stroop task?". Here the **population** would be
**people in East Sussex**. If, however, I want to make claims about **people in
general**, then the **population** might be all **living** humans.

### Theoretical populations

So far in our talk of populations we've only really be thinking about
populations as the **set of actually existing things** that we can take our
sample from---for example, all **living** humans. But populations don't have to
be sets of actually existing things. Instead, they can be the **set of possible
things** from which our samples can be drawn. This might seem a little
confusing, so an example might help.

Let's say we want to collect a sample of 6 coin flips. To collect our
**sample**, we take a coin and _flip it 6 times_ and count up the number of
heads and tails and from this we could work out, for example, how many _heads
are typically seen when flipping the coin 6 times_.

So that's our **sample**, but what is our **population**? One way to think of
our population is as the **set of possible outcomes (numbers of heads) that
would occur if we flipped the coin 6 times**. It turns out that can actually
work this out.

To work it out we would do something like the following:

<!-- FIXME: do binimials -->

We can work it out because we know something about the **process that gives rise
to our data**. Although we might not be able to clearly specify the process that
gives to the data in a Stroop task, there will still be some **data generating
process**, which we can think of as the **population** we're sampling from.


:::{.callout-note}

In later years you'll learn about something called the **null hypothesis**. The
**null hypothesis**, in simple terms, says that the **data generating process**
for two sets of observations is same. For example, it might say that the _data
generating process_ that gives rises to people's responses on the Stroop task is
the same for the **congruent condition** and the **incongruent condition**.
You'll learn how to perform statistical tests that tell you whether the set of
data that you've actually collecting is **surprising** or **unsurprising** if
you were to assume that _two_ **data generating processes** are actually the
same.

:::



### The relationship between samples and populations

Let's assume that we have explicitly defined our **population** (for example, as
_all people in the UK_) and we've collected a **sample** by taking measurements
from a **subset** of this population. What is the relationship between this
sample and the population from which it was drawn?

The _sample_ should **resemble** the _population_ in some way. Most often we're
interested is knowing something like: "_What is the typical value (i.e., the
mean) of the population?"_ In the example I introduced earlier, we were
interested in **average height**. But we might also be interested in things a
difference between two averages---for example, whether there is a difference in
**average depression levels** before and after some intervention, or whether
**average response times** are different between the two conditions of a Stroop
task. Ideally then, the **average height** of our **sample** should **resemble**
the **average** height of our **population**, or the _average_ _response_ _time_
_difference_ in our **experimental sample** should _resemble_ the _average
response time difference_ in our population. But if we don't know the
**average** of our **population**, then how will we know whether our **sample**
_resembles_ it?

Adults in the UK range in height between 78cm and 231cm but the the average
height of an adult in the UK is 170cm. So our population mean is 170cm. Now
let's collect a sample of data. We'll talk more about the influence of sample
size in [Lecture](). But for now let's just say that we collect a sample of 50
people. And let's say that we don't only collect one sample, but that we collect
a sample of 50 over and over again. In @exm-mean-samples you can see the average
height of our sample of 50 people, with a solid line showing the population
mean.

<!-- FIXME: Animation goes in here -->

::::{.callout-tip icon="false" appearance="simple"}

:::{#exm-mean-samples}
#### Explore sample means
:::

```{ojs}
//| echo: false
import {sample_means} from "@ljcolling/measures-of-central-tendency"
import {viewof replay_mean} from "@ljcolling/measures-of-central-tendency"
sample_means
```

\
```{ojs}
//| echo: false
///| panel: input
viewof replay_mean
```

::::

What do you notice? The sample means don't always line up exactly with the
population mean. Sometimes the sample mean is higher and sometimes the sample
mean is lower. It moves around a bit from sample to sample. Because it moves
around, and because we don't know the population mean, this tells us that **on
any particular sample** we won't know whether the sample mean is the same as the
population mean.


### The average of the sample means

But let's think of things from a slightly different perspective. Let's treat
the mean of each sample of 50 people as a measurement. We'll now take a
**"sample"** of these measurements. That is, we'll measure the height of 50
people and we'll work out the average height. This might be something like 168
cm. We'll then measure another 50 people and we'll work out the average height,
which might be something like 175 cm. We'll then work out the average of these
two averages (`r (168 + 175) / 2` cm). We'll do this over and over recalculating
our average of averages after each new sample of 50 people. In
@exm-running-samples we can see can see what happens to our average of our
**sample of samples**. What do you notice?

<!-- FIXME: Aniimatio -->


::::{.callout-tip icon="false" appearance="simple"}

:::{#exm-running-samples}
#### Explore sample of samples
:::

```{ojs}
//| echo: false
import {sample_means_ave} from "@ljcolling/measures-of-central-tendency"
import {viewof replay_running_mean} from "@ljcolling/measures-of-central-tendency"
sample_means_ave
```

\
```{ojs}
//| echo: false
///| panel: input
viewof replay_running_mean
```

::::

That's right, as we carry on collecting more and more **samples of samples** the
average of these will eventually line up and match the **population mean**.
What can we conclude from this?

This tells us that even though we don't know whether the mean of **any
particular sample** is the same as the population mean, the **sample mean** will
**on average** be the same as the population mean. We'll touch of this idea more
in future lectures when we talk about the **sampling distribution**, but for now
this simple idea is all you need to know. 

<!-- FIXME: This should probably be moved to the next lecture -->

