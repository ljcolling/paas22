## The normal distribution

In the previous lecture we encountered the **binomial distribution**. The shape
seen in the **binomial distribution** is also seen in another distribution
called **the normal distribution**. There are two key differences between the
normal distribution and the binomial distribution.

The **binomial** distribution is **bounded**. That means that one end represents
0 heads and the other end represents all heads. That is, the distribution can
only range from 0 to n (where n is the number of coins that have been
flipped)---it is bounded at 0 and n. The **normal distribution**, however,
ranges from negative infinity to positive infinity. Additionally, for the
**binomial distribution**, the steps along the x-axis are **discrete**. That is,
you can have 0 heads, one head, two heads and so on, but you can have anything
in between---for example, it's not possible to have sequences of coin flips that
results in 1.5 heads. In contrast, the normal distribution is **continuous**.

The **normal distribution** is a mathematical abstraction, but we can use it as
a **model** of real-life frequency distributions. That is, we can use it as a
model of **populations** that are produced by certain kinds of natural
processes. Because normal distributions are unbounded and continuous, nothing,
in reality, is normally distributed. For example, it's impossible to have
infinity or negative infinity of anything. This is what is meant by an
**abstraction**. But natural processes can give rise to frequency distributions
that look a lot like normal distributions, which means that normal distributions
can be used as a model of these processes.


### Processes that produce normal distributions 

In the previous lecture, we saw how sequences of coin flips, and the bean
machine, give rise to a **binomial distribution**, but what processes give rise
to a **normal distribution**. To see how a natural process can give rise to a
normal distribution, let us play a board game! We won't play it for real, but
we'll *simulate* it. 

In this game, each player rolls the dice a certain number of times, and they
move the number of spaces indicated by the dice. Not that dissimilar to what
you'd do in a game of monopoly, or a similar board game! For example, if a
player rolled the die three times, and they got 1, 4, 3, then they wouldn't move
`r 1 + 4 + 3` (1 + 4 + 3 = `r 1 + 4 - 4`) spaces along the board. At the end of
one round of rolls we can take a look at how far from the start each player is.
And we can draw a histogram of this data.

In the simulation below, you can set the number of players and the number of
dice that they roll. We'll start off by just having 10 players, and each player
will roll the 1 die. 


<!-- TODO: Add the simulations in here -->

#### Processes that don't produce normal distributions

We won't cover other distribution shapes in much detail, but let us take a look
at an example of a process that doesn't produce a normal distribution. To do
this, we'll just modify the dice game in Box \@ref(fig:box5).

In Box \@ref(fig:box5), click the option that says **Multiply**. Doing so
changes the rules of the dice game so that a player's score is determined by
*multiply*ing together the values of their dice rolls. For example, under the
new rules, if a player rolled 1, 4, 3 then their score would be 12 (1 × 4 × 3 =
12). Now try clicking "Roll" to see the shape of the distribution. This new
distribution has an extreme **skew**. The vast majority of players have fairly
low scores, but a tiny minority of players have extremely high scores. When you
have a process that grows by multiplying, then you'll get a distribution that
looks like this.

In psychology, we won't study many processes that grow like this, but some
processes that grow like this will be very familiar to you. Think about
something like wealth. Wealth tends to grow by multiplying. For example, earning
interest or a return on investment of 10% means that the new value of the
investment is 1.10 times the original value. This feature of wealth growth
explains why, for example, in the UK, the top 10% of people control more wealth
than the bottom 50%.

<!-- TODO: Add the simulations in here -->


### Describing normal distributions

The normal distribution has a characteristic bell shape, but not all normal
distributions are identical. They can vary in terms of where they're centered
and how spread out they are. The mean ($\mu$) determines where the centre is,
and the standard deviation ($\sigma$) determines how spread out it is. Use
@exm-norm to explore the **normal distribution**.

::::{.callout-tip icon="false" appearance="simple"}

:::{#exm-norm}
#### Explore the normal distribution
:::

Use the sliders to make adjustments to the $\mu$ and $\sigma$ and see how the
plot of the normal distribution changes.

```{ojs}
//| echo: false
import {normal_plot_output} from "@ljcolling/distribution-shapes"
import {normal_sliders} from "@ljcolling/distribution-shapes"
import {normal_description} from "@ljcolling/distribution-shapes"
normal_plot_output
```

\
```{ojs}
//| echo: false
//| panel: input
normal_sliders
```

```{ojs}
//| echo: false
normal_description
```

::::

<!-- FIXME: Finish stuff on this, and include visualisation -->

<!-- TODO: 
And include stuff here about how we can change the mean and the sd, but the
overall shape remains the same. Also talk about how if something is normally
distributed then we know a certain "amount" of the distribution will lie within
the x deviations from the mean -->

### Describing departures from the normal distribution.

When we looked at the example of the game where a players score was based on
**multiplying** together the dice rolls, it produced a distribution that was
**skewed**.

**Skew** is a technical term that describes one way in which
a distribution can _deviate_ from a _normal distribution_. The _normal
distribution_ is **symmetrical**, but a **skew** distribution is not. A
left-skewed distribution has a longer _left tail_, and a **right-skewed**
distribution has a longer _right tail_. Use @exm-skew to explore
**skewness**.

::::{.callout-tip icon="false" appearance="simple"}

:::{#exm-skew}
#### Explore Skewness
:::

Use the slider to make adjustments to the skewness of the distribution. Slide it
to the *left* to add **left-skew** and slide it to the *right* to add
**right-skew**. See how the distribution changes.

```{ojs}
//| echo: false
import {skew_normal_plot_output} from "@ljcolling/distribution-shapes"
import {viewof skew} from "@ljcolling/distribution-shapes"
skew_normal_plot_output
```

\
```{ojs}
//| echo: false
//| panel: input
viewof skew
```

```{ojs}
//| echo: false
import {skew_desc} from "@ljcolling/distribution-shapes"
skew_desc
```

::::


<!-- FIXME: Add visualisation -->

Apart from **skew**, deviations from the **normal** distribution can occur when
a distribution either has fatter or skinnier **tails** than the normal
distribution. The _tailedness_ of a distribution is given by its **kurtosis**.
The kurtosis of a distribution is often specified with reference to the **normal
distribution**. In this case, what is being reported is **excess** kurtosis. A
distribution with **positive excess kurtosis** has a higher kurtosis value than
the normal distribution, and a distribution with **negative excess kurtosis**
has a lower kurtosis value than the normal distribution. Distributions with no
execess kurtosis are called **mesokurtic**, distributions with negative excess
kurtosis are called **platykurtic**, and distributions with positive excess
kurtosis are called **leptokurtic**. In your research methods courses, you
probably won't come across many distributions that have negative excess
kurtosis. However, the distribution that describes dice rolls is one such
distribution, and this will be discussed briefly later in this course. You will
encounter distributions with positive excess kurtosis more often. In particular,
the _t_-distributions, a distribution with positive excess kurtosis, will be
used in several of the statistical procedures that you will learn. Use @exm-kurt
to explore excess kurtosis.

::::{.callout-tip icon="false" appearance="simple"}

:::{#exm-kurt}
#### Explore kurtosis
:::

Use the slider to adjust the excess kurtosis. A standard normal distribution
with an excess kurtosis of 0 is shown for reference.

```{ojs}
//| echo: false
import {kurtosis_plot} from "@ljcolling/distribution-shapes"
import {viewof kurtosis_value} from "@ljcolling/distribution-shapes"
import {kurtosis_desc} from "@ljcolling/distribution-shapes"
kurtosis_plot
```

\
```{ojs}
//|echo: false
//| panel: input
viewof kurtosis_value
```

```{ojs}
//|echo: false
kurtosis_desc
```
::::


## Distributions and samples 

Now that we've covered samples, distributions, and populations, we're going to
start putting them all together. We saw that whenever we look at the
distribution of values where the values are produced by **adding up numbers** we
get something that looks like a normal distribution. And we saw that when we
worked out the **mean of a sample of data** we did this by **adding up all the
values** and then _dividing that number by the number of values_.

Let's say that we want to measure some phenomenon---for example, scores on some
standardised reading tests. We collect a sample of data from 50 children, and
then we work out the **mean of this sample**, by **adding up the 50 scores** and
then dividing this value by 50. Now let's say that instead of only collecting
one sample, we collect 100,000 samples. We work out the **mean** for _each
sample_. And we then draw a histogram of the means for our 100,000 samples. What
will this distribution look like? In our games, any time we added up numbers, as
long as we added enough numbers, and we had a large enough set of
players, then we'd get a normal distribution.

The same goes for our sample. As long as we add up enough numbers (our sample
size) and we have enough samples (think, number of players) then the
distribution of the means (the distribution of the players scores) will be
**normally distribution**. This simple observation is fairly **central** in
statistics. So much so that it's called **the central limit theorem**.

### A distribution of our samples

The **distribution** of some statistic---for example, the **mean**---that we see
when repeatedly sample from the population is called **the sampling
distribution**. That's a bit of a mouthful, so we'll try to unpack it a bit.

Let us go back to example where we were working out the mean of a series of
dice rolls. In that example, we saw how the mean of our samples moved around
from sample to sample, so sometimes it was closer to the true mean, and
sometimes it was further away. We can see this in the box below.

<!-- TODO: Add box -->

Now instead of looking at our sample means wiggle about from sample to sample,
we're just going to collect a lot of samples and then plot the distribution. We
can see that here:

<!-- TODO: Add box -->

<!-- TODO: Need to emphasise sample size where it starts to look normal -->

As we can see, the shape of the **sampling distribution of the mean** is
unsurprisingly a **normal distribution**. Other statistics have sampling
distributions too. For example, we could calculate the **variance** for each
sample, and we could plot the **sampling distribution of the variance**. The
sampling distribution of variance **won't** be normally distributed. Why?
Remember that whenever we add stuff up we get a normal distribution, and working
out a mean is just a special way of adding up numbers. When we work out a
**variance** we're not adding numbers up. If you look at the formula, you'll
notice that there's a $^2$ in there. That means we're doing some multiplying.
And we saw that when we were multiplying numbers instead of adding them, we got
a distribution that wasn't normally distributed.

<!-- NOTE: Maybe pull the variance stuff out -->


<!-- FIXME: Link to above somewhere -->
Because the sampling distribution of **the mean** is **normally distributed**
(assuming the sample size is large enough, see) it's centre and spread can be
described the same way as any other normal distribution. It will have a centre,
or **mean** that is _equal to the population mean_. And it will have a spread,
or **standard deviation** that is _proportional_ to the _sample size_ and the
_population standard deviation_.

### Standard error of the mean

The _standard deviation of the sampling distribution_ of the mean has a special
name. It is called the **standard error** of the mean. 
<!-- TODO: As we saw above, how spread out the individuals samples were was -->
<!-- determined by two things. First, how spread out the population was and, -->
<!-- two, the sample size of the samples. -->
The spread of these individual samples **is the standard error**.
Therefore, just as we saw above, we know that the **standard error**
will get smaller as the sample size goes up. And we know that the
**standard error** will go down as the **population standard deviation**
goes down. We can see how these two factors works together when we look
at the formula for the **standard error of the mean (SEM)** in @eq-sem.

$$\mathrm{SEM}=\frac{\sigma}{\sqrt{N}}$${#eq-sem}

<!-- TODO: Add explorer box -->

<!-- NOTE: Maybe something in here about the sampling distribution is useful -->

<!-- NOTE: --> next lecture (lecture 9) of Z score, and comparisons -->

