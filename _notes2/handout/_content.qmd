<!-- NOTE: From lecture 6 -->
This lecture is made up of two parts. **Part one** will introduce the concepts
of **populations** and **samples** and how they are related to one another. In
**Part Two**, we'll learn about **distributions** and about how natural
processes can give rise to particular kinds of distributions. In particular,
we'll learn about how natural phenomena can give rise to the **normal
distribution**, and we will lay some of the groundwork for understanding the
**sampling distribution**, which will be introduced in a later lecture. Finally,
at the end of **Part Two**, we'll try to put everything together, and we'll look
at the relationship between **samples**, **populations**, and **distributions**.

## Samples and populations

One of the key things that we want to do with _statistics_ is to make
**inferences** about **populations** from _the information_ we get from
**samples**. That is, we often want to make a judgement, or draw a conclusion,
about an aspect of the population when all we have access to is a sample.

We'll get to more formal definitions of _populations_ and _samples_ shortly, but
first, let's make things more concrete by introducing an example.

Let's say you're interested in the **average height** of **people in the UK**.
The "easy" way to find an answer to this question is to measure **all the people
in the UK** and then work out the **average height**. Doing this will give you
the exact answer to your question. But if you can't measure everyone in the UK,
then what do you do?

One option is to select a smaller group, or subset, of people from the UK. You
can then measure the height of people in this group, and then try to use this
information to figure out plausible values for the average height of people in
the UK.

In this example, the group (or groups) you're making claims about is the
population. You want to claims about **the average height** of **people in the
UK**. And the **sample** is a subset of this population---the smaller group of
people that you were eventually able to measure.

It's important to note that there isn't a **single** population. What counts as
the population will depend on the claim you're making. For example, let's say
I'm interested in testing the claim, "Do **people in East Sussex** show an
interference effect on the Stroop task?". Here the **population** would be
**people in East Sussex**. If, however, I want to make claims about **people in
general**, then the **population** might be **all living humans**. The
**sample** is always going to be a subset of the **population**.

:::{.callout-tip collapse="true"}


<!-- TODO: Try to add something about weird somewhere. -->

### Learn more about WEIRD samples

Researchers in psychology aren't always explicit about which population they're
making inferences about. One might assume it to be something like "humans in
general". But if this is the case, then the **samples** that psychological
scientists study should be a **subset** of _humans in general_. However, several
researchers have noted that the samples used in the vast majority of psychology
research tend to be rather **WEIRD**. That is, the samples used in most
published psychological science are drawn from **W**estern, **E**ducated,
**I**ndustrialized, **R**ich, and **D**emocratic (WEIRD) societies [@weird]. The
preponderance of WEIRD samples implies that researchers either assume that there
is little variation across different population groups when it comes to the
phenomena they're studying, or that these samples are as representative of
**humans in general** as any other group.

But is this true? Answering this question is difficult because it would
plausibly be conditional on the nature of the research question or phenomenon
being studied. For example, it _seems_ plausible that there would be little
variation across human populations when it comes to phenomena like low-level
auditory and visual processing. However, it also _seems_ plausible to expect to
see more variation when it comes to phenomena like moral reasoning, where
cultural practices may play a more prominent role. However, it is an empirical
question---that is, it is a question that can only be answered by looking at the
actual data. Unfortunately, the data required to answer these questions are
sparse. However, the data that does exist suggests that there is probably more
variation than people would expect, even when it comes to _low-level_ phenomena
like visual perception[@weird]. In the past few years, there has been some
movement to try to make samples in psychological research more diverse, but
there is still a long way to go and, therefore, this is an issue worth bearing
in mind.

:::

### The relationship between samples and populations

Let's assume that we have explicitly defined our **population** (for example, as
_all people in the UK_) and we've collected a **sample** by taking measurements
from a **subset** of this population. What is the relationship between this
sample and the population from which it was drawn?

The _sample_ should **resemble** the _population_ in some way. Most often we're
interested in making **inferences** about **averages**---for example,
**average** performance or **average** score on a measure or a test. In the
example I introduced earlier, we were interested in **average height**. But we
might also be interested in things a difference between two averages---for
example, whether there is a difference in **average depression levels** before
and after some intervention, or whether **average response times** are different
between the two conditions of a Stroop task. Ideally then, the **average
height** of our **sample** should **resemble** the **average** height of our
**population**, or the _average_ _response_ _time_ _difference_ in our
**experimental sample** should _resemble_ the _average response time difference_
in our population. But if we don't know the **average** of our **population**,
then how will we know whether our **sample** _resembles_ it?

In short, **we can't know for sure**. But we can think of a couple of things
that will **influence** the relationship between our **sample** and the
**population**. To figure out what these are, let's do a thought experiment and
think of some **extreme cases**.

First, consider the case where **all the members** of a _population_ are
**identical**. If this were the case, then our **sample** will have an
**identical** average to the population. The height of one person would be the
same as the average height of two people, which would be the same as the average
height of 100 people, which would be same as the average height of the
population because people only come in one height. But if the **members** of the
**population** are all **different** from one another, then there is no
guarantee that the **sample's average** will **resemble** the **population's
average**.

The second extreme scenario is if our sample is **very large**. Let's say that
it is so large that it includes **all the members of the population**. If this
were the case, then, by definition, our **sample average** would be
**identical** to the **population average**. However, if our sample is smaller
than the entire population, then once again, there is no guarantee that the
**sample's average** will **resemble** the **population's average**.

Based on this reasoning, we can say that two things will influence whether your
_sample_ resembles your _population_. These are 1) the amount of **variation**
in our population, and 2) the **size** of our sample.

Importantly, however, and barring the extreme cases above, for **any particular
sample** we won't know whether it **resembles** the population or not, because,
remember, we don't know the average of the population. Instead, we should think
about these two factors as influencing **how likely** it is for samples to
resemble the population. But what does this mean?

One way to think about this is in terms of **repeatedly** taking samples from
the same population. For example, if we take a large sample from the
population---large, but not so large as to include the entire population---then
we can't say that our **particular** sample will resemble the population. But if
we take many samples (of that size), then we can say that **on average** those
samples will be closer to the population than would be the case for a collection
of smaller samples.

The same reasoning applies to **variation in the population**. If there is
**less variation** in the **population**, then the samples drawn from that
population will tend to be closer to each other and closer to the population
average. But again, we won't be able to say whether **a particular sample** has
an **average** that is close to the population average.

Of course, sample size and population variation exert their influence together.
If we want our sample averages to be close to the population average, then we
need samples that are **big enough**, but what counts as **big enough** will
depend on the **population variation**. Therefore, knowing whether our sample is
big enough depends on knowing the population variation. Unfortunately, we don't
know this; however, there is a way to estimate it. But that's a topic for
another lecture.

:::{.callout-tip}

### Explore more

In this box you can explore the relationship between samples and populations. In
particular, you can see how two factors (1) **sample size** and (2) **population
variability** have an impact on how closely samples will _on average_ resemble
the population.

```{ojs}
//| echo: false
md`The top plot shows the outcome of ${sample_size6} dice throws of a 6-sides dice.

Each dot represents the result of one throw. So one circle over the **1** means that
one of the throws showed a **1**. Two circles of the **5** means
that two of the throws showed a **5**. And so on. Because it's a 6-sided dice,
each throw can show any number from 1 to 6.

Below this is a plot showing each the **average** of each sample. This is 
just calculated by adding up all the numbers shows on the dice and 
dividing it by ${sample_size6}. The **population average** is also marked on
this plot, together with lines showing ± 1 from the **population average**.
Notice that the **sample average** is very rarely the same as the 
**population average**. Instead, it bounces around. Sometimes it's 
higher, and sometimes it's lower. Use the slider to add adjust the
**sample size**. What happens when the sample size is small---for example, 1?
What happens when the sample size is big--for example, 100?

`
```

```{ojs}
//| echo: false
import {doplot} from "@ljcolling/samples"
import {viewof sample_size6, viewof sample_size20} from "@ljcolling/samples"

```

```{ojs}
//| echo: false
//| fig-align: center
doplot(6)
```

```{ojs}
//| echo: false
viewof sample_size6
```

```{ojs}
//| echo: false
md`The next two plots are the same as above, expect they show the outcome of
${sample_size20} dice throws of a 20-sided dice. A 20-sided dice can show
any value between 1 and 20.

You can also see a plot showing each the **average** of each sample. This is 
just calculated by adding up all the numbers shows on the dice and 
dividing it by ${sample_size20}. The **population average**,
together with lines showing ± 1 from the **population average** are also shown.
Use the slider to add adjust the **sample size**. What happens when the 
sample size is small---for example, 1?
What happens when the sample size is big--for example, 100?

`
```

```{ojs}
//| echo: false
//| fig-align: center
doplot(20)
```

```{ojs}
//| echo: false
viewof sample_size20
```

::::

The 20-sided dice represents a population that has a lot of variability. The
individuals in the population (the dice throws) can be any number between 1
and 20. The 6-sided dice represents a population that has only a little
variability. The individuals in the population (the dice throws) can only be a
number between 1 and 6.

By looking at the running averages (the bottom plot for each dice) we can tell
whether the samples _on average_ resemble the population. If most of the
averages fall very close to the population average then we can say the samples
_on average_ resemble the population.

Notice how for a given sample size (say 5 for each dice) the average of the
samples from the low variability dice (6 sided) do a better job of _on average_
resembling the population average. For the high variability dice (20 sided), the
averages of the same size samples do a poorer job of _on average_ resembling the
population average. The important part here in _on average_. Individual sample
averages may do a good job of resembling the population average no matter what
the sample size is. This demonstration tells use the **population variability**
is one factor that influences whether the averages of our samples will _on
average_ resemble the average of our population.

Let's say that our sample averages do a good job of resembling the population
average if the _almost all_ the sample averages fall within ± 1 of the
population average (the lines marked on the plot). Try adjusting the _sample
sizes_ for the two dice? What smallest value that will cause _almost all_ the
sample averages to land between the marked lines?[**Hint** If you can't work it
out try setting the sample size to **16** for the 6-sided dice and to **180**
for the 20-side dice.]{.aside} Notice how for high variability dice this value
must be far higher? This demonstration tells use the **sample size** is the
second factor that influences whether the averages of our samples will _on
average_ resemble the average of our population.

<!-- DIV -> style="background:#EBF5E8;margin-left: -16px;margin-right: -16px;padding:12px 5px 12px 10px;border:#DADEE2;border-width:1px;border-style:solid;margin-top: -20px;border-top-left-radius: 5px;border-top-right-radius: 5px;" -->

<!-- H$ -> style="border:solid; border-width:1px;border-color: #DADEE2;padding-left: 15px;padding-right: 15px;border-top-left-radius: 10px;padding-bottom: 15px;padding-top: -8px;margin-top: 27px;" -->

```{r}
(sqrt((20^2 - 1) / 12) / sqrt(180)) * qnorm(.99)
(sqrt((6^2 - 1) / 12) / sqrt(16)) * qnorm(.99)
```

```{r}
rdu<-function(n,k) sample(1:k,n,replace=T)
```

```{r}
side6_mean <- function(d){
 n <- 1#5
 x <- rdu(n, 6) 
 mean(x)
}
lapply(X = 1:10000, side6_mean) |> unlist() |> sd() -> side6_se
side20_mean <- function(d){
 n <- 1#57
 x <- rdu(n, 20) 
 mean(x)
}
lapply(X = 1:10000, side20_mean) |> unlist() |> sd() -> side20_se
qnorm(0.99, lower.tail = T) * side6_se
qnorm(0.99, lower.tail = T) * side20_se
```
```{ojs}
//| echo: false
import {skew_normal_plot_output, viewof alpha_value} from "@ljcolling/distribution-shapes"
```

```{ojs}
//| echo: false
skew_normal_plot_output
```

```{ojs}
//| echo: false
viewof alpha_value
```

```{r}
#| echo: false
#| include: false

require(ggplot2)
require(cowplot)
require(tidyr)
require(dplyr)
require(plotly)
salary <- readr::read_csv(here::here("data","world_salary.csv"))
USD <- scales::dollar_format(suffix = " USD")
ggplot2::theme_set(cowplot::theme_cowplot())
```

In [Lecture 4]() we started talking about how quantitative methods deal with
**measurement**---that is, putting numbers to things. In today's lecture we're
going to start talking about what we actually do with these numbers. The first
thing we'll want to do is to describe these numbers, and today we'll cover two
main ways for describing your set of numbers.

## Describing numbers with numbers

Let's say we're interested in how well people perform at

<!-- fill something in here -->

```{r}
#| echo: false
#| include: false
set.seed(123)
data_points <- rnorm(n = 10, mean = 170, sd = 13) |> round(0)
```

We've taken a bunch of measurements, and our measurements are as follows:
`r data_points`. The first thing we'll want to do before we can do anything
useful with these numbers is to come up with a way for succinctly describing
them, or **summarising** them.

The first kind of summary that we might want is to describe the **typical
value**. There's a few different things that we could mean by the **typical
value**, so we go through each of them in turn.

## Measures of central tendency

What we mean by the **typical value** is not always clear. For example, in
@fig-income we can see the average annual salary (in US dollars) for a set of
`r dim(salary)[1]` countries. Each bar of the plot represents the number of
countries in the given salary bracket ($0--$10k, $10-$20k, ...).

```{r}
#| echo: false
#| warning: false
#| message: false
#| fig.align: center
#| fig.height: 3
#| label: fig-income
#| fig.cap: "National average annual salary [source: https://www.worlddata.info/average-income.php]"
(salary |> ggplot(aes(x = yearly)) +
  geom_histogram(binwidth = 1e4, boundary = 0, fill = "white", color = "black") +
  labs(x = "Annual salary in USD", y = "Count")) |>
  plotly::ggplotly(tooltip = "count") |>
  plotly::config(displayModeBar = F)
```

From the plot we can see that there are a lot of countries where the average
annual salary is less than $30,000 USD. There are also a handful of countries
where the average annual salary is more than $100,000 USD. What would you
consider the _most typical_ annual salary? The bracket with the most countries
in it? If so, that would mean the most typical salary on the planet is between
$0 and $10,000 USD per year. Or maybe we should pick the value where half the
countries have a lower average salary and half the countries have a higher one?
Choosing this option leads to an estimate of the most typical salary for
`r salary$yearly |> median() |> USD()` per year.

As you can see, depending on how we define _the most typical value_, we get
different answers. We'll cover the three main ways of defining the _typical
value_ or _average_.

<!-- make sure I use average as synonym -->

### Mode

The **mode** is a term that refers to the **most frequent value** in a set of
measurements. This is the kind of _average_ we discussed above when we said the
most typical salary on the planet is between $0 and $10,000 USD a year. The
easiest way to spot the **mode** is just to draw a plot like the one we did in
@fig-income and then just look for the tallest bar.

A set of numbers can have one or more modes. If it only has one mode, then it is
said to be **unimodal**. **Bimodal** means it has two modes. If it has three or
more modes, then it is usually called **multimodal**.

<!-- Maybe include some plot here -->

:::{.callout-important}

The mode is the only definition of _typical value_ that works for data that is
measured at the **nominal**/**categorical** level.

When it comes to truly **continuous** variables, such as height, a set of
measurements is likely to be multimodal. Why? Because no two people are exactly
the same height, so each person's height is the mode of the distribution because
there's only measurement that has exactly that value. For this reason, the mode
is rarely used for continuous variables measured at the **interval** or
**ratio** levels.

Average salary is continuous variable, but we turned it into a discrete variable
placing countries into discrete categories.

:::

### Median

The **median** is the second kind of average we talked about [above](); the
middle value where half the measurements are above that value and half the
measurements are below. To find the median, we first need to sort our data.

@fig-sorted

#| echo: false
```{r}
#| label: fig-sorted
#| fig.cap: |
#|  Average national salary per country sorted from lowest to highest (Hover 
#|  over the bars to see the name of the country and the value).
(salary |>
  dplyr::arrange(yearly) |>
  dplyr::mutate(index = 1:n()) |>
  dplyr::mutate(
    color =
      case_when(
        index == (1:78 |> median() |> floor()) ~ "red",
        index == (1:78 |> median() |> ceiling()) ~ "red",
        TRUE ~ " grey"
      )
  ) |>
  dplyr::mutate(text = glue::glue("{country} (${round(yearly/1000,2)}k)")) |>
  ggplot(aes(x = index, y = yearly, text = text, fill = color)) +
  ggplot2::scale_fill_manual(
    name = NULL,
    values = c("#454A60", "#ff4500"), labels = NULL, guide = "none"
  ) +
  labs(y = "Annual salary in USD", x = "Country") +
  scale_x_continuous(labels = NULL, breaks = NULL) +
  scale_y_continuous(labels = function(x) paste0(x / 1000, "k")) +
  ggplot2::theme(
    axis.text.x = element_blank(), axis.ticks.x = element_blank(),
    legend.position = "none", legend.title = element_blank(),
    panel.grid.major.y = element_line(color = "grey60"),
    axis.line.x = element_blank(), axis.line.y = element_blank()
  ) +
  geom_col() +
  NULL
) |>
  plotly::ggplotly(tooltip = "text") |>
  plotly::config(displayModeBar = F)
```

:::{.callout-important}

To be able to calculate a meaningful median, the variable must be measured on
**at least the ordinal level**.

:::

### Mean

<!-- Here I will need to put a quick distinction between samples -->
<!-- and populations -->

<!-- But I'll have to make a not that we this distinction might not -->
<!-- seem super important for now, but it'll become more important -->
<!-- when we talk about spread -->

### Mean vs Median

```{ojs}
//| echo: false
import {viewof clear, viewof ave_select, box, mutable inputArray} from "@ljcolling/samples"

box(inputArray)
```

```{ojs}
//| echo: false
//| panel: input
viewof clear
viewof ave_select
```

```{ojs}
import { box2 } from "./samples.qmd";
```

```{ojs}
box2(inputArray)
```

## Measures of spread

If you look at @fig-hist-width you'll see two data sets that are centred at the
same value but have very different amounts of variability. Both sets of data
have a mean of 0. But, as you can see, the values of one are spread out much
more widely than the values of the other.

```{r}
#| echo: false
#| warning: false
#| label: fig-hist-width
#| fig-cap: Histogram of two distributions with equal means but different spreads. *N = 10,000* in each case.
dist_narrow <- rnorm(10000, mean = 0, sd = 1)
dist_wide <- rnorm(10000, mean = 0, sd = 3)
tibble::tibble(
  narrrow = dist_narrow,
  wide = dist_wide
) |>
  tidyr::pivot_longer(1:2) |>
  ggplot2::ggplot(aes(
    x = value,
    # group = name,
    fill = name,
    alpha = name
  )) +
  geom_histogram(color = "black", position = "identity") +
  scale_alpha_manual(values = c(1, 0.7), guide = "none") +
  scale_fill_manual(values = c("seagreen", "blue"), guide = "none") +
  NULL
```

This is why, in addition to measures of central tendency, we also need measures
that tell us about the spread, or _dispersion_ of a variable. Once again, there
are several measures of spread available, and we'll talk about five of them:

1. Range
2. Interquartile range
3. Deviation
4. Variance
5. Standard deviation

<!-- Add in a new visualisation here and a preview paragraph -->

### Range

The **range** of a variable is simply the distance between its smallest and
largest values. For example, if we gather a sample of 100 participants and the
youngest one is 17 years old, and the oldest one is 67 years old, then the range
of our age variable in this sample if 67 - 15 = `r 67 - 17` years.

Checking the range of a variable can tell us something about whether our data
makes sense. Let's say that we've run a study examining reading ability in
primary school age children. In this study, we've also measured the ages of the
children. If the range of our age variable is, for example, 50 years, then that
tells us that we've measured _at least_ one person that is not school age.

Beyond that, the range doesn't tell us much of the information we'd usually like
to know. This is because the range is _extremely_ **sensitive to outliers**.
What this means is that it only takes one extreme value to inflate the range. In
our school example, it might be that all but one of the people measured is
actually in the correct age range. But the range alone cannot tell us if this is
the case.

### Interquartile range

A slightly more useful measure than the range is the **interquartile range** or
IQR. The IQR is the distance between the 1st and 3rd quartiles of the data.
Quartiles, like the name suggests, are created by splitting the data into four
chunks where each chunk has the same number of observations. Or put another way,
the median splits the data into two where half the observations are higher than
the median and half the observations are lower than the median. Quartiles are
created by taking each of these halves and splitting them in half again. The
range covered by the middle two 25% chunks is the IQR. It is the range that
covers the middle 50% of the data.

The benefit of the IQR over a simple range is that the IQR is not sensitive to
occasional extreme values. This is because the bottom 25% and the top 25% are
discarded. However, by discarding these data, the IQR provides no information
about how spread these outer areas are.

<!-- give some information about the visualisation. -->

```{r}
values <- salary$yearly
qs <- quantile(values, c(0, 0.25, 0.5, 0.75, 1))
```

### Deviation

Both the range and the IQR work by looking at the distance between only two
observations in the entire data. For the range, it's the distance between the
minimum point and the maximum. For the IQR, it's the distance between the
midpoint of the upper half and the midpoint of the lower half.

As a result, both these measures tend to be fairly course-grained. To get a more
fine-grained measure of the spread we could look at each data point and
calculate how far it is away from the typical value. Usually, this is done by
looking at how far away each data point is from the **mean** of the dataset.
This is known as the deviation.

This is shown in @eq-dev, below:

$$ \definecolor{point}{RGB}{114,0,172} \definecolor{every}{RGB}{46,139,87}
\definecolor{average}{RGB}{0,0,206} \color{point} x_{\color{every} i}
\color{black} - \color{average} \bar{x} $$ {#eq-dev}

Take the <font color="#7300ac">value</font> of <font color="#2E8b57">a
particular point</font> and <font color="black">subtract</font> it from
<font color="#0000ce">the average</font>.

Because we are calculating this for every data point there will be as many
deviations as we have values for our variable. To get a _single measure_, we'll
have to perform another step.

One thing we could try doing is just to add up the numbers. But this won't work.
To see why, try adding a few points below. Add you add the points take a look at
the table below. The table has a column with all the points, and all the
deviations from the mean. Below the table you can see what happens when you add
up all those deviation values. What do you notice?

As you can see, they add up to zero. Because the mean is our midpoint, the
distances for all the points higher than the mean cancel out the distances for
all the points lower than the mean.

We can get around this problem by taking the square of the deviations before
adding them up. Squaring a number will turn a negative number into a positive
number, so when we add up all the numbers they'll no longer add up to 0.

<!-- Why squaring and no absolute values -->

<!-- Mathematically blah blah -->

$$ \definecolor{SS}{RGB}{114,0,172} \definecolor{=}{RGB}{0, 0, 0}
\definecolor{sum}{RGB}{206, 69, 0} \definecolor{dev}{RGB}{58, 206, 0}
\definecolor{sq}{RGB}{46,23,255} \definecolor{every}{RGB}{143, 0, 172}
SS=\sum{}(x - x)^2 $$

$$ \color{black} \mathrm{SS} \color{=}= \color{sum} \sum^{\color{every}
N}_{\color{every} i=1}{\color{dev} (x_{\color{every} i} \color{dev} -
\bar{x})\color{sq}^2} $$

$$x^2$$

<font color="#2e17ff">Take the square</font> of the
<font color="#3ace00">deviation from the mean</font> for
<font color="#8f00ac">every value</font> and <font color="#ce4500">add them
up</font>.

Adding up all the numbers, however, leaves us with another problem.

The sum of the squared deviations gets bigger with bigger samples. That's not
good because even big samples can have small amount of variation, while smaller
samples can vary a lot. We want our measure of spread to be able to capture
this. Get around this, we'll move on to our next measure of spread.

### Variance

You next measure of spread is the **variance**. The **variance** gets around the
problem of the measure of spread just getting bigger when we have bigger
datasets. And it gets around this problem by just working out the _average
squared deviation_.

<!-- A note here about population vs sample -->

### Standard deviation

Variance is a good measure of dispersion and it is widely used. In fact, most of
the statistical tests we will be using are based around variance. However, there
is one minor inconvenience about this measure when it comes to interpretability:
it's measured in _squared units_! For example, if salary is measured in US
dollars, _s^2^_~salary~ is expressed in USD^2^, whatever those may be.

Fortunately, the solution to this problem is easy: we can simply take the square
root of variance. This is called the **standard deviation**.

Just like with variance, there is **population** standard deviation, $\sigma$
and the **sample** standard deviation, $s$ or $SD$.

<!-- show the calculations -->

:::{.callout-note} 

You can think of _SD_ as a measure of the differences of a
set of scores from their mean. If variance is the mean _squared_ deviation in
the variable, standard deviation is the **mean deviation**.

:::

## Samples and populations

One of the key things that we want to do with _statistics_ is to make
**inferences** about **populations** from _the information_ we get from
**samples**. That is, we often want to make a judgement, or draw a conclusion,
about an aspect of the population when all we have access to is a sample.

We'll get to more formal definitions of _populations_ and _samples_ shortly, but
first, let's make things more concrete by introducing an example.

Let's say you're interested in the **average height** of **people in the UK**.
The "easy" way to find an answer to this question is to measure **all the people
in the UK** and then work out the **average height**. Doing this will give you
the exact answer to your question. But if you can't measure everyone in the UK,
then what do you do?

One option is to select a smaller group, or subset, of people from the UK. You
can then measure the height of people in this group, and then try to use this
information to figure out plausible values for the average height of people in
the UK.

In this example, the group (or groups) you're making claims about is the
population. You want to claims about **the average height** of **people in the
UK**. And the **sample** is a subset of this population---the smaller group of
people that you were eventually able to measure.

It's important to note that there isn't a **single** population. What counts as
the population will depend on the claim you're making. For example, let's say
I'm interested in testing the claim, "Do **people in East Sussex** show an
interference effect on the Stroop task?". Here the **population** would be
**people in East Sussex**. If, however, I want to make claims about **people in
general**, then the **population** might be all **living** humans.

### Theoretical populations

So far in our talk of populations we've only really be thinking about
populations as the **set of actually existing things** that we can take our
sample from---for example, all **living** humans. But populations don't have to
be sets of actually existing things. Instead, they can be the **set of possible
things** from which our samples can be drawn. This might seem a little
confusing, so an example might help.

Let's say we want to collect a sample of 6 coin flips. To collect our
**sample**, we take a coin and _flip it 6 times_ and count up the number of
heads and tails and from this we could work out, for example, how many _heads
are typically seen when flipping the coin 6 times_.

So that's our **sample**, but what is our **population**? One way to think of
our population is as the **set of possible outcomes (numbers of heads) that
would occur if we flipped the coin 6 times**. It turns out that can actually
work this out.

To work it out we would do something like the following:

<!-- do binimials -->

We can work it out because we know something about the **process that gives rise
to our data**. Although we might not be able to clearly specify the process that
gives to the data in a Stroop task, there will still be some **data generating
process**, which we can think of as the **population** we're sampling from.

:::{.callout-note}

In later years you'll learn about something called the **null hypothesis**. The
**null hypothesis**, in simple terms, says that the **data generating process**
for two sets of observations is same. For example, it might say that the _data
generating process_ that gives rises to people's responses on the Stroop task is
the same for the **congruent condition** and the **incongruent condition**.
You'll learn how to perform statistical tests that tell you whether the set of
data that you've actually collecting is **surprising** or **unsurprising** if
you were to assume that _two_ **data generating processes** are actually the
same.

:::

### The relationship between samples and populations

Let's assume that we have explicitly defined our **population** (for example, as
_all people in the UK_) and we've collected a **sample** by taking measurements
from a **subset** of this population. What is the relationship between this
sample and the population from which it was drawn?

The _sample_ should **resemble** the _population_ in some way. Most often we're
interested is knowing something like: "_What is the typical value (i.e., the
mean) of the population?"_ In the example I introduced earlier, we were
interested in **average height**. But we might also be interested in things a
difference between two averages---for example, whether there is a difference in
**average depression levels** before and after some intervention, or whether
**average response times** are different between the two conditions of a Stroop
task. Ideally then, the **average height** of our **sample** should **resemble**
the **average** height of our **population**, or the _average_ _response_ _time_
_difference_ in our **experimental sample** should _resemble_ the _average
response time difference_ in our population. But if we don't know the
**average** of our **population**, then how will we know whether our **sample**
_resembles_ it?

In short, **we can't know for sure**. But we can think of a couple of things
that will **influence** the relationship between our **sample** and the
**population**. To figure out what these are, let's do a thought experiment and
think of some **extreme cases**.

First, consider the case where **all the members** of a _population_ are
**identical**. If this were the case, then our **sample** will have an
**identical** average to the population. The height of one person would be the
same as the average height of two people, which would be the same as the average
height of 100 people, which would be same as the average height of the
population because people only come in one height. But if the **members** of the
**population** are all **different** from one another, then there is no
guarantee that the **sample's average** will **resemble** the **population's
average**.

The second extreme scenario is if our sample is **very large**. Let's say that
it is so large that it includes **all the members of the population**. If this
were the case, then, by definition, our **sample average** would be
**identical** to the **population average**. However, if our sample is smaller
than the entire population, then once again, there is no guarantee that the
**sample's average** will **resemble** the **population's average**.

Based on this reasoning, we can say that two things will influence whether your
_sample_ resembles your _population_. These are 1) the amount of **variation**
in our population, and 2) the **size** of our sample.

Importantly, however, and barring the extreme cases above, for **any particular
sample** we won't know whether it **resembles** the population or not, because,
remember, we don't know the average of the population. Instead, we should think
about these two factors as influencing **how likely** it is for samples to
resemble the population. But what does this mean?

One way to think about this is in terms of **repeatedly** taking samples from
the same population. For example, if we take a large sample from the
population---large, but not so large as to include the entire population---then
we can't say that our **particular** sample will resemble the population. But if
we take many samples (of that size), then we can say that **on average** those
samples will be closer to the population than would be the case for a collection
of smaller samples.

The same reasoning applies to **variation in the population**. If there is
**less variation** in the **population**, then the samples drawn from that
population will tend to be closer to each other and closer to the population
average. But again, we won't be able to say whether **a particular sample** has
an **average** that is close to the population average.

Of course, sample size and population variation exert their influence together.
If we want our sample averages to be close to the population average, then we
need samples that are **big enough**, but what counts as **big enough** will
depend on the **population variation**. Therefore, knowing whether our sample is
big enough depends on knowing the population variation. Unfortunately, we don't
know this; however, there is a way to estimate it. But that's a topic for
another lecture.

<!-- Explore more about sample -->

The 20-sided dice represents a population that has a lot of variability. The
individuals in the population (the dice throws) can be any number between 1
and 20. The 6-sided dice represents a population that has only a little
variability. The individuals in the population (the dice throws) can only be a
number between 1 and 6.

By looking at the running averages (the bottom plot for each dice) we can tell
whether the samples _on average_ resemble the population. If most of the
averages fall very close to the population average then we can say the samples
_on average_ resemble the population.

Notice how for a given sample size (say 5 for each dice) the average of the
samples from the low variability dice (6 sided) do a better job of _on average_
resembling the population average. For the high variability dice (20 sided), the
averages of the same size samples do a poorer job of _on average_ resembling the
population average. The important part here in _on average_. Individual sample
averages may do a good job of resembling the population average no matter what
the sample size is. This demonstration tells use the **population variability**
is one factor that influences whether the averages of our samples will _on
average_ resemble the average of our population.

Let's say that our sample averages do a good job of resembling the population
average if the _almost all_ the sample averages fall within ± 1 of the
population average (the lines marked on the plot). Try adjusting the _sample
sizes_ for the two dice? What smallest value that will cause _almost all_ the
sample averages to land between the marked lines?[**Hint** If you can't work it
out try setting the sample size to **16** for the 6-sided dice and to **180**
for the 20-side dice.]{.aside} Notice how for high variability dice this value
must be far higher? This demonstration tells use the **sample size** is the
second factor that influences whether the averages of our samples will _on
average_ resemble the average of our population.

<!-- Note, here I'm going to have to have to talk about on average -->
<!-- resembling, meaning on average closer. -->

<!-- Now move on to the next lecture to talk about distributions -->

<!-- -- process the give rise to data --  -->

Before we talk more about the **sampling distribution** let us first talk about
**distributions** in general, and the processes that give rise to them. We'll
start off with the simplest distribution, the **binomial distribution** before
moving on to the **normal distribution**.

<!-- probably need to trim down the bit where I talk about coin flips -->
<!-- earlier -->

### The binomial distribution

To understand what the **binomial distribution** is, and where it comes from
we'll do a little _thought experiment_. In our thought experiment, we'll take a
coin, and we'll flip it. When we flip a coin, one of two outcomes is possible.
Either the coin will land showing heads, or it will land showing tails. We can
say that there are two possible events or two possible sequences of events
(_sequences_ will make more sense when we add more coins) that can happen when
we flip a coin.

Now let's think of each of the sequences and count up how many heads are showing
in each. In the first sequence, where the coin lands showing tails, no heads
occur. In the second sequence, where the coin lands showing heads, there is one
head. Therefore, we have one sequence that produces 0 heads, and one sequence
that produces 1 head. And those are all the possible sequences.

But now let's make it more complicated. Let's flip two coins. Now there's a
greater number of possible sequences. We can list them:

1. The first coin shows heads, and so does the second (HH),
2. The first coin shows heads and the second shows tails (HT)
3. The first coin shows tails and the second shows heads (TH)
4. and the first coins shows tails and the second shows tails (TT)

Therefore, there are four possible sequences. Let's count up the **number of
sequences** that lead to 0 heads, one head, two heads, etc. If we do this, we'll
see that one sequence leads to 0 heads (TT). Two sequences lead to 1 head (HT,
and TH). And one sequence leads to 2 heads (HH).

Let's now add more coins. Things will get trickier from here because the number
of sequences rapidly goes up. With three coins, there would be eight possible
sequences, and with four coins, there would be 16 possible sequences. Figuring
out the number sequences, and the nature of the sequences (whether they produce
0 heads, one head, two heads, etc.) quickly becomes difficult. To make things
easier, we'll draw a plot. First, we'll draw a plot to trace out the sequences.
We'll use different coloured dots to indicate heads and tails. We can do this in
the form of the branching tree diagram shown in Box \@ref(fig:box2). Once we've

<!-- FIXME: Update the references -->

visualised the sequences, it's easy to count up how many sequences result in 0
heads, one head, two heads etc. We can put our counts on another plot. For this,
we'll make a frequency plot or histogram. On the x-axis, we'll have the number
of heads. And on the y-axis, we'll have the count of how many sequences result
in that number of heads. This frequency plot is also shown in

<!-- TODO:: Add in a visualisation here about coin flips -->

You can adjust the slider in Box \@ref(fig:box2) to change the number of coins
you want to flip. Increasing the number of coins increases the number of
possible sequences, and it changes the number of ways of getting one head, two
heads, three heads and so on changes (however, there's always only one way to
get 0 heads and one way to get all heads). Notice that as you adjust the slider
and add more and more coins, the frequency plot takes on a characteristic shape.
You can mathematically model the shape of this plot using a **binomial
distribution**.

In our coin flipping example, we created this shape by counting up the number of
sequences that produced various quantities of heads. But if we look around at
**natural processes**, we'll see that this shape occurs often.

One natural process that gives rise to this shape is the "bean machine"[^1]. In
a bean machine, small steel balls fall from the top of the device to the bottom
of the device. On their way down, they bump into pegs. When one of the balls
hits a peg, it has a roughly equal chance of bouncing off to the left or the
right, not unlike a coin which has a roughly equal chance of landing heads up or
tails up. At the bottom of the device are equally-spaced bins for collecting the
balls. If enough balls are dropped into the device, then the **distribution** of
balls across the bins will start to take on the shape of the **binomial
distribution**. Very few balls will be at the very edges, because this would
require the balls to bounce left or right every time.

Similarly, very few sequences of coin flips result in large numbers of heads or
large numbers of tails. The greatest number of balls are seen in the bins near
the middle. The balls that land here have bounced left and right a roughly equal
number of times. Again a similar pattern can be seen with the coin flips.

In Box \@ref(fig:box3), I've included a computer simulation of a bean machine.
Press the Start button to display the bean machine and watch the balls drop.
Press Replay to drop more balls.

<!-- TODO: Add in an actual video of a bean machine -->

### The normal distribution

The shape seen in the **binomial distribution** is also seen in another
distribution called **the normal distribution**. There are two key differences
between the normal distribution and the binomial distribution.

The **binomial** distribution is **bounded**. That means that one end represents
0 heads and the other end represents all heads. That is, the distribution can
only range from 0 to n (where n is the number of coins that have been
flipped)---it is bounded at 0 and n. The **normal distribution**, however,
ranges from negative infinity to positive infinity. Additionally, for the
**binomial distribution**, the steps along the x-axis are **discrete**. That is,
you can have 0 heads, one head, two heads and so on, but you can have anything
in between---for example, it's not possible to have sequences of coin flips that
results in 1.5 heads. In contrast, the normal distribution is **continuous**.

The **normal distribution** is a mathematical abstraction, but we can use it as
a **model** of real-life frequency distributions. That is, we can use it as a
model of **populations** that are produced by certain kinds of natural
processes. Because normal distributions are unbounded and continuous, nothing,
in reality, is normally distributed. For example, it's impossible to have
infinity or negative infinity of anything. This is what is meant by an
**abstraction**. But natural processes can give rise to frequency distributions
that look a lot like normal distributions, which means that normal distributions
can be used as a model of these processes.


<!-- NOTE: THIS WILL COME LATER / -->
<!-- #### Describing the normal distribution

Before we see how natural processes can give rise to the normal distribution,
let us take a look at one. Box \@ref(fig:box4) shows an example of a **normal
distribution**. Two parameters can be used to describe the normal distribution.
The location parameter denoted μ describes where the distribution is centred.
The scale parameter denoted σ describes the width of the distribution. When you
learn how to represent populations with distributions, then these two parameters
will correspond to the population average and population variation,
respectively.
-->
<!-- NOTE: This is just the mean and starndard deviation we talked about  -->
<!-- in the *previous lecture* -->

<!-- TODO: Add visualisation -->

#### Processes giving rise to the normal distribution

To see how natural processes can give rise to the normal distribution let us
take a look at a simple simulation. The simulation in

<!-- TODO: Referenes -->

In this game, each player rolls the dice a certain number of times, and they
move the number of spaces indicated by the dice. Not that dissimilar to what
you'd do in a game of monopoly, or a similar board game! For example, if a
player rolled the die three times and they got 1, 4, 3, then they wouldn't move
`r 1 + 4 + 3` (1 + 4 + 3 = `r 1 + 4

- 4`) spaces along the board. At the end of one round of rolls we can take a
  look at how far from the start each player is. And we can draw a histogram of
  this data.

<!-- TODO: Simulation and then add details about sample size -->

It's not only dice rolling a 6-sided dice that will give rise to a normal
distribution. It could be any dice.

<!-- TODO: Simulation -->

Or, instead of dice, we could pull scrabble tiles out of a bag. We'd just pull
out one tile, then put it back, and then pull out another tile. And each player
will just do this certain number of times. We can then just add up the tile
values for the tiles that each player has drawn out and we can plot a histogram
of the scores.

<!-- TOOD: Simulation -->

Now these games might seem a little artificial, but they actually represent
something of a _general principle_ that is _widely applicable_. What do all
these games have in common? In all the games the final score of each player is
determined by **adding up** a bunch of numbers. In fact, any process that
involves **adding up** a bunch of numbers will produce something that looks like
a **normal distribution**.

Let's take something that seems very different to these games---for example, a
development process like a people's heights. A person's current height is
determined by **adding up** all the bits of growth they've done from the moment
that they were born. Because there's this adding up process, we might expect
heights of people to be **normally distributed**. As a result, **normal
distributions** are pretty ubiquitous.

#### Processes that don't produce normal distributions

We won't cover other distribution shapes in much detail, but let us take a look
at an example of a process that doesn't produce a normal distribution. To do
this, we'll just modify the dice game in Box \@ref(fig:box5).

In Box \@ref(fig:box5), click the option that says **Multiply**. Doing so
changes the rules of the dice game so that a player's score is determined by
*multiply*ing together the values of their dice rolls. For example, under the
new rules, if a player rolled 1, 4, 3 then their score would be 12 (1 × 4 × 3 =
12). Now try clicking Roll! to see the shape of the distribution. This new
distribution has an extreme **skew**. The vast majority of players have fairly
low scores, but a tiny minority of players have extremely high scores. When you
have a process that grows by multiplying, then you'll get a distribution that
looks like this.

In psychology, we won't study many processes that grow like this, but some
processes that grow like this will be very familiar to you. Think about
something like wealth. Wealth tends to grow by multiplying. For example, earning
interest or a return on investment of 10% means that the new value of the
investment is 1.10 times the original value. This feature of wealth growth
explains why, for example, in the UK, the top 10% of people control more wealth
than the bottom 50%.

#### Describing deviations from the normal distribution

When you clicked the _Multiply_ button, the dice game produced a distribution
that was **skew**. **Skew** is a technical term that describes one way in which
a distribution can _deviate_ from a _normal distribution_. The _normal
distribution_ is **symmetrical**, but a **skew** distribution is not. A
left-skewed distribution has a longer _left tail_, and a **right-skewed**
distribution has a longer _right tail_. Use Box \@ref(fig:box6) to explore
**skewness**.

```{r box6, fig.cap="An example of a skew distribution. Negative, or left skew, means the left tail is longer. Positive, or right skew, means the right tail is longer."}
knitr::include_url(url = "https://paas-embed.netlify.app/skew/index.html", height = "250px")
```

Apart from **skew**, deviations from the **normal** distribution can occur when
a distribution either has fatter or skinnier **tails** than the normal
distribution. The _tailedness_ of a distribution is given by its **kurtosis**.
The kurtosis of a distribution is often specified with reference to the **normal
distribution**. In this case, what is being reported is **excess** kurtosis. A
distribution with **positive excess kurtosis** has a higher kurtosis value than
the normal distribution, and a distribution with **negative excess kurtosis**
has a lower kurtosis value than the normal distribution. In your research
methods courses, you probably won't come across many distributions that have
negative excess kurtosis. However, the distribution that describes dice rolls is
one such distribution, and this will be discussed briefly later in this course.
You will encounter distributions with positive excess kurtosis more often. In
particular, the _t_-distributions, a distribution with positive excess kurtosis,
will be used in several of the statistical procedures that you will learn. In
Box \@ref(fig:box7), you can explore excess kurtosis. When excess kurtosis is
set to 0, then the figure displays a normal distribution. Distributions will no
excess kurtosis are called _mesokurtotic_. When excess kurtosis is negative, the
figure displays a thin tailed or _platykurtotic_ distribution. And when excess
kurtosis is positive, the figure displays a fat-tailed or _leptokurtotic_
distribution.

<!-- FIXME: update the kurtosis visualisation -->

```{r box7, fig.cap="An example of excess kurtosis. Positive excess kutoris results in fat tails (exteme values are more common) and negative excess kurtosis results in skinny tails (extreme values are less common)"}
knitr::include_url(url = "https://paas-embed.netlify.app/kurtosis/index.html", height = "250px")
```

## Distributions and samples

Now that we've covered samples, distributions, and populations, we're going to
start putting them all together. We saw that whenever we look at the
distribution of values where the values are produced by **adding up numbers** we
get something that looks like a normal distribution. And we saw that when we
worked out the **mean of a sample of data** we did this by **adding up all the
values** and then _dividing that number by the number of values_.

Let's say that we want to measure some phenomenon---for example, scores on some
standardised reading tests. We collect a sample of data from 50 children, and
then we work out the **mean of this sample**, by **adding up the 50 scores** and
then dividing this value by 50. Now let's say that instead of only collecting
one sample, we collect 100,000 samples. We work out the **mean** for _each
sample_. And we then draw a histogram of the means for our 100,000 samples. What
will this distribution look like? In our games, any time we added up numbers, as
long as we added enough enough numbers, and we had a large enough set of
players, then we'd get a normal distribution.

The same goes for our sample. As long as we add up enough numbers (our sample
size) and we have enough samples (think, number of players) then the
distribution of the means (the distribution of the players scores) will be
**normally distribution**. This simple observation is fairly **central** in
statistics. So much so that it's called **the central limit theorem**.

### A distribution of our samples

The **distribution** of some statistic---for example, the **mean**---that we see
when repeatedly sample from the population is called **the sampling
distribution**. That's a bit of a mouthful so we'll try to unpack it a bit.

Let's us go back to example where we were working out the mean of a series of
dice rolls. In that example, we saw how the mean of our samples moved around
from sample to sample, so sometimes it was closer to the true mean, and
sometimes it was further away. We can see this in the box below.

<!-- TODO: Add box -->

Now instead of looking at our sample means wiggle about from sample to sample,
we're just going to collect a lot of samples and then plot the distribution. We
can see that here:

<!-- TODO: Add box -->

<!-- TODO: Need to emphasise sample size where it starts to look normal -->

As we can see, the shape of the **sampling distribution of the mean** is
unsurprisingly a **normal distribution**. Other statistics have sampling
distributions too. For example, we could calculate the **variance** for each
sample and the we could plot the **sampling distribution of the variance**. The
sampling distribution of variance **won't** be normally distributed. Why?
Remember that whenever we add stuff up we get a normal distribution, and working
out a mean is just a special way of adding up numbers. When we work out a
**variance** we're not adding numbers up. If you look at the formula, you'll
notice that there's a $^2$ in there. That means we're doing some multiplying.
And we saw that when we were multiplying numbers instead of adding them, we got
a distribution that wasn't normally distributed.

<!-- NOTE: Maybe pull the variance stuff out -->


Because the sampling distribution of **the mean** is **normally
distributed** (assuming the sample size is large enough, see
<!-- FIXME: Link to above somewhere -->
) it's centre and spread can be described the same way as any other
normal distribution. It will have a centre, or **mean** that is 
_equal to the population mean_. And it will have a spread, or
**standard deviation** that is _proportional_ to the _sample size_ and
the _population standard deviation_.

### Standard error of the mean

The _standard deviation of the sampling distribution_ of the mean has a special
name. It is called the **standard error** of the mean. 
<!-- TODO: As we saw above, how spread out the individuals samples were was -->
<!-- determined by two things. First, how spread out the population was and, -->
<!-- two, the sample size of the samples. -->
The spread of these individual samples **is the standard error**.
Therefore, just as we saw above, we know that the **standard error**
will get smaller as the sample size goes up. And we know that the
**standard error** will go down as the **population standard deviation**
goes down. We can see how these two factors works together when we look
at the formula for the **standard error of the mean (SEM)** in @eq-sem.

$$\mathrm{SEM}=\frac{\sigma}{\sqrt{N}}$${#eq-sem}

<!-- TODO: Add explorer box -->

<!-- NOTE: Maybe something in here about the sampling distribution is useful -->


# Distributions, functions, and transformations

<!-- NOTE: THIS IS MILAN'S INTRODUCTION
# Distributions, functions, transformations

In this lecture we will start thinking about variables in terms of
distributions. We will see how we can perform simple arithmetic
operations, such as addition and multiplication on entire variables to
perform linear transformations. We'll discuss one transformation in
particular, the *z*-transformation, and see how it's used to standardise
the values of a variable. Finally, we will talk about how we can use
simple maths to compare groups on a measured variable of interest. -->

<!-- FIXME: I'm going to need a bit of an introduction here
In previous lectures we  -->

```{r}
#| echo: false
#| label: fig-height-histogram
#| fig.cap: |
#|    Distribution of heights in a sample of 1000 women. Not real
#|    data.
# require(ggplot2)
# require(plotshow)
require(cowplot)
set.seed(124)
df <- tibble::tibble(x = rnorm(n = 1000, mean = 0, sd = 1) |>
scale() |> as.numeric())
df$x <- df$x * 10
df$x <- df$x + 165
df_m <- df$x |> mean()
df_s <- df$x |> sd()
range <- glue::glue("{round(df_m - df_s  * 1)}--{round(df_m + df_s * 1)}")
ggplot(df, aes(x = x)) +
  geom_histogram(binwidth = 1, boundary = 130) +
  labs(x = "height in cm") +
  theme_cowplot() +
  NULL
```

For example, if we were to measure the height of `r df$x |> length()` women and plot the
values then we might get something like the plot in
@fig-height-histogram. As you can see, the vast majority of the measured
heights are in the `r range` centimetre range. Only a small number of
people fall outside of this range. You can also see that the
distribution is roughly symmetrical around its mean (`r round(df_m)` cm)
and it has a shape characteristic of a normal distribution.

<!-- 
NOTE: Milan's passage
coincidence: height is a normally-distributed variable as you found out
in
[Lecture 6](https://paas.netlify.app/lectures/06/handout/#describing-the-normal-distribution).
Of course, the shape isn't as smooth as the normal curve you saw in the
same lecture. This is because 500 observations is too few to smooth out
any statistical fluctuations due to sampling. So, just by chance alone,
we can end up with a few more 173-cm-tall people and a few fewer
172-cm-tall people than we would expect based on what we know about the
normal distribution. -->

Of course it doesn't look exactly like a normal distribution, because a
normal distribution (as we saw in
<!-- FIXME: add ref -->
is a smooth line. Our plot is a histogram, where we've just counted up
the number of people that fall into each bin of 5 centimetres in width.
However, we could imagine measuring the heights of many more people and
making the bins narrower and narrower. 


```{r}
ideal <- \(x) (dnorm(x, df_m, df_s) / dnorm(df_m, df_m, df_s)) * 195
ggplot(df, aes(x)) +
  geom_histogram(binwidth = 5, boundary = 130) +
  geom_density(col = "grey20", size = 2, aes(y = ..count.. * 5)) +
  # geom_function(fun = ideal, col = "blue", size = 2) +
  labs(y = "count", x = "height in cm") +
  theme_cowplot()
```

We can see an idealised representation of this curve in @fig- shown in
blue. This idealised representation is a normal distribution with a mean
of `r df_m` and a standard deviation of `r df_s`. 

<!-- NOTE: +/- 1 SD -->

## The standard normal distribution

The **standard normal distribution** is a normal distribution with 
$\mu=0$ and $\sigma=1$.

<!-- FIXME: Add visualisation -->

There are two import things to take note of here:

First, the mean and the standard deviation are **independent of one
another**. You can change one and the other will stay the same.

<!-- NOTE: I should probably preface this earlier -->
Second, changing the mean and standard deviation doesn't change the
*fundamental shape* of the distribution. The distribution might appear
flatter or pointer but the **relative position of points on the line
with respect to each other doesn't change**. So, for example, if one
point A is twice as high as point B when $\sigma$ is 1, then point A is
still twice as high as point B when $\sigma$ is 0.5 or when $\sigma$ is
2. 

<!-- TODO: Maybe add a visualisation -->

To make it a little more concrete, we'll use an example. Let's take our
people's heights that we plotted in @fig-height-histogram. In this
example, we measured height in centimetres. It should be pretty obvious
that your height doesn't change depending on the units you measure it
in: you're the same height whether you get measured in centimetres,
metres, millimetres, feet, or inches.

If we measured height of the sample of women in metres instead of
centimetres, the shape of the plot should remain the same. You can see
this in @fig-height-histogram-new. 

```{r}
#| echo: false
#| label: fig-height-histogram-new
#| fig-cap: |
#|    Distribution of heights in a sample of 1000 women. Not real
#|    data.
#| fig-subcap:
#|    - "Measured in centimetres"
#|    - "Measured in metres"
#| layout-ncol: 2
# require(ggplot2)
# require(plotshow)
require(cowplot)
set.seed(124)
df <- tibble::tibble(x = rnorm(n = 1000, mean = 0, sd = 1) |>
scale() |> as.numeric())
df$x <- df$x * 10
df$x <- df$x + 165
df_m <- df$x |> mean()
df_s1 <- df$x |> sd()
range <- glue::glue("{round(df_m - df_s  * 1)}--{round(df_m + df_s * 1)}")
ggplot(df, aes(x = x)) +
  geom_histogram(binwidth = 5, boundary = 130) +
  labs(x = "height in cm") +
  theme_cowplot() +
  NULL


set.seed(124)
df <- tibble::tibble(x = rnorm(n = 1000, mean = 0, sd = 1) |>
scale() |> as.numeric())
df$x <- df$x * (10 / 100)
df$x <- df$x + (165 / 100)
df_m <- df$x |> mean()
df_s2 <- df$x |> sd()
range <- glue::glue("{round(df_m - df_s  * 1)}--{round(df_m + df_s * 1)}")
ggplot(df, aes(x = x)) +
  geom_histogram(binwidth = 5 / 100, boundary = 130 / 100) +
  labs(x = "height in m") +
  theme_cowplot() +
  NULL
```

The distribution in @fig-height-histogram-new-1 has a standard deviation
of `r df_s1` while the distribution in @fig-height-histogram-new-2 has a
standard deviation of `r df_s2`. But as you can see, they're they same
distributions---they're just displaced on **different scales**
(centimetres versus metres).

:::{.callout-note}

Changing the **scale** changes the **standard deviation**. This is why
the **standard deviation** is sometimes referred to as the **scale
parameter** for the distribution.

:::

We've seen how we can change the **scale** of the distribution, by
measuring it in metres instead of centimetres. But we can also change
the where the distribution is centred. We can see an example of this in
@fig-height-histogram-new-location.


```{r}
#| echo: false
#| label: fig-height-histogram-new-location
#| fig-cap: |
#|    Distribution of heights in a sample of 1000 women. Not real
#|    data.
#| fig-subcap:
#|    - "Measured in centimetres"
#|    - "Measured in difference from the average height"
#| layout-ncol: 2
# require(ggplot2)
# require(plotshow)
require(cowplot)
set.seed(124)
df <- tibble::tibble(x = rnorm(n = 1000, mean = 0, sd = 1) |>
scale() |> as.numeric())
df$x <- df$x * 10
df$x <- df$x + 165 
df_m <- df$x |> mean()
df_s1 <- df$x |> sd()
range <- glue::glue("{round(df_m - df_s  * 1)}--{round(df_m + df_s * 1)}")
ggplot(df, aes(x = x)) +
  geom_histogram(binwidth = 5, boundary = 130) +
  labs(x = "height in cm") +
  theme_cowplot() +
  NULL


set.seed(124)
df <- tibble::tibble(x = rnorm(n = 1000, mean = 0, sd = 1) |>
scale() |> as.numeric())
df$x <- df$x * (10 )
df$x <- df$x + (165 - 166)
df_m <- df$x |> mean()
df_s2 <- df$x |> sd()
range <- glue::glue("{round(df_m - df_s  * 1)}--{round(df_m + df_s * 1)}")
ggplot(df, aes(x = x)) +
  geom_histogram(binwidth = 5,  boundary = 130 - 165) +
  labs(x = "difference from average height in cm") +
  theme_cowplot() +
  NULL
```

In @fig-height-histogram-new-location-1 we can see the same distribution
as before. But in @fig-height-histogram-new-location-2 we can see a
distribution that is now centred at 0. In this distribution we've just
changed where the centred is **located**, but the distribution is still
the same.

:::{.callout-note}

Changing the **mean** changes where the centre of the distribution is
**located**. This is why the mean is sometimes referred to as the
**location parameter** for the distribution.

:::

## Transformations

In @fig-height-histogram-new and @fig-height-histogram-new-location we
saw how we could **transform** a variable so that the shape of the
distribution stayed the same, but the **mean** and the **standard
deviation** changed. These two kinds of **transformations** are known as
**centring** and **scaling**.


### Centering

Centring is performed by **subtracting** a fixed value from each observation in
our dataset. This has the effect of shifting the distribution of our variable
_along the x-axis_. You can technically centre a variable by subtracting _any_
value from it but the most frequently used method is **mean-centring**.
This is shown in @eq-center, below:

$$x_i - \bar{x}$${#eq-center}

:::{.callout-tip}


Applying this transformation results in shifting the variable so that
it's mean is at the zero point and the individual values of the
mean-centred variable tell us how far that observation is from the mean
of the entire variable.

:::


<!-- NOTE: Some kind of visualiation -->
<!-- ::: warn
It's crucially important to understand that mean-centring **does not
alter the shape of the variable, nor does it change the scale at which
the variable is measured**. It only **changes the interpretation** of
the values from the raw scores to differences from the mean.
:::

The variable in panel B in Figure [5](#fig:height-centred) is still
measured in centimetres and still has the same standard deviation as the
original height variable (panel A). -->

### Scaling

Scaling is performed by **dividing** each observation by some fixed
value. This has the effect of stretching or compressing the variable
*along the x-axis*.

Just like centring, you can technically scaled a variable by dividing it
by *any* value. For example, we created @fig-height-histogram-new-2 by
taking the values in @fig-height-histogram-new-1 and dividing them by
100 to transform the height in centimetres to a height in metres.
However, the most frequent method of scaling is by dividing values by
the standard deviation of the dataset. This is shown in @eq-scale,
below:

$$\frac{x_i}{s_x}$${#eq-scale}

<!-- FIXME: Add a figure of scaled heights by SD -->


Just like with centring, the fundamental shape of the variable's
distribution did not change as a result of scaling.


<!-- ::: warn
Unlike centring, however, scaling does change the **scale** on which the
variable is measured. After all, that's why it's called scaling...
:::

Unfortunately, the scale on which the variable in panel B of
Figure [6](#fig:height-scaled) is shown is not very interpretable: the
individual numbers don't mean much.

::: warn
To make scaling by standard deviation more useful, we need to make sure
the variable is **mean-centred** first!
:::

To understand why, have a little play around with the [visualisation
below](#z-transform-app). For the time being, leave the subtraction
control (purple circle) alone. Instead, drag the division control (green
circle) left and right to see what happens. -->


### The *z*-transform

The combination of first mean-centring a variable and then scaling a
variable by it's standard deviation is known as the **z-transform**. The
formula for this is shown in @eq-z, below:

$$z(x) = \frac{x_i - \bar{x}}{s_x}$${#eq-z}

<!-- FIXME: Show an example of z-transformed heights -->


<!-- Again, the shape of the variable remains intact and the relative
differences between any two values in the variable are preserved. That's
because **standardisation is a linear transformation**, just like
addition and multiplication.

::: warn
This time, however, the scores are interpretable. Standardisation (or
*z*-transformation) converts values of a variable to numbers that can be
interpreted in terms of **the number of [standard deviations from the
mean](https://youtu.be/YZgoIGmwrs4?t=36){target="_blank"}**.
::: -->


### *z*-scores

Values of a standardised/*z*-transformed variables are called
*z*-scores. To repeat, they are interpreted as **the distance from the
mean in units of standard deviation**.

<!-- FIXME: update values -->
So, for example, let's say that, in the sample of 500 women whose height
we've been plotting over and over again, the mean height was 164.94 cm
with a *SD* of 5.99 cm. Let's say we standardised the variable. A person
with a *z*-score of 1 will be *one* SD *taller than average*: 164.94 +
(1 × 5.99) = 170.93 cm. Someone with a *z*-score of &mius;0.8 will be
0.8 *SD* **shorter** than the average person in the sample: 164.94 +
(−0.8 × 5.99) = 160.15 cm.

<!-- NOTE: KEEP THIS MORE OR LESS -->
<!-- The interpretation of *z*-scores **is the same no matter what variable
we are working with**! This is very useful for comparing scores on
different variables or across different groups. -->

## Making comparisons


<!-- TODO: Sampling distribution of z-transformed values -->
<!-- NOTE: Not sure if I should include anything about
this
Standardisation is also useful when it comes to the **sampling
distribution**. A see saw in REF, the sampling distribution has a mean
that is the same as the population mean at it has a standard deviation
that is equal to the population standard deviation divided by the square
root of the sample size. We could standardise the **sampling
distribution** by taking each value in a sample and subtract the
population mean and dividing it by the standard error. This would result
in a **sampling distribution** that matches the **standard normal
distribution**. -->

<!-- FIXME: SWITCH THESE AROUND -->
### Comparing groups

```{r}
#| echo: false
#| warning: false
#| label: fig-sport-histogram
#| fig.align: center
#| fig.cap: |
#|    Distribution of reaction times in a sample of amateur (green) and
#|    500 professional (blue) sportspeople. Group means are indicated
#|    with the vertical lines.
require(ggplot2)
sport_data <- tibble::tribble(
  ~group, ~mean, ~sd,
  "amateur", 500, 30,
  "sport", 460, 15
)


ama_mean <- sport_data |>
  dplyr::filter(group == "amateur") |>
  dplyr::pull(mean)
spo_mean <- sport_data |>
  dplyr::filter(group == "sport") |>
  dplyr::pull(mean)

N <- 500

set.seed(123)
sport_tib <- purrr::pmap_df(sport_data, function(group, mean, sd) {
  d <- rnorm(N, mean, sd) |> scale() |> as.numeric()
  tibble::tibble(group = group, x = mean + (d * sd))
})

ggplot2::ggplot(sport_tib, aes(x = x, 
  fill = group,
  group = group,
  alpha = group,
  color = group,
  size = group)) +
  geom_histogram(bins = 50, boundary = 100, position = "identity") +
  scale_alpha_manual(values = c(1, 0.7), guide = "none") +
  scale_fill_manual(values = c("seagreen", "blue"), guide = "none") +
  scale_color_manual(values = c("white", "white"), guide = "none") +
  scale_size_manual(values = c(.2, .2), guide = "none") +
  geom_vline(xintercept = ama_mean, color = "seagreen", size = 1.5) +
  geom_vline(xintercept = spo_mean, color = "blue", size = 1.5) +
  ggplot2::annotate(geom = "text", x = ama_mean, y = 60,
  label = expression(bar('X')[amateur]), hjust = -0.3, color = "seagreen") +
  ggplot2::annotate(geom = "text", x = spo_mean, y = 60,
  label = expression(bar('X')[pro]), hjust = -0.3, color = "blue") +
  theme_cowplot() +
  labs(x = "reaction time (ms)", y = "count") +
  NULL

```


When we talk about comparing groups on some variable in the context of
quantitative research we are most often talking about looking at the
**average difference** in the variable between the groups. In other
words, we are asking, how different the groups are *on average*.

Let's make it concrete with an example. Suppose we're interested in
amateur and professional sportspeople on a simple target detection task
where participants simply have to press a button as quickly as possible
when a particular stimulus appears on computer screen. We get 500
amateur and 500 professional sportspeople to participant in this
experiment. The results of this experiment show that amateurs have a 
mean reaction time of `r ama_mean` ms and professionals have a mean 
reaction time of `r spo_mean` ms. @fig-sport-histogram shows the 
histogram of these data. 

In this example we can see that there is a lot of overlap between the
two groups. However, we can also clearly see that there is a difference
in the *average* reaction times between amateur and professional
sportspeople. To quantify this difference, all we would need to do is to
**subtract the mean of one group from the mean of the other group**.

<!-- NOTE: Do the calculations -->


The sign indicates the direction of the difference. If the number is
positive, that means that the first group's mean is larger than that of
the second group. If the number is negative, the opposite is true. Of
course, it is completely arbitrary which group is *first* and which is
*second*.


<!-- NOTE: This should be about comparing full distributions. -->

## Comparing across groups

Suppose we're interested in comparing 8-year-olds and 14-year-olds on
puzzle completion task. Because 8-year-olds and 14-years-olds are at
different developmental stages there are two versions of the task that
are scored in slightly different ways.

```{r}
#| echo: false
stats <- tibble::tribble(
  ~age, ~mean, ~sd,
  "8", 80, 2,
  "14", 120, 8,
  )

a_z <- 3
b_z <- .5
a_child <- stats$mean[1] + (stats$sd[1] * a_z)
b_child <- stats$mean[2] + (stats$sd[2] * b_z)


```

Ahorangi is 8 years old, and she got a score of `r a_child`. Benjamin is
14 years old, and he got a score of `r b_child`. We an easily tell that
Benjamin got a higher score than Ahorangi. But the scores are not
directly comparable, because they're measured on different scales. So
how can we compare them? What we need to do instead is look at how each
of them performed **relative** to their age groups. Is
Ahorangi better performing relative to 8-year-olds than Benjamin is
relative to 14-year-olds?

To answer this question we can use *z*-scores. By standardising the
time variable across each group, we get variables that are on the same
scale. Do do this, we'll need to know the **mean** and **standard
deviation** for each of the age groups. We can see these details in
@tbl-age-means.

```{r}
#| echo: false
#| label: tbl-age-means
#| tbl-cap: |
#|      Means and Standard deviations for the 8-year-old and
#|      14-year-old age groups
stats |>
  dplyr::mutate(age = dplyr::case_when(
    age == 8 ~ "8-year-olds",
    age == "14" ~ "14-year-olds")) |>
  knitr::kable(col.names = c("Age group", "Mean", "Standard deviation"))
```


<!-- TODO: ADD FORMULA -->

Let's use this formula to calculate Ahorangi and Benjamin's
*z*-score. First, for Ahorangi:

$$`r a_z` = \frac{`r a_child` - `r stats$mean[1]`}{`r stats$sd[1]`}$$

And next for Benjamin:


$$`r b_z` = \frac{`r b_child` - `r stats$mean[2]`}{`r stats$sd[2]`}$$

So we now know that Ahorangi's *z*-score is `r a_z`, and that Benjamin's  
*z*-score is `r b_z`. This means that Ahorangi's score is `r a_z`
standard deviations *higher* than the average 8-year-old.
Benjamin's *z*-score is `r b_z` *higher* than the average 14-year-old.
That means, that Ahorangi, despite having a lower score, actually scored
very high for an 8-year-old. Benjamin, on the other hand, only scored a
little higher than the average 14-year-old.


We could use the same idea to compare values of **of variables measured
on any scale**.






